{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7edebe7d",
   "metadata": {},
   "source": [
    "## TabNet Implentation for Tabular Data\n",
    "\n",
    "TabNet is proposed in [this article] (https://arxiv.org/abs/1908.07442) as a neural network architecture capable of learning a canonical representation of tabular data. This architecture has shown to perform well against the current gold-standard gradient boosting models for learning on tabular data.\n",
    "\n",
    "TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful\n",
    "features to process at each decision step. Instance-wise feature selection enables efficient learning as the model capacity is fully used for the most salient features, and also yields\n",
    "more interpretable decision making via visualization of selection masks. \n",
    "\n",
    "**Taken**\n",
    "\n",
    "This implementation closely follows [the TabNet implementation in PyTorch linked here](https://github.com/dreamquark-ai/tabnet/tree/b6e1ebaf694f37ad40a6ba525aa016fd3cec15da). The description of that implementation is [explained in this helpful video by Sebastian Fischman](https://www.youtube.com/watch?v=ysBaZO8YmX8).\n",
    "\n",
    "<img src=\"images/tabnet_schematic.jpg\" width=\"1000\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe92b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import global_variables as gv\n",
    "import utilities\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c86241",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = gv.outcomes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aead3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessed dataset \n",
    "  \n",
    "df0 = pd.read_csv(gv.tabnet_data)\n",
    "df0.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs = df0.iloc[:,:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59fb14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30850-0.0</th>\n",
       "      <th>30780-0.0</th>\n",
       "      <th>30690-0.0</th>\n",
       "      <th>30790-0.0</th>\n",
       "      <th>23101-0.0</th>\n",
       "      <th>23099-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>23100-0.0</th>\n",
       "      <th>30710-0.0</th>\n",
       "      <th>30760-0.0</th>\n",
       "      <th>30640-0.0</th>\n",
       "      <th>30750-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>30770-0.0</th>\n",
       "      <th>30740-0.0</th>\n",
       "      <th>30630-0.0</th>\n",
       "      <th>30870-0.0</th>\n",
       "      <th>21001-0.0</th>\n",
       "      <th>1488-0.0</th>\n",
       "      <th>4079-0.0</th>\n",
       "      <th>1299-0.0</th>\n",
       "      <th>21003-0.0</th>\n",
       "      <th>1160-0.0</th>\n",
       "      <th>1438-0.0</th>\n",
       "      <th>4080-0.0</th>\n",
       "      <th>1458-0.0</th>\n",
       "      <th>1528-0.0</th>\n",
       "      <th>1319-0.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>1289-0.0</th>\n",
       "      <th>1309-0.0</th>\n",
       "      <th>1418-0.0</th>\n",
       "      <th>1329-0.0</th>\n",
       "      <th>1220-0.0</th>\n",
       "      <th>1428-0.0</th>\n",
       "      <th>1249-0.0</th>\n",
       "      <th>1349-0.0</th>\n",
       "      <th>1369-0.0</th>\n",
       "      <th>20117-0.0</th>\n",
       "      <th>2100-0.0</th>\n",
       "      <th>2654-0.0</th>\n",
       "      <th>1339-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>2050-0.0</th>\n",
       "      <th>1408-0.0</th>\n",
       "      <th>1200-0.0</th>\n",
       "      <th>1538-0.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>1359-0.0</th>\n",
       "      <th>1389-0.0</th>\n",
       "      <th>1478-0.0</th>\n",
       "      <th>2090-0.0</th>\n",
       "      <th>1508-0.0</th>\n",
       "      <th>1379-0.0</th>\n",
       "      <th>6142-0.0</th>\n",
       "      <th>1468-0.0</th>\n",
       "      <th>1548-0.0</th>\n",
       "      <th>1239-0.0</th>\n",
       "      <th>1448-0.0</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>outcome_cardiomyopathies</th>\n",
       "      <th>outcome_ischemic_heart_disease</th>\n",
       "      <th>outcome_heart_failure</th>\n",
       "      <th>outcome_myocardial_infarction</th>\n",
       "      <th>outcome_peripheral_vascular_disease</th>\n",
       "      <th>outcome_cardiac_arrest</th>\n",
       "      <th>outcome_cerebral_infarction</th>\n",
       "      <th>outcome_arrhythmia</th>\n",
       "      <th>multi-labels</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50800</td>\n",
       "      <td>3.88800</td>\n",
       "      <td>6.47700</td>\n",
       "      <td>65.1984</td>\n",
       "      <td>45.2</td>\n",
       "      <td>35.6</td>\n",
       "      <td>74.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.70600</td>\n",
       "      <td>1.21100</td>\n",
       "      <td>35.065</td>\n",
       "      <td>102.0</td>\n",
       "      <td>26.339</td>\n",
       "      <td>5.62200</td>\n",
       "      <td>1.59300</td>\n",
       "      <td>0.97700</td>\n",
       "      <td>24.5790</td>\n",
       "      <td>6.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.52</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.08800</td>\n",
       "      <td>3.52000</td>\n",
       "      <td>5.51200</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>74.6</td>\n",
       "      <td>36.5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.17300</td>\n",
       "      <td>1.01900</td>\n",
       "      <td>40.900</td>\n",
       "      <td>113.0</td>\n",
       "      <td>10.701</td>\n",
       "      <td>5.05200</td>\n",
       "      <td>1.39000</td>\n",
       "      <td>2.35800</td>\n",
       "      <td>35.0861</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>65</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.73364</td>\n",
       "      <td>4.10892</td>\n",
       "      <td>6.47949</td>\n",
       "      <td>50.8588</td>\n",
       "      <td>71.7</td>\n",
       "      <td>29.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.58546</td>\n",
       "      <td>1.22432</td>\n",
       "      <td>84.100</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.763</td>\n",
       "      <td>13.71763</td>\n",
       "      <td>1.74423</td>\n",
       "      <td>2.78764</td>\n",
       "      <td>30.7934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 1, 1]</td>\n",
       "      <td>55</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.78800</td>\n",
       "      <td>2.88700</td>\n",
       "      <td>5.56500</td>\n",
       "      <td>56.5183</td>\n",
       "      <td>40.2</td>\n",
       "      <td>29.8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.11500</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>36.400</td>\n",
       "      <td>91.0</td>\n",
       "      <td>31.672</td>\n",
       "      <td>4.82700</td>\n",
       "      <td>1.89100</td>\n",
       "      <td>1.15700</td>\n",
       "      <td>20.7577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>Irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75600</td>\n",
       "      <td>2.67000</td>\n",
       "      <td>4.68000</td>\n",
       "      <td>4.7700</td>\n",
       "      <td>46.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.49300</td>\n",
       "      <td>0.73300</td>\n",
       "      <td>34.200</td>\n",
       "      <td>105.0</td>\n",
       "      <td>42.209</td>\n",
       "      <td>5.06300</td>\n",
       "      <td>1.86900</td>\n",
       "      <td>1.67700</td>\n",
       "      <td>25.9766</td>\n",
       "      <td>7.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   30850-0.0  30780-0.0  30690-0.0  30790-0.0  23101-0.0  23099-0.0  48-0.0  \\\n",
       "0    0.50800    3.88800    6.47700    65.1984       45.2       35.6    74.0   \n",
       "1   13.08800    3.52000    5.51200    15.4000       74.6       36.5   120.0   \n",
       "2    9.73364    4.10892    6.47949    50.8588       71.7       29.7   112.0   \n",
       "3    1.78800    2.88700    5.56500    56.5183       40.2       29.8    67.0   \n",
       "4    0.75600    2.67000    4.68000     4.7700       46.5       30.1    85.0   \n",
       "\n",
       "   23100-0.0  30710-0.0  30760-0.0  30640-0.0  30750-0.0  49-0.0  30770-0.0  \\\n",
       "0       25.0       0.34    1.70600    1.21100     35.065   102.0     26.339   \n",
       "1       42.9       3.94    1.17300    1.01900     40.900   113.0     10.701   \n",
       "2       30.3       3.88    1.58546    1.22432     84.100   107.0     18.763   \n",
       "3       17.0       0.87    2.11500    0.81000     36.400    91.0     31.672   \n",
       "4       20.0       0.18    1.49300    0.73300     34.200   105.0     42.209   \n",
       "\n",
       "   30740-0.0  30630-0.0  30870-0.0  21001-0.0  1488-0.0  4079-0.0  1299-0.0  \\\n",
       "0    5.62200    1.59300    0.97700    24.5790       6.0      77.0      10.0   \n",
       "1    5.05200    1.39000    2.35800    35.0861       2.0      91.0       2.0   \n",
       "2   13.71763    1.74423    2.78764    30.7934       0.0      99.0       2.0   \n",
       "3    4.82700    1.89100    1.15700    20.7577       0.0      71.0       5.0   \n",
       "4    5.06300    1.86900    1.67700    25.9766       7.0      73.0       4.0   \n",
       "\n",
       "   21003-0.0  1160-0.0  1438-0.0  4080-0.0  1458-0.0  1528-0.0  1319-0.0  \\\n",
       "0       54.0       7.0      10.0     110.0      3.73       2.0       0.0   \n",
       "1       65.0       9.0      12.0     166.0      7.00       2.4       0.0   \n",
       "2       55.0       7.0      10.0     135.0      7.00       2.0       0.0   \n",
       "3       49.0       8.0      14.0     116.0      5.00       3.0       1.0   \n",
       "4       61.0       7.0       2.0     113.0      7.00       4.0       2.0   \n",
       "\n",
       "   845-0.0  1289-0.0  1309-0.0  1418-0.0  1329-0.0  1220-0.0  1428-0.0  \\\n",
       "0    23.52       6.0       2.0         3         2         0         0   \n",
       "1    16.00       2.0       1.0         2         2         0         1   \n",
       "2    21.00       3.0       1.0         2         1         0         0   \n",
       "3    18.00       5.0       1.0         2         2         0         0   \n",
       "4    16.00       3.0       3.0         3         2         1         1   \n",
       "\n",
       "   1249-0.0  1349-0.0  1369-0.0  20117-0.0  2100-0.0  2654-0.0  1339-0.0  \\\n",
       "0         1         1         1          2         1         6         2   \n",
       "1         1         4         2          2         0         7         2   \n",
       "2         1         2         1          2         0         7         2   \n",
       "3         4         1         2          2         0         7         2   \n",
       "4         4         1         1          2         0         7         3   \n",
       "\n",
       "   21000-0.0  2050-0.0  1408-0.0  1200-0.0  1538-0.0  31-0.0  6138-0.0  \\\n",
       "0          0         2         1         3         2       0         1   \n",
       "1          0         1         3         2         0       1         3   \n",
       "2          0         1         2         2         1       1         3   \n",
       "3          2         1         2         1         2       0         6   \n",
       "4          0         1         3         1         0       0         3   \n",
       "\n",
       "   1359-0.0  1389-0.0  1478-0.0  2090-0.0  1508-0.0  1379-0.0  6142-0.0  \\\n",
       "0         2         1         1         1         3         1         1   \n",
       "1         3         1         1         0         2         2         1   \n",
       "2         3         2         1         0         2         2         1   \n",
       "3         2         2         1         0         2         2         1   \n",
       "4         3         1         2         0         1         1         1   \n",
       "\n",
       "   1468-0.0  1548-0.0  1239-0.0  1448-0.0  hypertension  \\\n",
       "0         3         2         0         3             0   \n",
       "1         5         2         0         1             1   \n",
       "2         4         2         0         3             1   \n",
       "3         3         2         0         3             0   \n",
       "4         4         2         0         3             1   \n",
       "\n",
       "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
       "0                         0                               0   \n",
       "1                         0                               1   \n",
       "2                         0                               1   \n",
       "3                         0                               0   \n",
       "4                         0                               0   \n",
       "\n",
       "   outcome_heart_failure  outcome_myocardial_infarction  \\\n",
       "0                      0                              0   \n",
       "1                      0                              1   \n",
       "2                      0                              0   \n",
       "3                      0                              0   \n",
       "4                      0                              1   \n",
       "\n",
       "   outcome_peripheral_vascular_disease  outcome_cardiac_arrest  \\\n",
       "0                                    0                       0   \n",
       "1                                    0                       0   \n",
       "2                                    0                       1   \n",
       "3                                    0                       0   \n",
       "4                                    0                       0   \n",
       "\n",
       "   outcome_cerebral_infarction  outcome_arrhythmia              multi-labels  \\\n",
       "0                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "1                            0                   0  [1, 0, 1, 0, 0, 0, 0, 0]   \n",
       "2                            1                   1  [0, 0, 1, 0, 0, 1, 1, 1]   \n",
       "3                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "4                            0                   0  [1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   age  gender     race  \n",
       "0   54  Female  British  \n",
       "1   65    Male  British  \n",
       "2   55    Male  British  \n",
       "3   49  Female    Irish  \n",
       "4   61  Female  British  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(gv.data_link)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df1.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd30e0",
   "metadata": {},
   "source": [
    "## Build TabNet Architecture\n",
    "\n",
    "#### GLU Block\n",
    "\n",
    "Gated Linear Units act as an attention mechanism where the gates formed involve taking two dense layer outputs, applying a sigmoid to one of them, and then multiplying them together\n",
    "\n",
    "Following GLU blcok contains two dense layers, two ghost batch normalization layers, identity and sigmoid activation functions and multiplication operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20aa12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None,\n",
    "                 virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(GLUBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
    "                                               use_bias=False)\n",
    "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                            momentum=self.momentum)\n",
    "        \n",
    "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
    "                                             use_bias=False)\n",
    "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                          momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        output = self.bn_outout(self.fc_outout(inputs), \n",
    "                                training=training)\n",
    "        gate = self.bn_gate(self.fc_gate(inputs), \n",
    "                            training=training)\n",
    "    \n",
    "        return output * tf.keras.activations.sigmoid(gate) # GLU  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74568525",
   "metadata": {},
   "source": [
    "### Feature Transformer Block\n",
    "\n",
    "Builds two GLU blocks with a skip connection from the output of the first\n",
    "\n",
    "<img src=\"images/tabnet_feature_transformer.jpg\" width=\"700\" height=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d08894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] = 0.02, skip=False):\n",
    "        super(FeatureTransformerBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.skip = skip\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.initial = GLUBlock(units = self.units, \n",
    "                                virtual_batch_size=self.virtual_batch_size, \n",
    "                                momentum=self.momentum)\n",
    "        self.residual =  GLUBlock(units = self.units, \n",
    "                                  virtual_batch_size=self.virtual_batch_size, \n",
    "                                  momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) # skip\n",
    "        \n",
    "        return (initial + residual) * np.sqrt(0.5) # decision and attention dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5721a81",
   "metadata": {},
   "source": [
    "#### Attentive Transformer Block\n",
    "\n",
    "Use TabNet prior as an input to layer and reserve to handle prior updates in TabNet step layer\n",
    "\n",
    "> *prior is used to encourage orthogonal feature selection across decision steps, tell us what we know about features and how we have used them in the previous step\n",
    "\n",
    "<img src=\"images/tabnet_attentive_transformer.jpg\" width=\"200\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c195dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(self.units, \n",
    "                                        use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        return tfa.activations.sparsemax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32359068",
   "metadata": {},
   "source": [
    "### TabNetStep\n",
    "\n",
    "Inputs are batch normalized features, output a shared feature transformer, and priors of the current step. Block outputs the feature embedding at our split point, the masked feature to used in teh shared feature transformer blcok of the next step, and the mask used in our attention operation. Block is built from the Feature Transformer block and AttentiveTransformer block\n",
    "\n",
    "> Mask provided local and global feature attributions for each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bb7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformerBlock(units = self.units, \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum,\n",
    "                                              skip=True)\n",
    "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
    "        split = self.unique(shared, training=training)\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e58836",
   "metadata": {},
   "source": [
    "### TabNet Encoder\n",
    "\n",
    "Entire model architecture as a single layer. Accumulates feature embeddings at each desicion step, updates priors, and computes entropy loss to limit the frequency of feature use across steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca49b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int =1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 outputs: int = 1, \n",
    "                 gamma: float = 1.3,\n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):            \n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size,\n",
    "        momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(units = self.n_features, \n",
    "                                       virtual_batch_size=self.virtual_batch_size, \n",
    "                                       momentum=self.momentum)\n",
    "        self.steps = [TabNetStep(units = self.n_features, \n",
    "                                 virtual_batch_size=self.virtual_batch_size, \n",
    "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.final = tf.keras.layers.Dense(units = self.units, \n",
    "                                           use_bias=False) \n",
    "        \n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
    "        entropy_loss = 0.\n",
    "        encoded = 0.\n",
    "        output = 0.\n",
    "        importance = 0.\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4d9a5",
   "metadata": {},
   "source": [
    "### apply to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ffdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:, gv.numerical_cols+gv.continuous_cols] = df1.loc[:, gv.numerical_cols+gv.continuous_cols].astype(np.float32)\n",
    "df1.loc[:, gv.categorical_cols] = df1.loc[:, gv.categorical_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00891bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts strings to unqiue ints for use in Pytorch Embedding\n",
    "    \"\"\"\n",
    "    labels, levels = pd.factorize(x)\n",
    "    return pd.Series(labels, name=x.name, index=x.index)\n",
    "\n",
    "X, E, y = (df1\n",
    "           .loc[:, gv.numerical_cols+gv.continuous_cols]\n",
    "           .astype('float32')\n",
    "           .join(pd.get_dummies(df1.loc[:, gv.categorical_cols])),\n",
    "           df1\n",
    "           .loc[:, gv.numerical_cols+gv.continuous_cols]\n",
    "           .astype('float32')\n",
    "           .join(df1.loc[:, gv.categorical_cols].apply(get_labels).add(1).astype('int32')),\n",
    "           df1[target] == 'Y')\n",
    "\n",
    "X_train, X_valid, E_train, E_valid, y_train, y_valid = train_test_split(X.to_numpy(), E, y.to_numpy(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b4db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(x: pd.DataFrame, dimension=1) -> Union[tf.feature_column.numeric_column,tf.feature_column.embedding_column]:\n",
    "    if x.dtype == np.float32:\n",
    "        return tf.feature_column.numeric_column(x.name)\n",
    "    else:\n",
    "        return tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_identity(x.name, num_buckets=x.max() + 1, default_value=0),\n",
    "        dimension=dimension)\n",
    "    \n",
    "def df_to_dataset(X: pd.DataFrame, y: pd.Series, shuffle=False, batch_size=50000):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(X.copy()), y.copy()))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "columns = [get_feature(f) for k, f in E_train.iteritems()]\n",
    "feature_column = tf.keras.layers.DenseFeatures(columns, trainable=True)\n",
    "\n",
    "train, valid = df_to_dataset(E_train, y_train), df_to_dataset(E_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "287895d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'1488-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '4079-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1299-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '21003-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1160-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1438-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '4080-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1458-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1528-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1319-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '845-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1289-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1309-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30850-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30780-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30690-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30790-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '23101-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '23099-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '48-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '23100-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30710-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30760-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30640-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30750-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '49-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30770-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30740-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30630-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '30870-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '21001-0.0': TensorSpec(shape=(None,), dtype=tf.float32, name=None), '1418-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1329-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1220-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1428-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1249-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1349-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1369-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '20117-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '2100-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '2654-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1339-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '21000-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '2050-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1408-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1200-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1538-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '31-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '6138-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1359-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1389-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1478-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '2090-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1508-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1379-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '6142-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1468-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1548-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1239-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '1448-0.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'hypertension': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.bool, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb87504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifier(tf.keras.Model):\n",
    "    def __init__(self, outputs: int = 1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 61,\n",
    "                 gamma: float = 1.3, \n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "                 pretrained_encoder: Optional[tf.keras.layers.Layer] = None,\n",
    "                 virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(TabNetClassifier, self).__init__()\n",
    "        \n",
    "        self.outputs = outputs\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.feature_column = feature_column\n",
    "        self.pretrained_encoder = pretrained_encoder\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "        if feature_column is None:\n",
    "            self.feature = tf.keras.layers.Lambda(identity)\n",
    "        else:\n",
    "            self.feature = feature_column\n",
    "        if pretrained_encoder is None:\n",
    "            self.encoder = TabNetEncoder(units=outputs, \n",
    "                                        n_steps=n_steps, \n",
    "                                        n_features = n_features,\n",
    "                                        outputs=outputs, \n",
    "                                        gamma=gamma, \n",
    "                                        epsilon=epsilon, \n",
    "                                        sparsity=sparsity,\n",
    "                                        virtual_batch_size=self.virtual_batch_size, \n",
    "                                        momentum=momentum)\n",
    "        else:\n",
    "            self.encoder = pretrained_encoder\n",
    "\n",
    "    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
    "        X = self.feature(X)\n",
    "        output, encoded, importance = self.encoder(X)\n",
    "          \n",
    "        prediction = tf.keras.activations.sigmoid(output)\n",
    "        return prediction, encoded, importance\n",
    "    \n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        prediction, _, _ = self.forward(X)\n",
    "        return prediction\n",
    "    \n",
    "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        _, encoded, _ = self.forward(X)\n",
    "        return encoded\n",
    "    \n",
    "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        _, _, importance = self.forward(X)\n",
    "        return importance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbc38d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filefime2xxa.py\", line 10, in tf__call\n        (prediction, _, _) = ag__.converted_call(ag__.ld(self).forward, (ag__.ld(X),), None, fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_fileoqzqbhwa.py\", line 11, in tf__forward\n        (output, encoded, importance) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(X),), None, fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filevqecsale.py\", line 17, in tf__call\n        (_, masked, keys) = ag__.converted_call(ag__.ld(self).initial_step, (ag__.ld(B), ag__.ld(shared), ag__.ld(prior)), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n        keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n\n    NameError: Exception encountered when calling layer \"tab_net_classifier_3\" (type TabNetClassifier).\n    \n    in user code:\n    \n        File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/1448419760.py\", line 50, in call  *\n            prediction, _, _ = self.forward(X)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/1448419760.py\", line 44, in forward  *\n            output, encoded, importance = self.encoder(X)\n        File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filevqecsale.py\", line 17, in tf__call\n            (_, masked, keys) = ag__.converted_call(ag__.ld(self).initial_step, (ag__.ld(B), ag__.ld(shared), ag__.ld(prior)), dict(training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n            keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n            retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n    \n        NameError: Exception encountered when calling layer \"tab_net_encoder_3\" (type TabNetEncoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/52959058.py\", line 46, in call  *\n                _, masked, keys = self.initial_step(B, shared, prior, training=training)\n            File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n                keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n            File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n                retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n        \n            NameError: Exception encountered when calling layer \"tab_net_step\" (type TabNetStep).\n            \n            in user code:\n            \n                File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/828386580.py\", line 23, in call  *\n                    keys = self.attention(split, priors, training=training)\n                File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n                    retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n            \n                NameError: Exception encountered when calling layer \"attentive_transformer\" (type AttentiveTransformer).\n                \n                in user code:\n                \n                    File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/525209892.py\", line 26, in call  *\n                        return tfa.activations.sparsemax(output)\n                \n                    NameError: name 'tfa' is not defined\n                \n                \n                Call arguments received by layer \"attentive_transformer\" (type AttentiveTransformer):\n                   inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n                   priors=tf.Tensor(shape=(61,), dtype=float32)\n                   training=True\n            \n            \n            Call arguments received by layer \"tab_net_step\" (type TabNetStep):\n               inputs=tf.Tensor(shape=(None, 61), dtype=float32)\n               shared=tf.Tensor(shape=(None, 2), dtype=float32)\n               priors=tf.Tensor(shape=(61,), dtype=float32)\n               training=True\n        \n        \n        Call arguments received by layer \"tab_net_encoder_3\" (type TabNetEncoder):\n           X=tf.Tensor(shape=(None, 61), dtype=float32)\n           training=True\n    \n    \n    Call arguments received by layer \"tab_net_classifier_3\" (type TabNetClassifier):\n       X={'1488-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '4079-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1299-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '21003-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1160-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1438-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '4080-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1458-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1528-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1319-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '845-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1289-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1309-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30850-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30780-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30690-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30790-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23101-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23099-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '48-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23100-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30710-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30760-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30640-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30750-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '49-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30770-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30740-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30630-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30870-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '21001-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1418-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1329-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1220-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1428-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1249-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1349-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1369-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '20117-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2100-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2654-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1339-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '21000-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2050-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1408-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1200-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1538-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '31-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '6138-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1359-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1389-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1478-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2090-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1508-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1379-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '6142-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1468-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1548-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1239-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1448-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', 'hypertension': 'tf.Tensor(shape=(None,), dtype=int32)'}\n       training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21548/3459827668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTabNetClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvirtual_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefime2xxa.py\u001b[0m in \u001b[0;36mtf__call\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileoqzqbhwa.py\u001b[0m in \u001b[0;36mtf__forward\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportance\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevqecsale.py\u001b[0m in \u001b[0;36mtf__call\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mshared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_block\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\u001b[0m in \u001b[0;36mtf__call\u001b[1;34m(self, inputs, shared, priors, training)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0msplit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mmasked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\u001b[0m in \u001b[0;36mtf__call\u001b[1;34m(self, inputs, priors, training)\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparsemax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filefime2xxa.py\", line 10, in tf__call\n        (prediction, _, _) = ag__.converted_call(ag__.ld(self).forward, (ag__.ld(X),), None, fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_fileoqzqbhwa.py\", line 11, in tf__forward\n        (output, encoded, importance) = ag__.converted_call(ag__.ld(self).encoder, (ag__.ld(X),), None, fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filevqecsale.py\", line 17, in tf__call\n        (_, masked, keys) = ag__.converted_call(ag__.ld(self).initial_step, (ag__.ld(B), ag__.ld(shared), ag__.ld(prior)), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n        keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n\n    NameError: Exception encountered when calling layer \"tab_net_classifier_3\" (type TabNetClassifier).\n    \n    in user code:\n    \n        File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/1448419760.py\", line 50, in call  *\n            prediction, _, _ = self.forward(X)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/1448419760.py\", line 44, in forward  *\n            output, encoded, importance = self.encoder(X)\n        File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filevqecsale.py\", line 17, in tf__call\n            (_, masked, keys) = ag__.converted_call(ag__.ld(self).initial_step, (ag__.ld(B), ag__.ld(shared), ag__.ld(prior)), dict(training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n            keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n            retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n    \n        NameError: Exception encountered when calling layer \"tab_net_encoder_3\" (type TabNetEncoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/52959058.py\", line 46, in call  *\n                _, masked, keys = self.initial_step(B, shared, prior, training=training)\n            File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_file__tv5qyc.py\", line 11, in tf__call\n                keys = ag__.converted_call(ag__.ld(self).attention, (ag__.ld(split), ag__.ld(priors)), dict(training=ag__.ld(training)), fscope)\n            File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n                retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n        \n            NameError: Exception encountered when calling layer \"tab_net_step\" (type TabNetStep).\n            \n            in user code:\n            \n                File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/828386580.py\", line 23, in call  *\n                    keys = self.attention(split, priors, training=training)\n                File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\anali\\AppData\\Local\\Temp\\__autograph_generated_filetda3k_81.py\", line 30, in tf__call\n                    retval_ = ag__.converted_call(ag__.ld(tfa).activations.sparsemax, (ag__.ld(output),), None, fscope)\n            \n                NameError: Exception encountered when calling layer \"attentive_transformer\" (type AttentiveTransformer).\n                \n                in user code:\n                \n                    File \"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_21548/525209892.py\", line 26, in call  *\n                        return tfa.activations.sparsemax(output)\n                \n                    NameError: name 'tfa' is not defined\n                \n                \n                Call arguments received by layer \"attentive_transformer\" (type AttentiveTransformer):\n                   inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n                   priors=tf.Tensor(shape=(61,), dtype=float32)\n                   training=True\n            \n            \n            Call arguments received by layer \"tab_net_step\" (type TabNetStep):\n               inputs=tf.Tensor(shape=(None, 61), dtype=float32)\n               shared=tf.Tensor(shape=(None, 2), dtype=float32)\n               priors=tf.Tensor(shape=(61,), dtype=float32)\n               training=True\n        \n        \n        Call arguments received by layer \"tab_net_encoder_3\" (type TabNetEncoder):\n           X=tf.Tensor(shape=(None, 61), dtype=float32)\n           training=True\n    \n    \n    Call arguments received by layer \"tab_net_classifier_3\" (type TabNetClassifier):\n       X={'1488-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '4079-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1299-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '21003-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1160-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1438-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '4080-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1458-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1528-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1319-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '845-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1289-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1309-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30850-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30780-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30690-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30790-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23101-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23099-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '48-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '23100-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30710-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30760-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30640-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30750-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '49-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30770-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30740-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30630-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '30870-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '21001-0.0': 'tf.Tensor(shape=(None,), dtype=float32)', '1418-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1329-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1220-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1428-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1249-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1349-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1369-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '20117-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2100-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2654-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1339-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '21000-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2050-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1408-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1200-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1538-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '31-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '6138-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1359-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1389-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1478-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '2090-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1508-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1379-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '6142-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1468-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1548-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1239-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', '1448-0.0': 'tf.Tensor(shape=(None,), dtype=int32)', 'hypertension': 'tf.Tensor(shape=(None,), dtype=int32)'}\n       training=True\n"
     ]
    }
   ],
   "source": [
    "m = TabNetClassifier(outputs=1, n_steps=3, n_features = 2, feature_column=feature_column, virtual_batch_size=250)\n",
    "m.compile(tf.keras.optimizers.Adam(learning_rate=0.025), tf.keras.losses.binary_crossentropy)\n",
    "m.fit(train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f23d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
