{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0f0353",
   "metadata": {
    "id": "9d0f0353"
   },
   "outputs": [],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "import global_variables as gv\n",
    "import utilities\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88b1262",
   "metadata": {
    "id": "d88b1262"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Concatenate\n",
    "from tensorflow.keras.optimizers  import Adam, Adagrad, SGD\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b3c88",
   "metadata": {
    "id": "440b3c88"
   },
   "outputs": [],
   "source": [
    "# Importing from Scikit-Learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "from keras.layers import Input, Embedding, Reshape, GlobalAveragePooling1D\n",
    "from keras.layers import Flatten, concatenate, Concatenate, Lambda, Dropout, SpatialDropout1D\n",
    "from keras.layers import Activation, LeakyReLU\n",
    "from keras.models import Model, load_model\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55b3cd",
   "metadata": {
    "id": "ed55b3cd",
    "outputId": "ff1a7971-259a-4ca7-ddb8-2863b3343ef7"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce03cbb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "ce03cbb9",
    "outputId": "a7a94335-a7bd-4c45-9f4d-587720233d66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30850-0.0</th>\n",
       "      <th>30780-0.0</th>\n",
       "      <th>30690-0.0</th>\n",
       "      <th>30790-0.0</th>\n",
       "      <th>23101-0.0</th>\n",
       "      <th>23099-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>23100-0.0</th>\n",
       "      <th>30710-0.0</th>\n",
       "      <th>30760-0.0</th>\n",
       "      <th>30640-0.0</th>\n",
       "      <th>30750-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>30770-0.0</th>\n",
       "      <th>30740-0.0</th>\n",
       "      <th>30630-0.0</th>\n",
       "      <th>30870-0.0</th>\n",
       "      <th>21001-0.0</th>\n",
       "      <th>1488-0.0</th>\n",
       "      <th>4079-0.0</th>\n",
       "      <th>1299-0.0</th>\n",
       "      <th>21003-0.0</th>\n",
       "      <th>1160-0.0</th>\n",
       "      <th>1438-0.0</th>\n",
       "      <th>4080-0.0</th>\n",
       "      <th>1458-0.0</th>\n",
       "      <th>1528-0.0</th>\n",
       "      <th>1319-0.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>1289-0.0</th>\n",
       "      <th>1309-0.0</th>\n",
       "      <th>1418-0.0</th>\n",
       "      <th>1329-0.0</th>\n",
       "      <th>1220-0.0</th>\n",
       "      <th>1428-0.0</th>\n",
       "      <th>1249-0.0</th>\n",
       "      <th>1349-0.0</th>\n",
       "      <th>1369-0.0</th>\n",
       "      <th>20117-0.0</th>\n",
       "      <th>2100-0.0</th>\n",
       "      <th>2654-0.0</th>\n",
       "      <th>1339-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>2050-0.0</th>\n",
       "      <th>1408-0.0</th>\n",
       "      <th>1200-0.0</th>\n",
       "      <th>1538-0.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>1359-0.0</th>\n",
       "      <th>1389-0.0</th>\n",
       "      <th>1478-0.0</th>\n",
       "      <th>2090-0.0</th>\n",
       "      <th>1508-0.0</th>\n",
       "      <th>1379-0.0</th>\n",
       "      <th>6142-0.0</th>\n",
       "      <th>1468-0.0</th>\n",
       "      <th>1548-0.0</th>\n",
       "      <th>1239-0.0</th>\n",
       "      <th>1448-0.0</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>outcome_cardiomyopathies</th>\n",
       "      <th>outcome_ischemic_heart_disease</th>\n",
       "      <th>outcome_heart_failure</th>\n",
       "      <th>outcome_myocardial_infarction</th>\n",
       "      <th>outcome_peripheral_vascular_disease</th>\n",
       "      <th>outcome_cardiac_arrest</th>\n",
       "      <th>outcome_cerebral_infarction</th>\n",
       "      <th>outcome_arrhythmia</th>\n",
       "      <th>multi-labels</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50800</td>\n",
       "      <td>3.88800</td>\n",
       "      <td>6.47700</td>\n",
       "      <td>65.1984</td>\n",
       "      <td>45.2</td>\n",
       "      <td>35.6</td>\n",
       "      <td>74.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.70600</td>\n",
       "      <td>1.21100</td>\n",
       "      <td>35.065</td>\n",
       "      <td>102.0</td>\n",
       "      <td>26.339</td>\n",
       "      <td>5.62200</td>\n",
       "      <td>1.59300</td>\n",
       "      <td>0.97700</td>\n",
       "      <td>24.5790</td>\n",
       "      <td>6.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.52</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.08800</td>\n",
       "      <td>3.52000</td>\n",
       "      <td>5.51200</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>74.6</td>\n",
       "      <td>36.5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.17300</td>\n",
       "      <td>1.01900</td>\n",
       "      <td>40.900</td>\n",
       "      <td>113.0</td>\n",
       "      <td>10.701</td>\n",
       "      <td>5.05200</td>\n",
       "      <td>1.39000</td>\n",
       "      <td>2.35800</td>\n",
       "      <td>35.0861</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>65</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.73364</td>\n",
       "      <td>4.10892</td>\n",
       "      <td>6.47949</td>\n",
       "      <td>50.8588</td>\n",
       "      <td>71.7</td>\n",
       "      <td>29.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.58546</td>\n",
       "      <td>1.22432</td>\n",
       "      <td>84.100</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.763</td>\n",
       "      <td>13.71763</td>\n",
       "      <td>1.74423</td>\n",
       "      <td>2.78764</td>\n",
       "      <td>30.7934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 1, 1]</td>\n",
       "      <td>55</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.78800</td>\n",
       "      <td>2.88700</td>\n",
       "      <td>5.56500</td>\n",
       "      <td>56.5183</td>\n",
       "      <td>40.2</td>\n",
       "      <td>29.8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.11500</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>36.400</td>\n",
       "      <td>91.0</td>\n",
       "      <td>31.672</td>\n",
       "      <td>4.82700</td>\n",
       "      <td>1.89100</td>\n",
       "      <td>1.15700</td>\n",
       "      <td>20.7577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>Irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75600</td>\n",
       "      <td>2.67000</td>\n",
       "      <td>4.68000</td>\n",
       "      <td>4.7700</td>\n",
       "      <td>46.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.49300</td>\n",
       "      <td>0.73300</td>\n",
       "      <td>34.200</td>\n",
       "      <td>105.0</td>\n",
       "      <td>42.209</td>\n",
       "      <td>5.06300</td>\n",
       "      <td>1.86900</td>\n",
       "      <td>1.67700</td>\n",
       "      <td>25.9766</td>\n",
       "      <td>7.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   30850-0.0  30780-0.0  30690-0.0  30790-0.0  23101-0.0  23099-0.0  48-0.0  \\\n",
       "0    0.50800    3.88800    6.47700    65.1984       45.2       35.6    74.0   \n",
       "1   13.08800    3.52000    5.51200    15.4000       74.6       36.5   120.0   \n",
       "2    9.73364    4.10892    6.47949    50.8588       71.7       29.7   112.0   \n",
       "3    1.78800    2.88700    5.56500    56.5183       40.2       29.8    67.0   \n",
       "4    0.75600    2.67000    4.68000     4.7700       46.5       30.1    85.0   \n",
       "\n",
       "   23100-0.0  30710-0.0  30760-0.0  30640-0.0  30750-0.0  49-0.0  30770-0.0  \\\n",
       "0       25.0       0.34    1.70600    1.21100     35.065   102.0     26.339   \n",
       "1       42.9       3.94    1.17300    1.01900     40.900   113.0     10.701   \n",
       "2       30.3       3.88    1.58546    1.22432     84.100   107.0     18.763   \n",
       "3       17.0       0.87    2.11500    0.81000     36.400    91.0     31.672   \n",
       "4       20.0       0.18    1.49300    0.73300     34.200   105.0     42.209   \n",
       "\n",
       "   30740-0.0  30630-0.0  30870-0.0  21001-0.0  1488-0.0  4079-0.0  1299-0.0  \\\n",
       "0    5.62200    1.59300    0.97700    24.5790       6.0      77.0      10.0   \n",
       "1    5.05200    1.39000    2.35800    35.0861       2.0      91.0       2.0   \n",
       "2   13.71763    1.74423    2.78764    30.7934       0.0      99.0       2.0   \n",
       "3    4.82700    1.89100    1.15700    20.7577       0.0      71.0       5.0   \n",
       "4    5.06300    1.86900    1.67700    25.9766       7.0      73.0       4.0   \n",
       "\n",
       "   21003-0.0  1160-0.0  1438-0.0  4080-0.0  1458-0.0  1528-0.0  1319-0.0  \\\n",
       "0       54.0       7.0      10.0     110.0      3.73       2.0       0.0   \n",
       "1       65.0       9.0      12.0     166.0      7.00       2.4       0.0   \n",
       "2       55.0       7.0      10.0     135.0      7.00       2.0       0.0   \n",
       "3       49.0       8.0      14.0     116.0      5.00       3.0       1.0   \n",
       "4       61.0       7.0       2.0     113.0      7.00       4.0       2.0   \n",
       "\n",
       "   845-0.0  1289-0.0  1309-0.0  1418-0.0  1329-0.0  1220-0.0  1428-0.0  \\\n",
       "0    23.52       6.0       2.0         3         2         0         0   \n",
       "1    16.00       2.0       1.0         2         2         0         1   \n",
       "2    21.00       3.0       1.0         2         1         0         0   \n",
       "3    18.00       5.0       1.0         2         2         0         0   \n",
       "4    16.00       3.0       3.0         3         2         1         1   \n",
       "\n",
       "   1249-0.0  1349-0.0  1369-0.0  20117-0.0  2100-0.0  2654-0.0  1339-0.0  \\\n",
       "0         1         1         1          2         1         6         2   \n",
       "1         1         4         2          2         0         7         2   \n",
       "2         1         2         1          2         0         7         2   \n",
       "3         4         1         2          2         0         7         2   \n",
       "4         4         1         1          2         0         7         3   \n",
       "\n",
       "   21000-0.0  2050-0.0  1408-0.0  1200-0.0  1538-0.0  31-0.0  6138-0.0  \\\n",
       "0          0         2         1         3         2       0         1   \n",
       "1          0         1         3         2         0       1         3   \n",
       "2          0         1         2         2         1       1         3   \n",
       "3          2         1         2         1         2       0         6   \n",
       "4          0         1         3         1         0       0         3   \n",
       "\n",
       "   1359-0.0  1389-0.0  1478-0.0  2090-0.0  1508-0.0  1379-0.0  6142-0.0  \\\n",
       "0         2         1         1         1         3         1         1   \n",
       "1         3         1         1         0         2         2         1   \n",
       "2         3         2         1         0         2         2         1   \n",
       "3         2         2         1         0         2         2         1   \n",
       "4         3         1         2         0         1         1         1   \n",
       "\n",
       "   1468-0.0  1548-0.0  1239-0.0  1448-0.0  hypertension  \\\n",
       "0         3         2         0         3             0   \n",
       "1         5         2         0         1             1   \n",
       "2         4         2         0         3             1   \n",
       "3         3         2         0         3             0   \n",
       "4         4         2         0         3             1   \n",
       "\n",
       "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
       "0                         0                               0   \n",
       "1                         0                               1   \n",
       "2                         0                               1   \n",
       "3                         0                               0   \n",
       "4                         0                               0   \n",
       "\n",
       "   outcome_heart_failure  outcome_myocardial_infarction  \\\n",
       "0                      0                              0   \n",
       "1                      0                              1   \n",
       "2                      0                              0   \n",
       "3                      0                              0   \n",
       "4                      0                              1   \n",
       "\n",
       "   outcome_peripheral_vascular_disease  outcome_cardiac_arrest  \\\n",
       "0                                    0                       0   \n",
       "1                                    0                       0   \n",
       "2                                    0                       1   \n",
       "3                                    0                       0   \n",
       "4                                    0                       0   \n",
       "\n",
       "   outcome_cerebral_infarction  outcome_arrhythmia              multi-labels  \\\n",
       "0                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "1                            0                   0  [1, 0, 1, 0, 0, 0, 0, 0]   \n",
       "2                            1                   1  [0, 0, 1, 0, 0, 1, 1, 1]   \n",
       "3                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "4                            0                   0  [1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   age  gender     race  \n",
       "0   54  Female  British  \n",
       "1   65    Male  British  \n",
       "2   55    Male  British  \n",
       "3   49  Female    Irish  \n",
       "4   61  Female  British  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(gv.data_link)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27ac1",
   "metadata": {
    "id": "79f27ac1"
   },
   "source": [
    "### Build MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9d66d",
   "metadata": {
    "id": "1bd9d66d"
   },
   "outputs": [],
   "source": [
    "# METRICS = [\n",
    "#       keras.metrics.TruePositives(name='tp'),\n",
    "#       keras.metrics.FalsePositives(name='fp'),\n",
    "#       keras.metrics.TrueNegatives(name='tn'),\n",
    "#       keras.metrics.FalseNegatives(name='fn'), \n",
    "#       keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#       keras.metrics.Precision(name='precision'),\n",
    "#       keras.metrics.Recall(name='recall'),\n",
    "#       keras.metrics.AUC(name='auc'),\n",
    "#       keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f83cd",
   "metadata": {
    "id": "617f83cd"
   },
   "source": [
    "#### optimize number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5bf9b2",
   "metadata": {
    "id": "5d5bf9b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_nodes(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation=tf.keras.activations.gelu , input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(n_nodes, activation=tf.keras.activations.gelu ))\n",
    "    model.add(Dense(n_nodes, activation=tf.keras.activations.gelu ))\n",
    "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=0.000001),\n",
    "        metrics=['acc',utilities.f1_m,utilities.precision_m, utilities.recall_m])\n",
    "\n",
    "    # fit model on train set\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bgpit_ZemX1V",
   "metadata": {
    "id": "bgpit_ZemX1V"
   },
   "outputs": [],
   "source": [
    "def evaluate_layers(n_layers, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation=tf.keras.activations.gelu , input_shape=(X_train.shape[1],)))\n",
    "    for _ in range(1,n_layers):\n",
    "        model.add(Dense(50, activation=tf.keras.activations.gelu ))\n",
    "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=0.000001),\n",
    "        metrics=['acc',utilities.f1_m,utilities.precision_m, utilities.recall_m])\n",
    "\n",
    "    # fit model on train set\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cc2f2",
   "metadata": {
    "id": "9b6cc2f2"
   },
   "source": [
    "#### test range of input nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9dae95",
   "metadata": {
    "id": "cf9dae95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "112/112 [==============================] - 6s 19ms/step - loss: 0.6965 - acc: 0.4980 - f1_m: 0.6397 - precision_m: 0.4984 - recall_m: 0.8942 - val_loss: 0.6936 - val_acc: 0.5160 - val_f1_m: 0.6580 - val_precision_m: 0.5241 - val_recall_m: 0.8849\n",
      "Epoch 2/400\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.6962 - acc: 0.4972 - f1_m: 0.6369 - precision_m: 0.4984 - recall_m: 0.8832 - val_loss: 0.6935 - val_acc: 0.5150 - val_f1_m: 0.6548 - val_precision_m: 0.5239 - val_recall_m: 0.8742\n",
      "Epoch 3/400\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.6961 - acc: 0.4974 - f1_m: 0.6341 - precision_m: 0.4988 - recall_m: 0.8715 - val_loss: 0.6935 - val_acc: 0.5142 - val_f1_m: 0.6514 - val_precision_m: 0.5237 - val_recall_m: 0.8625\n",
      "Epoch 4/400\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.6959 - acc: 0.4969 - f1_m: 0.6308 - precision_m: 0.4987 - recall_m: 0.8594 - val_loss: 0.6934 - val_acc: 0.5145 - val_f1_m: 0.6489 - val_precision_m: 0.5242 - val_recall_m: 0.8527\n",
      "Epoch 5/400\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.6957 - acc: 0.4972 - f1_m: 0.6275 - precision_m: 0.4984 - recall_m: 0.8485 - val_loss: 0.6934 - val_acc: 0.5120 - val_f1_m: 0.6442 - val_precision_m: 0.5230 - val_recall_m: 0.8394\n",
      "Epoch 6/400\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.6956 - acc: 0.4980 - f1_m: 0.6250 - precision_m: 0.4994 - recall_m: 0.8366 - val_loss: 0.6933 - val_acc: 0.5098 - val_f1_m: 0.6390 - val_precision_m: 0.5221 - val_recall_m: 0.8245\n",
      "Epoch 7/400\n",
      " 99/112 [=========================>....] - ETA: 0s - loss: 0.6956 - acc: 0.4981 - f1_m: 0.6205 - precision_m: 0.4978 - recall_m: 0.8250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28908/283384018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_nodes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# evaluate model with a given number of nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msave_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msave_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28908/220400949.py\u001b[0m in \u001b[0;36mevaluate_nodes\u001b[1;34m(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# fit model on train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_nodes = [10, 25, 50, 100, 150, 200]\n",
    "epochs = 400\n",
    "batch = 500\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
    "X_train, y_train= utilities.resample_data(X_train, y_train, 'under')\n",
    "\n",
    "save_history = pd.DataFrame()\n",
    "for n_nodes in num_nodes:\n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = evaluate_nodes(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\n",
    "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
    "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
    "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
    "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle('Number of Nodes: '+str(n_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kJOOk6B5ZxXS",
   "metadata": {
    "id": "kJOOk6B5ZxXS"
   },
   "outputs": [],
   "source": [
    "def plot_compare(data, metric):\n",
    "  labels = []\n",
    "  for i in range(data.shape[1]):\n",
    "    sns.lineplot(x=data.index, y=data.loc[:,data.columns[i-1]], data=data)\n",
    "    plt.title(metric+' for all num_nodes')\n",
    "    labels.append(str(data.columns[i-1]))\n",
    "  plt.legend(title=metric, loc='best', labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3HVZFtPQcE-B",
   "metadata": {
    "id": "3HVZFtPQcE-B"
   },
   "outputs": [],
   "source": [
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[::4]], 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dQxjf1IXvO_C",
   "metadata": {
    "id": "dQxjf1IXvO_C"
   },
   "outputs": [],
   "source": [
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[1::4]], 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zz0p182tzyHR",
   "metadata": {
    "id": "zz0p182tzyHR"
   },
   "outputs": [],
   "source": [
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[2::4]], 'F1-Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XhRlCOvtzyRo",
   "metadata": {
    "id": "XhRlCOvtzyRo"
   },
   "outputs": [],
   "source": [
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XB4Rv9251GnD",
   "metadata": {
    "id": "XB4Rv9251GnD"
   },
   "outputs": [],
   "source": [
    "num_layers = [3, 4, 5]\n",
    "epochs = 400\n",
    "batch = 500\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
    "X_train, y_train= utilities.resample_data(X_train, y_train, 'under')\n",
    "\n",
    "save_history = pd.DataFrame()\n",
    "for n_layers in num_layers:\n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = evaluate_layers(n_layers, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\n",
    "    save_history['loss_'+str(n_layers)]=history.history[\"loss\"]\n",
    "    save_history['acc_'+str(n_layers)]=history.history[\"acc\"]\n",
    "    save_history['f1'+str(n_layers)]=history.history[\"f1_m\"]\n",
    "    save_history['recall'+str(n_layers)]=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_layers, result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle('Number of Layers: '+str(n_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e3458",
   "metadata": {
    "id": "2a0e3458"
   },
   "source": [
    "### Build Model after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009fde5",
   "metadata": {
    "id": "0009fde5"
   },
   "outputs": [],
   "source": [
    "def mlp_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='tanh',opt=SGD, lr=0.000001):\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, activation=activation , input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(500, activation=activation))\n",
    "    model.add(Dense(500, activation=activation ))\n",
    "    model.add(Dense(200, activation=activation ))\n",
    "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=opt(learning_rate=lr),\n",
    "        metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "    # fit model on train set\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad05a1b",
   "metadata": {
    "id": "6ad05a1b"
   },
   "outputs": [],
   "source": [
    "epochs =300\n",
    "batch = 400\n",
    "sample_methods =['ADASYN', 'over', 'under', 'partial_under']\n",
    "activations=['relu', 'tanh', tf.keras.activations.gelu]\n",
    "optimizers = [SGD, Adam, Adagrad]\n",
    "num_transformers = [StandardScaler(), MinMaxScaler(), QuantileTransformer(output_distribution='uniform')]\n",
    "# test one_hot==True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51f06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e51f06c",
    "outputId": "c55f6aaf-a2ac-4304-c726-b8f3c2a700f5"
   },
   "outputs": [],
   "source": [
    "# Test numerical transformers\n",
    "save_history = pd.DataFrame()\n",
    "for param in num_transformers:\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia', param, one_hot=True)\n",
    "    X_train, y_train= utilities.resample_data(X_train, y_train, 'under')\n",
    "    \n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = mlp_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='tanh',opt=SGD, lr=0.000001)\n",
    "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
    "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
    "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
    "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle(str(param))\n",
    "\n",
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a7317",
   "metadata": {
    "id": "404a7317"
   },
   "outputs": [],
   "source": [
    "# Test sample methods\n",
    "save_history = pd.DataFrame()\n",
    "for param in sample_methods:\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia')\n",
    "    X_train, y_train= utilities.resample_data(X_train, y_train, param)\n",
    "    \n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = mlp_model(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch,opt=SGD, lr=0.000001)\n",
    "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
    "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
    "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
    "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle(str(param))\n",
    "\n",
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84752a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f84752a4",
    "outputId": "8bf77509-7036-4d5a-c909-0474e9422d75"
   },
   "outputs": [],
   "source": [
    "# Test activations\n",
    "activations = ['tanh']\n",
    "save_history = pd.DataFrame()\n",
    "for param in activations:\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia')\n",
    "    X_train, y_train= utilities.resample_data(X_train, y_train, 'under')\n",
    "    \n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = mlp_model(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation=param,opt=SGD, lr=0.0001) #lr = 0.0002\n",
    "    save_history['loss_']=history.history[\"loss\"]\n",
    "    save_history['acc_']=history.history[\"acc\"]\n",
    "    save_history['f1']=history.history[\"f1_m\"]\n",
    "    save_history['recall']=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle(str(param))\n",
    "\n",
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a1880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "580a1880",
    "outputId": "07f27940-6b33-434f-e547-24e0fa82ec50"
   },
   "outputs": [],
   "source": [
    "# Test optimizers\n",
    "\n",
    "optimizers =[Adam]\n",
    "save_history = pd.DataFrame()\n",
    "for param in optimizers:\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utilities.process_features(df, 'outcome_arrhythmia')\n",
    "    X_train, y_train= utilities.resample_data(X_train, y_train, 'under')\n",
    "    \n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = mlp_model(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='tanh',opt=param, lr=0.000002)\n",
    "    save_history['loss_']=history.history[\"loss\"]\n",
    "    save_history['acc_']=history.history[\"acc\"]\n",
    "    save_history['f1']=history.history[\"f1_m\"]\n",
    "    save_history['recall']=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    utilities.plot_history(history)\n",
    "    plt.suptitle(str(param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5FnEICHArGz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5FnEICHArGz",
    "outputId": "3c8e5c09-4ca3-4484-99b8-cfeb1b37e304"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-XAdsqrw8HZ",
   "metadata": {
    "id": "m-XAdsqrw8HZ"
   },
   "source": [
    "#### Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zNwMETydw7Ay",
   "metadata": {
    "id": "zNwMETydw7Ay"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dAaElnGDpvzg",
   "metadata": {
    "id": "dAaElnGDpvzg"
   },
   "outputs": [],
   "source": [
    "# Test opt opt params\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-6, some_changing_thing()]\n",
    "beta1, beta2 = [1,2,3,4], [1,2,3,4]\n",
    "save_history = pd.DataFrame()\n",
    "for param in optimizers:\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
    "    X_train, y_train= resample_data(X_train, y_train, 'under')\n",
    "    \n",
    "    # evaluate model with a given number of nodes\n",
    "    history, result = basic_model(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='relu',opt=param, lr=0.000001)\n",
    "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
    "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
    "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
    "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
    "\n",
    "\n",
    "    # summarize final test set accuracy\n",
    "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
    "    \n",
    "    # plot learning curves\n",
    "    plot_history(history)\n",
    "    plt.suptitle(str(param))\n",
    "\n",
    "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7132839",
   "metadata": {
    "id": "b7132839"
   },
   "source": [
    "### Build MLP Model with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f78ee8",
   "metadata": {
    "id": "32f78ee8"
   },
   "outputs": [],
   "source": [
    "def build_categorical_inputs(features):\n",
    "\n",
    "    initial_inputs = {}\n",
    "    cat_input_layers={}\n",
    "    \n",
    "    train_test_cat_features = pd.concat([X_train[categorical_cols], X_test[categorical_cols]])\n",
    "    \n",
    "    for feature in features:\n",
    "        no_of_unique_cats  = train_test_cat_features[feature].nunique()\n",
    "        embedding_size = int(min(np.ceil((no_of_unique_cats)/2), 50))\n",
    "        categories  = no_of_unique_cats + 1\n",
    "\n",
    "        initial_inputs[feature] = Input(shape=(1,))\n",
    "        embedding_layer = Embedding(categories, \n",
    "                                    embedding_size,\n",
    "                                    embeddings_regularizer=regularizers.l2(0.01),\n",
    "                                    input_length=1)(initial_inputs[feature])\n",
    "        cat_input_layers[feature] = Reshape(target_shape=(embedding_size,))(embedding_layer)\n",
    "\n",
    "    return initial_inputs, cat_input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a231b6",
   "metadata": {
    "id": "58a231b6"
   },
   "outputs": [],
   "source": [
    "def build_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
    "    \n",
    "    models = []\n",
    "    for categorical_var in categorical_cols :\n",
    "        model = Sequential()\n",
    "        no_of_unique_cat  = X_train[categorical_var].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab  = no_of_unique_cat+1\n",
    "        model.add( Embedding(vocab ,embedding_size, input_length = 1 ))\n",
    "        model.add(Reshape(target_shape=(embedding_size,)))\n",
    "        models.append( model )\n",
    "        \n",
    "    model_rest = Sequential()\n",
    "    model_rest.add(Dense(16, input_dim= X_train[numerical_cols+continuous_cols].shape[1]))\n",
    "    models.append(model_rest)\n",
    "\n",
    "    full_model = Sequential()\n",
    "    full_model.add(Concatenate(models))\n",
    "    full_model.add(Dense(1000))\n",
    "    full_model.add(Activation('relu'))\n",
    "    full_model.add(Dense(400))\n",
    "    full_model.add(Activation('relu'))\n",
    "    full_model.add(Dense(1))\n",
    "    full_model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=0.000001),\n",
    "        metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "    # fit model on train set\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df20d86",
   "metadata": {
    "id": "7df20d86"
   },
   "outputs": [],
   "source": [
    "def build_model( X_train, X_val, X_test ,y_train, y_val, y_test, epochs, batch):\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    for col in categorical_cols:\n",
    "        no_of_unique_cat  = X_train[col].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab  = no_of_unique_cat+1\n",
    "        model.add(Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation=tf.keras.activations.gelu ))\n",
    "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=0.000001),\n",
    "        metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "    # fit model on train set\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504bd9e",
   "metadata": {
    "id": "7504bd9e",
    "outputId": "461b0328-9c50-49ca-c506-82ac05ebb67c"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=False)\n",
    "X_train, y_train= resample_data(X_train, y_train, 'under')\n",
    "model=build_model(X_train, X_val, X_test ,y_train, y_val, y_test, 400, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac49e1",
   "metadata": {
    "id": "1fac49e1"
   },
   "outputs": [],
   "source": [
    "initial_inputs, input_layers = build_categorical_inputs(categorical_cols)\n",
    "\n",
    "no_of_num_features = len(X_train.columns) - len(categorical_cols)\n",
    "\n",
    "initial_inputs['numerical_features'] = Input(shape=(no_of_num_features,))\n",
    "input_layers['numerical_features'] = initial_inputs['numerical_features']\n",
    "\n",
    "inputs = Concatenate(axis=-1)([layer for layer in input_layers.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccdade",
   "metadata": {
    "id": "40ccdade"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "model_rest = Sequential()\n",
    "model_rest.add(Dense(100, input_dim= len(numerical_cols+continuous_cols) ))\n",
    "models.append(model_rest)\n",
    "\n",
    "for categorical_var in categorical_cols :\n",
    "     \n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = X_train[categorical_var].nunique()\n",
    "    \n",
    "    # jeremy howard rule\n",
    "    embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "    embedding_size = int(embedding_size)\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    model.add( Embedding(vocab ,embedding_size, input_length = 1 ))\n",
    "    model.add(Reshape(target_shape=(embedding_size,)))\n",
    "    models.append( model )\n",
    "\n",
    "\n",
    "full_model = Sequential()\n",
    "full_model.add(Concatenate(models))\n",
    "full_model.add(Dense(1000))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(400))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(200))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "full_model.add(Dense(1))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f57a0",
   "metadata": {
    "id": "987f57a0",
    "outputId": "d565f829-b10a-4bf0-eae3-9a584de3dd05"
   },
   "outputs": [],
   "source": [
    "batch=1000\n",
    "epochs=100\n",
    "# fit model on train set\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch,\n",
    "    epochs=epochs,\n",
    "#     shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val, y_val),\n",
    ")\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0276b8",
   "metadata": {
    "id": "6a0276b8"
   },
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b62e0",
   "metadata": {
    "id": "436b62e0"
   },
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((input_train, input_test), axis=0)\n",
    "targets = np.concatenate((target_train, target_test), axis=0)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss_function,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c9624",
   "metadata": {
    "id": "779c9624"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd4df5",
   "metadata": {
    "id": "00dd4df5"
   },
   "outputs": [],
   "source": [
    "Model.save(\n",
    "    saved_models/'model_'+str(num)+'.h5',\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None,\n",
    "    save_traces=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "02-basic_model.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
