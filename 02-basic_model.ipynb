{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0f0353",
      "metadata": {
        "id": "9d0f0353"
      },
      "outputs": [],
      "source": [
        "# Importing core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import pprint\n",
        "import joblib\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Data transformation pipelines\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import QuantileTransformer, RobustScaler, StandardScaler,MinMaxScaler\n",
        "\n",
        "# Graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88b1262",
      "metadata": {
        "id": "d88b1262"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, PReLU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Concatenate\n",
        "from tensorflow.keras.optimizers  import Adam, Adagrad, SGD\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440b3c88",
      "metadata": {
        "id": "440b3c88"
      },
      "outputs": [],
      "source": [
        "# Importing from Scikit-Learn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, FactorAnalysis\n",
        "\n",
        "from keras.layers import Input, Embedding, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import Flatten, concatenate, Concatenate, Lambda, Dropout, SpatialDropout1D\n",
        "from keras.layers import Activation, LeakyReLU\n",
        "from keras.models import Model, load_model\n",
        "from keras.losses import binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed55b3cd",
      "metadata": {
        "id": "ed55b3cd",
        "outputId": "ff1a7971-259a-4ca7-ddb8-2863b3343ef7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce03cbb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ce03cbb9",
        "outputId": "766532c6-8cf6-4819-f9d6-eb47587b48cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   30850-0.0  30780-0.0  30690-0.0  30790-0.0  23101-0.0  23099-0.0  48-0.0  \\\n",
              "0    0.50800    3.88800    6.47700    65.1984       45.2       35.6    74.0   \n",
              "1   13.08800    3.52000    5.51200    15.4000       74.6       36.5   120.0   \n",
              "2    9.73364    4.10892    6.47949    50.8588       71.7       29.7   112.0   \n",
              "3    1.78800    2.88700    5.56500    56.5183       40.2       29.8    67.0   \n",
              "4    0.75600    2.67000    4.68000     4.7700       46.5       30.1    85.0   \n",
              "\n",
              "   23100-0.0  30710-0.0  30760-0.0  30640-0.0  30750-0.0  49-0.0  30770-0.0  \\\n",
              "0       25.0       0.34    1.70600    1.21100     35.065   102.0     26.339   \n",
              "1       42.9       3.94    1.17300    1.01900     40.900   113.0     10.701   \n",
              "2       30.3       3.88    1.58546    1.22432     84.100   107.0     18.763   \n",
              "3       17.0       0.87    2.11500    0.81000     36.400    91.0     31.672   \n",
              "4       20.0       0.18    1.49300    0.73300     34.200   105.0     42.209   \n",
              "\n",
              "   30740-0.0  30630-0.0  30870-0.0  21001-0.0  1488-0.0  4079-0.0  1299-0.0  \\\n",
              "0    5.62200    1.59300    0.97700    24.5790       6.0      77.0      10.0   \n",
              "1    5.05200    1.39000    2.35800    35.0861       2.0      91.0       2.0   \n",
              "2   13.71763    1.74423    2.78764    30.7934       0.0      99.0       2.0   \n",
              "3    4.82700    1.89100    1.15700    20.7577       0.0      71.0       5.0   \n",
              "4    5.06300    1.86900    1.67700    25.9766       7.0      73.0       4.0   \n",
              "\n",
              "   21003-0.0  1160-0.0  1438-0.0  4080-0.0  1458-0.0  1528-0.0  1319-0.0  \\\n",
              "0       54.0       7.0      10.0     110.0      3.73       2.0       0.0   \n",
              "1       65.0       9.0      12.0     166.0      7.00       2.4       0.0   \n",
              "2       55.0       7.0      10.0     135.0      7.00       2.0       0.0   \n",
              "3       49.0       8.0      14.0     116.0      5.00       3.0       1.0   \n",
              "4       61.0       7.0       2.0     113.0      7.00       4.0       2.0   \n",
              "\n",
              "   845-0.0  1289-0.0  1309-0.0  1418-0.0  1329-0.0  1220-0.0  1428-0.0  \\\n",
              "0    23.52       6.0       2.0         3         2         0         0   \n",
              "1    16.00       2.0       1.0         2         2         0         1   \n",
              "2    21.00       3.0       1.0         2         1         0         0   \n",
              "3    18.00       5.0       1.0         2         2         0         0   \n",
              "4    16.00       3.0       3.0         3         2         1         1   \n",
              "\n",
              "   1249-0.0  1349-0.0  1369-0.0  20117-0.0  2100-0.0  2654-0.0  1339-0.0  \\\n",
              "0         1         1         1          2         1         6         2   \n",
              "1         1         4         2          2         0         7         2   \n",
              "2         1         2         1          2         0         7         2   \n",
              "3         4         1         2          2         0         7         2   \n",
              "4         4         1         1          2         0         7         3   \n",
              "\n",
              "   21000-0.0  2050-0.0  1408-0.0  1200-0.0  1538-0.0  31-0.0  6138-0.0  \\\n",
              "0          0         2         1         3         2       0         1   \n",
              "1          0         1         3         2         0       1         3   \n",
              "2          0         1         2         2         1       1         3   \n",
              "3          2         1         2         1         2       0         6   \n",
              "4          0         1         3         1         0       0         3   \n",
              "\n",
              "   1359-0.0  1389-0.0  1478-0.0  2090-0.0  1508-0.0  1379-0.0  6142-0.0  \\\n",
              "0         2         1         1         1         3         1         1   \n",
              "1         3         1         1         0         2         2         1   \n",
              "2         3         2         1         0         2         2         1   \n",
              "3         2         2         1         0         2         2         1   \n",
              "4         3         1         2         0         1         1         1   \n",
              "\n",
              "   1468-0.0  1548-0.0  1239-0.0  1448-0.0  hypertension  \\\n",
              "0         3         2         0         3             0   \n",
              "1         5         2         0         1             1   \n",
              "2         4         2         0         3             1   \n",
              "3         3         2         0         3             0   \n",
              "4         4         2         0         3             1   \n",
              "\n",
              "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
              "0                         0                               0   \n",
              "1                         0                               1   \n",
              "2                         0                               1   \n",
              "3                         0                               0   \n",
              "4                         0                               0   \n",
              "\n",
              "   outcome_heart_failure  outcome_myocardial_infarction  \\\n",
              "0                      0                              0   \n",
              "1                      0                              1   \n",
              "2                      0                              0   \n",
              "3                      0                              0   \n",
              "4                      0                              1   \n",
              "\n",
              "   outcome_peripheral_vascular_disease  outcome_cardiac_arrest  \\\n",
              "0                                    0                       0   \n",
              "1                                    0                       0   \n",
              "2                                    0                       1   \n",
              "3                                    0                       0   \n",
              "4                                    0                       0   \n",
              "\n",
              "   outcome_cerebral_infarction  outcome_arrhythmia              multi-labels  \\\n",
              "0                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
              "1                            0                   0  [1, 0, 1, 0, 0, 0, 0, 0]   \n",
              "2                            1                   1  [0, 0, 1, 0, 0, 1, 1, 1]   \n",
              "3                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
              "4                            0                   0  [1, 0, 0, 0, 0, 0, 0, 0]   \n",
              "\n",
              "   age  gender     race  \n",
              "0   54  Female  British  \n",
              "1   65    Male  British  \n",
              "2   55    Male  British  \n",
              "3   49  Female    Irish  \n",
              "4   61  Female  British  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7bee5a-5f2a-443b-8263-5ce0db0be578\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>30850-0.0</th>\n",
              "      <th>30780-0.0</th>\n",
              "      <th>30690-0.0</th>\n",
              "      <th>30790-0.0</th>\n",
              "      <th>23101-0.0</th>\n",
              "      <th>23099-0.0</th>\n",
              "      <th>48-0.0</th>\n",
              "      <th>23100-0.0</th>\n",
              "      <th>30710-0.0</th>\n",
              "      <th>30760-0.0</th>\n",
              "      <th>30640-0.0</th>\n",
              "      <th>30750-0.0</th>\n",
              "      <th>49-0.0</th>\n",
              "      <th>30770-0.0</th>\n",
              "      <th>30740-0.0</th>\n",
              "      <th>30630-0.0</th>\n",
              "      <th>30870-0.0</th>\n",
              "      <th>21001-0.0</th>\n",
              "      <th>1488-0.0</th>\n",
              "      <th>4079-0.0</th>\n",
              "      <th>1299-0.0</th>\n",
              "      <th>21003-0.0</th>\n",
              "      <th>1160-0.0</th>\n",
              "      <th>1438-0.0</th>\n",
              "      <th>4080-0.0</th>\n",
              "      <th>1458-0.0</th>\n",
              "      <th>1528-0.0</th>\n",
              "      <th>1319-0.0</th>\n",
              "      <th>845-0.0</th>\n",
              "      <th>1289-0.0</th>\n",
              "      <th>1309-0.0</th>\n",
              "      <th>1418-0.0</th>\n",
              "      <th>1329-0.0</th>\n",
              "      <th>1220-0.0</th>\n",
              "      <th>1428-0.0</th>\n",
              "      <th>1249-0.0</th>\n",
              "      <th>1349-0.0</th>\n",
              "      <th>1369-0.0</th>\n",
              "      <th>20117-0.0</th>\n",
              "      <th>2100-0.0</th>\n",
              "      <th>2654-0.0</th>\n",
              "      <th>1339-0.0</th>\n",
              "      <th>21000-0.0</th>\n",
              "      <th>2050-0.0</th>\n",
              "      <th>1408-0.0</th>\n",
              "      <th>1200-0.0</th>\n",
              "      <th>1538-0.0</th>\n",
              "      <th>31-0.0</th>\n",
              "      <th>6138-0.0</th>\n",
              "      <th>1359-0.0</th>\n",
              "      <th>1389-0.0</th>\n",
              "      <th>1478-0.0</th>\n",
              "      <th>2090-0.0</th>\n",
              "      <th>1508-0.0</th>\n",
              "      <th>1379-0.0</th>\n",
              "      <th>6142-0.0</th>\n",
              "      <th>1468-0.0</th>\n",
              "      <th>1548-0.0</th>\n",
              "      <th>1239-0.0</th>\n",
              "      <th>1448-0.0</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>outcome_cardiomyopathies</th>\n",
              "      <th>outcome_ischemic_heart_disease</th>\n",
              "      <th>outcome_heart_failure</th>\n",
              "      <th>outcome_myocardial_infarction</th>\n",
              "      <th>outcome_peripheral_vascular_disease</th>\n",
              "      <th>outcome_cardiac_arrest</th>\n",
              "      <th>outcome_cerebral_infarction</th>\n",
              "      <th>outcome_arrhythmia</th>\n",
              "      <th>multi-labels</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>race</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50800</td>\n",
              "      <td>3.88800</td>\n",
              "      <td>6.47700</td>\n",
              "      <td>65.1984</td>\n",
              "      <td>45.2</td>\n",
              "      <td>35.6</td>\n",
              "      <td>74.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.70600</td>\n",
              "      <td>1.21100</td>\n",
              "      <td>35.065</td>\n",
              "      <td>102.0</td>\n",
              "      <td>26.339</td>\n",
              "      <td>5.62200</td>\n",
              "      <td>1.59300</td>\n",
              "      <td>0.97700</td>\n",
              "      <td>24.5790</td>\n",
              "      <td>6.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>3.73</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.52</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>54</td>\n",
              "      <td>Female</td>\n",
              "      <td>British</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.08800</td>\n",
              "      <td>3.52000</td>\n",
              "      <td>5.51200</td>\n",
              "      <td>15.4000</td>\n",
              "      <td>74.6</td>\n",
              "      <td>36.5</td>\n",
              "      <td>120.0</td>\n",
              "      <td>42.9</td>\n",
              "      <td>3.94</td>\n",
              "      <td>1.17300</td>\n",
              "      <td>1.01900</td>\n",
              "      <td>40.900</td>\n",
              "      <td>113.0</td>\n",
              "      <td>10.701</td>\n",
              "      <td>5.05200</td>\n",
              "      <td>1.39000</td>\n",
              "      <td>2.35800</td>\n",
              "      <td>35.0861</td>\n",
              "      <td>2.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>7.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>65</td>\n",
              "      <td>Male</td>\n",
              "      <td>British</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.73364</td>\n",
              "      <td>4.10892</td>\n",
              "      <td>6.47949</td>\n",
              "      <td>50.8588</td>\n",
              "      <td>71.7</td>\n",
              "      <td>29.7</td>\n",
              "      <td>112.0</td>\n",
              "      <td>30.3</td>\n",
              "      <td>3.88</td>\n",
              "      <td>1.58546</td>\n",
              "      <td>1.22432</td>\n",
              "      <td>84.100</td>\n",
              "      <td>107.0</td>\n",
              "      <td>18.763</td>\n",
              "      <td>13.71763</td>\n",
              "      <td>1.74423</td>\n",
              "      <td>2.78764</td>\n",
              "      <td>30.7934</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>7.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 1, 1]</td>\n",
              "      <td>55</td>\n",
              "      <td>Male</td>\n",
              "      <td>British</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.78800</td>\n",
              "      <td>2.88700</td>\n",
              "      <td>5.56500</td>\n",
              "      <td>56.5183</td>\n",
              "      <td>40.2</td>\n",
              "      <td>29.8</td>\n",
              "      <td>67.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.87</td>\n",
              "      <td>2.11500</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>36.400</td>\n",
              "      <td>91.0</td>\n",
              "      <td>31.672</td>\n",
              "      <td>4.82700</td>\n",
              "      <td>1.89100</td>\n",
              "      <td>1.15700</td>\n",
              "      <td>20.7577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.00</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>49</td>\n",
              "      <td>Female</td>\n",
              "      <td>Irish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75600</td>\n",
              "      <td>2.67000</td>\n",
              "      <td>4.68000</td>\n",
              "      <td>4.7700</td>\n",
              "      <td>46.5</td>\n",
              "      <td>30.1</td>\n",
              "      <td>85.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.18</td>\n",
              "      <td>1.49300</td>\n",
              "      <td>0.73300</td>\n",
              "      <td>34.200</td>\n",
              "      <td>105.0</td>\n",
              "      <td>42.209</td>\n",
              "      <td>5.06300</td>\n",
              "      <td>1.86900</td>\n",
              "      <td>1.67700</td>\n",
              "      <td>25.9766</td>\n",
              "      <td>7.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>7.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>61</td>\n",
              "      <td>Female</td>\n",
              "      <td>British</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7bee5a-5f2a-443b-8263-5ce0db0be578')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b7bee5a-5f2a-443b-8263-5ce0db0be578 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b7bee5a-5f2a-443b-8263-5ce0db0be578');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "link ='https://github.com/analiseb/UB-Masters-Thesis/blob/main/data/CVD_data.csv?raw=true'\n",
        "df = pd.read_csv(link)\n",
        "pd.set_option('display.max_columns', None)\n",
        "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23351717",
      "metadata": {
        "id": "23351717"
      },
      "outputs": [],
      "source": [
        "outcomes = ['outcome_myocardial_infarction','outcome_cardiomyopathies','outcome_ischemic_heart_disease','outcome_heart_failure','outcome_peripheral_vascular_disease','outcome_cardiac_arrest','outcome_cerebral_infarction','outcome_arrhythmia']\n",
        "\n",
        "# classifying features by datatype for appropriate use in model\n",
        "continuous_cols = df.iloc[:,:18].columns.to_list()\n",
        "numerical_cols = df.iloc[:,18:18+13].columns.to_list()\n",
        "categorical_cols = df.iloc[:,18+13:18+13+30].columns.to_list() # ordinal encoded\n",
        "nominal_cats = ['1428-0.0','20117-0.0','2100-0.0','2654-0.0','21000-0.0','1538-0.0','31-0.0','6138-0.0','2090-0.0','1508-0.0','6142-0.0','1468-0.0','1239-0.0','1448-0.0','hypertension']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7b2cff",
      "metadata": {
        "id": "1e7b2cff"
      },
      "source": [
        "### transform features & split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb52956",
      "metadata": {
        "id": "8eb52956"
      },
      "outputs": [],
      "source": [
        "def process_features(data, target, norm_method, one_hot=True):\n",
        "    \n",
        "    # split data into features and target \n",
        "    X = data.iloc[:,:61]\n",
        "    y = data[target]\n",
        "    \n",
        "    # split into training and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
        "    \n",
        "    # scale numerical features\n",
        "    scaler = norm_method # QuantileTransformer(output_distribution='uniform'), StandardScaler(), MaxMinScaler()\n",
        "    X_train[continuous_cols+numerical_cols]=scaler.fit_transform(X_train[continuous_cols+numerical_cols])\n",
        "    X_test[continuous_cols+numerical_cols] = scaler.transform(X_test[continuous_cols+numerical_cols])\n",
        "    \n",
        "    # get_dummies on nominal categorical features & drop original cols\n",
        "    if one_hot:\n",
        "        join = pd.concat([X_train,X_test],axis=0)\n",
        "        dummies = pd.get_dummies(join[nominal_cats], columns=nominal_cats, drop_first=True)\n",
        "        X_train[dummies.columns] = dummies.iloc[:len(X_train),:]\n",
        "        X_test[dummies.columns] = dummies.iloc[len(X_train):,:]\n",
        "        X_test = X_test.reindex(columns = X_train.columns, fill_value=0)\n",
        "        del(dummies)\n",
        "\n",
        "        X_train.drop(nominal_cats,axis=1,inplace=True)\n",
        "        X_test.drop(nominal_cats,axis=1,inplace=True)\n",
        "\n",
        "    X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.15, random_state=1)\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879ff13a",
      "metadata": {
        "id": "879ff13a"
      },
      "source": [
        "### Resampling on training data to address class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28072b3",
      "metadata": {
        "id": "d28072b3"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "def resample_data(X_train, y_train, method):\n",
        "    \n",
        "    if method=='ADASYN':\n",
        "        X_temp,y_train= ADASYN().fit_resample(X_train,y_train)\n",
        "        X_train = pd.DataFrame(X_temp, columns=X_train.columns)\n",
        "        \n",
        "    elif method=='over':\n",
        "        # over sample the majority class in train\n",
        "        oversample = RandomOverSampler(sampling_strategy='minority',random_state=1)\n",
        "        X_temp, y_train = oversample.fit_resample(X_train, y_train)\n",
        "        X_train = pd.DataFrame(X_temp, columns=X_train.columns)\n",
        "\n",
        "    elif method=='under':\n",
        "        # under sample the majority class in train\n",
        "        undersample = RandomUnderSampler(sampling_strategy='majority',random_state=1)\n",
        "        X_temp, y_train = undersample.fit_resample(X_train, y_train)\n",
        "        X_train = pd.DataFrame(X_temp, columns=X_train.columns)\n",
        "        \n",
        "    elif method=='partial_under':\n",
        "        # under sample the majority class in train, ~2:1 neg/pos ratio\n",
        "        undersample_uneven = RandomUnderSampler(sampling_strategy=0.5,random_state=1)\n",
        "        X_temp, y_train = undersample_uneven.fit_resample(X_train, y_train)\n",
        "        X_train = pd.DataFrame(X_temp, columns=X_train2.columns)\n",
        "        \n",
        "    return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f27ac1",
      "metadata": {
        "id": "79f27ac1"
      },
      "source": [
        "### Build MLP Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4311a358",
      "metadata": {
        "id": "4311a358"
      },
      "source": [
        "#### define model metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd9d66d",
      "metadata": {
        "id": "1bd9d66d"
      },
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617f83cd",
      "metadata": {
        "id": "617f83cd"
      },
      "source": [
        "#### optimize number of nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5bf9b2",
      "metadata": {
        "id": "5d5bf9b2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def evaluate_nodes(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
        "    \n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, activation=tf.keras.activations.gelu , input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(n_nodes, activation=tf.keras.activations.gelu ))\n",
        "    model.add(Dense(n_nodes, activation=tf.keras.activations.gelu ))\n",
        "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=Adam(learning_rate=0.000001),\n",
        "        metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    # fit model on train set\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val, y_val),\n",
        "    )\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return history, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bgpit_ZemX1V",
      "metadata": {
        "id": "bgpit_ZemX1V"
      },
      "outputs": [],
      "source": [
        "def evaluate_layers(n_layers, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
        "    \n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, activation=tf.keras.activations.gelu , input_shape=(X_train.shape[1],)))\n",
        "    for _ in range(1,n_layers):\n",
        "        model.add(Dense(50, activation=tf.keras.activations.gelu ))\n",
        "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=Adam(learning_rate=0.000001),\n",
        "        metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    # fit model on train set\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val, y_val),\n",
        "    )\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return history, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8fd955",
      "metadata": {
        "id": "6d8fd955"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    fig, ((ax1,ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    ax1.set_title(\"Loss\")\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"Test Loss\", \"Validation Loss\"])\n",
        "    ax1.set_xlabel (\"Epoch\")\n",
        "\n",
        "    ax2.set_title(\"Accuracy\")\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"Test Accuracy\", \"Validation Accuracy\"])\n",
        "    ax2.set_xlabel (\"Epoch\")\n",
        "\n",
        "    ax3.set_title(\"Recall\")\n",
        "    ax3.plot(history.history[\"recall_m\"])\n",
        "    ax3.plot(history.history[\"val_recall_m\"])\n",
        "    ax3.legend([\"Test Recall\", \"Validation Recall\"])\n",
        "    ax3.set_xlabel (\"Epoch\")\n",
        "\n",
        "    ax4.set_title(\"F1\")\n",
        "    ax4.plot(history.history[\"f1_m\"])\n",
        "    ax4.plot(history.history[\"val_f1_m\"])\n",
        "    ax4.legend([\"Test F1-score\", \"Validation F1-score\"])\n",
        "    ax4.set_xlabel (\"Epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6cc2f2",
      "metadata": {
        "id": "9b6cc2f2"
      },
      "source": [
        "#### test range of input nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9dae95",
      "metadata": {
        "id": "cf9dae95"
      },
      "outputs": [],
      "source": [
        "num_nodes = [10, 25, 50, 100, 150, 200]\n",
        "epochs = 400\n",
        "batch = 500\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
        "X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "\n",
        "save_history = pd.DataFrame()\n",
        "for n_nodes in num_nodes:\n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = evaluate_nodes(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle('Number of Nodes: '+str(n_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJOOk6B5ZxXS",
      "metadata": {
        "id": "kJOOk6B5ZxXS"
      },
      "outputs": [],
      "source": [
        "def plot_compare(data, metric):\n",
        "  labels = []\n",
        "  for i in range(data.shape[1]):\n",
        "    sns.lineplot(x=data.index, y=data.loc[:,data.columns[i-1]], data=data)\n",
        "    plt.title(metric+' for all num_nodes')\n",
        "    labels.append(str(data.columns[i-1]))\n",
        "  plt.legend(title=metric, loc='best', labels=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3HVZFtPQcE-B",
      "metadata": {
        "id": "3HVZFtPQcE-B"
      },
      "outputs": [],
      "source": [
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[::4]], 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dQxjf1IXvO_C",
      "metadata": {
        "id": "dQxjf1IXvO_C"
      },
      "outputs": [],
      "source": [
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[1::4]], 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zz0p182tzyHR",
      "metadata": {
        "id": "zz0p182tzyHR"
      },
      "outputs": [],
      "source": [
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[2::4]], 'F1-Score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XhRlCOvtzyRo",
      "metadata": {
        "id": "XhRlCOvtzyRo"
      },
      "outputs": [],
      "source": [
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XB4Rv9251GnD",
      "metadata": {
        "id": "XB4Rv9251GnD"
      },
      "outputs": [],
      "source": [
        "num_layers = [3, 4, 5]\n",
        "epochs = 400\n",
        "batch = 500\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
        "X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "\n",
        "save_history = pd.DataFrame()\n",
        "for n_layers in num_layers:\n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = evaluate_layers(n_layers, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\n",
        "    save_history['loss_'+str(n_layers)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_layers)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_layers)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_layers)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_layers, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle('Number of Layers: '+str(n_layers))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a0e3458",
      "metadata": {
        "id": "2a0e3458"
      },
      "source": [
        "### Build Model after testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0009fde5",
      "metadata": {
        "id": "0009fde5"
      },
      "outputs": [],
      "source": [
        "def basic_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='relu',opt=SGD, lr=0.000001):\n",
        "    \n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1000, activation=activation , input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(500, activation=activation))\n",
        "    model.add(Dense(500, activation=activation ))\n",
        "    model.add(Dense(200, activation=activation ))\n",
        "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=opt(learning_rate=lr),\n",
        "        metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    # fit model on train set\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val, y_val),\n",
        "    )\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return history, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad05a1b",
      "metadata": {
        "id": "6ad05a1b"
      },
      "outputs": [],
      "source": [
        "epochs =500\n",
        "batch = 400\n",
        "sample_methods =['ADASYN', 'over', 'under', 'partial_under']\n",
        "activations=['relu', 'tanh', tf.keras.activations.gelu]\n",
        "optimizers = [SGD, Adam, Adagrad]\n",
        "num_transformers = [StandardScaler(), MinMaxScaler(), QuantileTransformer(output_distribution='uniform')]\n",
        "# test one_hot==True/False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e51f06c",
      "metadata": {
        "id": "3e51f06c",
        "outputId": "c55f6aaf-a2ac-4304-c726-b8f3c2a700f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "139/139 [==============================] - 4s 21ms/step - loss: 0.7230 - acc: 0.4847 - f1_m: 0.3388 - precision_m: 0.4727 - recall_m: 0.2649 - val_loss: 0.7283 - val_acc: 0.4751 - val_f1_m: 0.3410 - val_precision_m: 0.4922 - val_recall_m: 0.2618\n",
            "Epoch 2/500\n",
            "139/139 [==============================] - 2s 18ms/step - loss: 0.7228 - acc: 0.4845 - f1_m: 0.3404 - precision_m: 0.4732 - recall_m: 0.2668 - val_loss: 0.7281 - val_acc: 0.4746 - val_f1_m: 0.3413 - val_precision_m: 0.4911 - val_recall_m: 0.2624\n",
            "Epoch 3/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7227 - acc: 0.4847 - f1_m: 0.3420 - precision_m: 0.4731 - recall_m: 0.2687 - val_loss: 0.7279 - val_acc: 0.4749 - val_f1_m: 0.3459 - val_precision_m: 0.4959 - val_recall_m: 0.2664\n",
            "Epoch 4/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7225 - acc: 0.4847 - f1_m: 0.3437 - precision_m: 0.4730 - recall_m: 0.2707 - val_loss: 0.7277 - val_acc: 0.4750 - val_f1_m: 0.3475 - val_precision_m: 0.4960 - val_recall_m: 0.2683\n",
            "Epoch 5/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7224 - acc: 0.4844 - f1_m: 0.3452 - precision_m: 0.4732 - recall_m: 0.2726 - val_loss: 0.7275 - val_acc: 0.4754 - val_f1_m: 0.3521 - val_precision_m: 0.4999 - val_recall_m: 0.2726\n",
            "Epoch 6/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7223 - acc: 0.4846 - f1_m: 0.3468 - precision_m: 0.4734 - recall_m: 0.2745 - val_loss: 0.7273 - val_acc: 0.4757 - val_f1_m: 0.3538 - val_precision_m: 0.5004 - val_recall_m: 0.2745\n",
            "Epoch 7/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7222 - acc: 0.4849 - f1_m: 0.3488 - precision_m: 0.4745 - recall_m: 0.2768 - val_loss: 0.7271 - val_acc: 0.4755 - val_f1_m: 0.3547 - val_precision_m: 0.5000 - val_recall_m: 0.2757\n",
            "Epoch 8/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7220 - acc: 0.4847 - f1_m: 0.3502 - precision_m: 0.4741 - recall_m: 0.2786 - val_loss: 0.7269 - val_acc: 0.4758 - val_f1_m: 0.3559 - val_precision_m: 0.5005 - val_recall_m: 0.2771\n",
            "Epoch 9/500\n",
            "139/139 [==============================] - 2s 15ms/step - loss: 0.7219 - acc: 0.4847 - f1_m: 0.3517 - precision_m: 0.4741 - recall_m: 0.2805 - val_loss: 0.7267 - val_acc: 0.4766 - val_f1_m: 0.3589 - val_precision_m: 0.5017 - val_recall_m: 0.2802\n",
            "Epoch 10/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7218 - acc: 0.4848 - f1_m: 0.3533 - precision_m: 0.4743 - recall_m: 0.2822 - val_loss: 0.7266 - val_acc: 0.4766 - val_f1_m: 0.3603 - val_precision_m: 0.5017 - val_recall_m: 0.2820\n",
            "Epoch 11/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7217 - acc: 0.4848 - f1_m: 0.3546 - precision_m: 0.4742 - recall_m: 0.2840 - val_loss: 0.7264 - val_acc: 0.4771 - val_f1_m: 0.3625 - val_precision_m: 0.5025 - val_recall_m: 0.2844\n",
            "Epoch 12/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7215 - acc: 0.4847 - f1_m: 0.3557 - precision_m: 0.4742 - recall_m: 0.2856 - val_loss: 0.7262 - val_acc: 0.4774 - val_f1_m: 0.3671 - val_precision_m: 0.5053 - val_recall_m: 0.2891\n",
            "Epoch 13/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7214 - acc: 0.4850 - f1_m: 0.3572 - precision_m: 0.4749 - recall_m: 0.2873 - val_loss: 0.7261 - val_acc: 0.4769 - val_f1_m: 0.3686 - val_precision_m: 0.5044 - val_recall_m: 0.2912\n",
            "Epoch 14/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7213 - acc: 0.4852 - f1_m: 0.3593 - precision_m: 0.4755 - recall_m: 0.2897 - val_loss: 0.7259 - val_acc: 0.4761 - val_f1_m: 0.3691 - val_precision_m: 0.5031 - val_recall_m: 0.2923\n",
            "Epoch 15/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7212 - acc: 0.4854 - f1_m: 0.3610 - precision_m: 0.4760 - recall_m: 0.2917 - val_loss: 0.7257 - val_acc: 0.4758 - val_f1_m: 0.3694 - val_precision_m: 0.5027 - val_recall_m: 0.2928\n",
            "Epoch 16/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7211 - acc: 0.4852 - f1_m: 0.3620 - precision_m: 0.4759 - recall_m: 0.2930 - val_loss: 0.7256 - val_acc: 0.4762 - val_f1_m: 0.3718 - val_precision_m: 0.5032 - val_recall_m: 0.2956\n",
            "Epoch 17/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7210 - acc: 0.4854 - f1_m: 0.3636 - precision_m: 0.4769 - recall_m: 0.2947 - val_loss: 0.7254 - val_acc: 0.4767 - val_f1_m: 0.3737 - val_precision_m: 0.5039 - val_recall_m: 0.2979\n",
            "Epoch 18/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7209 - acc: 0.4856 - f1_m: 0.3650 - precision_m: 0.4772 - recall_m: 0.2965 - val_loss: 0.7253 - val_acc: 0.4766 - val_f1_m: 0.3747 - val_precision_m: 0.5036 - val_recall_m: 0.2993\n",
            "Epoch 19/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7208 - acc: 0.4857 - f1_m: 0.3662 - precision_m: 0.4771 - recall_m: 0.2982 - val_loss: 0.7251 - val_acc: 0.4759 - val_f1_m: 0.3757 - val_precision_m: 0.5024 - val_recall_m: 0.3011\n",
            "Epoch 20/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7207 - acc: 0.4857 - f1_m: 0.3674 - precision_m: 0.4769 - recall_m: 0.2997 - val_loss: 0.7249 - val_acc: 0.4762 - val_f1_m: 0.3777 - val_precision_m: 0.5029 - val_recall_m: 0.3033\n",
            "Epoch 21/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7206 - acc: 0.4858 - f1_m: 0.3688 - precision_m: 0.4777 - recall_m: 0.3013 - val_loss: 0.7248 - val_acc: 0.4752 - val_f1_m: 0.3781 - val_precision_m: 0.4998 - val_recall_m: 0.3048\n",
            "Epoch 22/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7205 - acc: 0.4858 - f1_m: 0.3702 - precision_m: 0.4778 - recall_m: 0.3031 - val_loss: 0.7247 - val_acc: 0.4755 - val_f1_m: 0.3793 - val_precision_m: 0.5003 - val_recall_m: 0.3062\n",
            "Epoch 23/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7204 - acc: 0.4856 - f1_m: 0.3712 - precision_m: 0.4778 - recall_m: 0.3043 - val_loss: 0.7245 - val_acc: 0.4757 - val_f1_m: 0.3803 - val_precision_m: 0.5006 - val_recall_m: 0.3075\n",
            "Epoch 24/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7203 - acc: 0.4857 - f1_m: 0.3723 - precision_m: 0.4769 - recall_m: 0.3062 - val_loss: 0.7244 - val_acc: 0.4753 - val_f1_m: 0.3807 - val_precision_m: 0.5000 - val_recall_m: 0.3082\n",
            "Epoch 25/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7202 - acc: 0.4855 - f1_m: 0.3736 - precision_m: 0.4779 - recall_m: 0.3079 - val_loss: 0.7242 - val_acc: 0.4754 - val_f1_m: 0.3819 - val_precision_m: 0.5001 - val_recall_m: 0.3096\n",
            "Epoch 26/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7201 - acc: 0.4854 - f1_m: 0.3746 - precision_m: 0.4773 - recall_m: 0.3093 - val_loss: 0.7241 - val_acc: 0.4754 - val_f1_m: 0.3848 - val_precision_m: 0.5020 - val_recall_m: 0.3127\n",
            "Epoch 27/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7200 - acc: 0.4852 - f1_m: 0.3756 - precision_m: 0.4772 - recall_m: 0.3107 - val_loss: 0.7240 - val_acc: 0.4749 - val_f1_m: 0.3854 - val_precision_m: 0.5012 - val_recall_m: 0.3138\n",
            "Epoch 28/500\n",
            "139/139 [==============================] - 2s 15ms/step - loss: 0.7199 - acc: 0.4852 - f1_m: 0.3767 - precision_m: 0.4768 - recall_m: 0.3123 - val_loss: 0.7238 - val_acc: 0.4748 - val_f1_m: 0.3863 - val_precision_m: 0.5012 - val_recall_m: 0.3150\n",
            "Epoch 29/500\n",
            "139/139 [==============================] - 2s 17ms/step - loss: 0.7198 - acc: 0.4854 - f1_m: 0.3781 - precision_m: 0.4774 - recall_m: 0.3136 - val_loss: 0.7237 - val_acc: 0.4744 - val_f1_m: 0.3869 - val_precision_m: 0.5005 - val_recall_m: 0.3161\n",
            "Epoch 30/500\n",
            "139/139 [==============================] - 2s 16ms/step - loss: 0.7198 - acc: 0.4856 - f1_m: 0.3796 - precision_m: 0.4782 - recall_m: 0.3155 - val_loss: 0.7236 - val_acc: 0.4743 - val_f1_m: 0.3875 - val_precision_m: 0.5004 - val_recall_m: 0.3170\n",
            "Epoch 31/500\n",
            " 88/139 [=================>............] - ETA: 0s - loss: 0.7203 - acc: 0.4854 - f1_m: 0.3792 - precision_m: 0.4763 - recall_m: 0.3158"
          ]
        }
      ],
      "source": [
        "# Test numerical transformers\n",
        "save_history = pd.DataFrame()\n",
        "for param in num_transformers:\n",
        "    \n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', param, one_hot=True)\n",
        "    X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "    \n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = basic_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='tanh',opt=SGD, lr=0.000001)\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle(str(param))\n",
        "\n",
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404a7317",
      "metadata": {
        "id": "404a7317"
      },
      "outputs": [],
      "source": [
        "# Test sample methods\n",
        "save_history = pd.DataFrame()\n",
        "for param in sample_methods:\n",
        "    \n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
        "    X_train, y_train= resample_data(X_train, y_train, param)\n",
        "    \n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = basic_model(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='relu',opt=SGD, lr=0.000001)\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle(str(param))\n",
        "\n",
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "f84752a4",
      "metadata": {
        "id": "f84752a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bf77509-7036-4d5a-c909-0474e9422d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "139/139 [==============================] - 2s 12ms/step - loss: 0.7196 - acc: 0.5023 - f1_m: 0.2960 - precision_m: 0.5003 - recall_m: 0.2302 - val_loss: 0.7078 - val_acc: 0.4942 - val_f1_m: 0.4492 - val_precision_m: 0.5204 - val_recall_m: 0.3959\n",
            "Epoch 2/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.7032 - acc: 0.5071 - f1_m: 0.4886 - precision_m: 0.5075 - recall_m: 0.4731 - val_loss: 0.7019 - val_acc: 0.5067 - val_f1_m: 0.5038 - val_precision_m: 0.5285 - val_recall_m: 0.4821\n",
            "Epoch 3/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.7000 - acc: 0.5124 - f1_m: 0.5103 - precision_m: 0.5124 - recall_m: 0.5096 - val_loss: 0.6987 - val_acc: 0.5145 - val_f1_m: 0.5200 - val_precision_m: 0.5350 - val_recall_m: 0.5067\n",
            "Epoch 4/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6974 - acc: 0.5182 - f1_m: 0.5173 - precision_m: 0.5182 - recall_m: 0.5178 - val_loss: 0.6960 - val_acc: 0.5233 - val_f1_m: 0.5342 - val_precision_m: 0.5439 - val_recall_m: 0.5259\n",
            "Epoch 5/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6952 - acc: 0.5227 - f1_m: 0.5247 - precision_m: 0.5225 - recall_m: 0.5282 - val_loss: 0.6939 - val_acc: 0.5289 - val_f1_m: 0.5399 - val_precision_m: 0.5489 - val_recall_m: 0.5321\n",
            "Epoch 6/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6932 - acc: 0.5287 - f1_m: 0.5296 - precision_m: 0.5287 - recall_m: 0.5318 - val_loss: 0.6920 - val_acc: 0.5352 - val_f1_m: 0.5460 - val_precision_m: 0.5548 - val_recall_m: 0.5383\n",
            "Epoch 7/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6915 - acc: 0.5345 - f1_m: 0.5364 - precision_m: 0.5345 - recall_m: 0.5397 - val_loss: 0.6904 - val_acc: 0.5388 - val_f1_m: 0.5476 - val_precision_m: 0.5594 - val_recall_m: 0.5371\n",
            "Epoch 8/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6899 - acc: 0.5385 - f1_m: 0.5435 - precision_m: 0.5379 - recall_m: 0.5507 - val_loss: 0.6892 - val_acc: 0.5408 - val_f1_m: 0.5441 - val_precision_m: 0.5642 - val_recall_m: 0.5265\n",
            "Epoch 9/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6885 - acc: 0.5435 - f1_m: 0.5429 - precision_m: 0.5436 - recall_m: 0.5436 - val_loss: 0.6878 - val_acc: 0.5446 - val_f1_m: 0.5494 - val_precision_m: 0.5676 - val_recall_m: 0.5335\n",
            "Epoch 10/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6873 - acc: 0.5456 - f1_m: 0.5424 - precision_m: 0.5459 - recall_m: 0.5401 - val_loss: 0.6863 - val_acc: 0.5495 - val_f1_m: 0.5606 - val_precision_m: 0.5714 - val_recall_m: 0.5515\n",
            "Epoch 11/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6862 - acc: 0.5494 - f1_m: 0.5507 - precision_m: 0.5491 - recall_m: 0.5536 - val_loss: 0.6854 - val_acc: 0.5535 - val_f1_m: 0.5624 - val_precision_m: 0.5789 - val_recall_m: 0.5478\n",
            "Epoch 12/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6852 - acc: 0.5516 - f1_m: 0.5502 - precision_m: 0.5518 - recall_m: 0.5503 - val_loss: 0.6843 - val_acc: 0.5560 - val_f1_m: 0.5679 - val_precision_m: 0.5806 - val_recall_m: 0.5569\n",
            "Epoch 13/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6842 - acc: 0.5548 - f1_m: 0.5517 - precision_m: 0.5553 - recall_m: 0.5492 - val_loss: 0.6832 - val_acc: 0.5619 - val_f1_m: 0.5790 - val_precision_m: 0.5839 - val_recall_m: 0.5753\n",
            "Epoch 14/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6834 - acc: 0.5564 - f1_m: 0.5579 - precision_m: 0.5559 - recall_m: 0.5613 - val_loss: 0.6826 - val_acc: 0.5628 - val_f1_m: 0.5754 - val_precision_m: 0.5868 - val_recall_m: 0.5657\n",
            "Epoch 15/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6826 - acc: 0.5590 - f1_m: 0.5584 - precision_m: 0.5591 - recall_m: 0.5589 - val_loss: 0.6818 - val_acc: 0.5642 - val_f1_m: 0.5767 - val_precision_m: 0.5882 - val_recall_m: 0.5667\n",
            "Epoch 16/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6819 - acc: 0.5612 - f1_m: 0.5625 - precision_m: 0.5606 - recall_m: 0.5657 - val_loss: 0.6813 - val_acc: 0.5647 - val_f1_m: 0.5742 - val_precision_m: 0.5900 - val_recall_m: 0.5605\n",
            "Epoch 17/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6813 - acc: 0.5633 - f1_m: 0.5598 - precision_m: 0.5643 - recall_m: 0.5567 - val_loss: 0.6804 - val_acc: 0.5676 - val_f1_m: 0.5818 - val_precision_m: 0.5896 - val_recall_m: 0.5755\n",
            "Epoch 18/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6807 - acc: 0.5651 - f1_m: 0.5653 - precision_m: 0.5649 - recall_m: 0.5669 - val_loss: 0.6799 - val_acc: 0.5702 - val_f1_m: 0.5829 - val_precision_m: 0.5925 - val_recall_m: 0.5748\n",
            "Epoch 19/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6801 - acc: 0.5658 - f1_m: 0.5646 - precision_m: 0.5657 - recall_m: 0.5648 - val_loss: 0.6793 - val_acc: 0.5723 - val_f1_m: 0.5877 - val_precision_m: 0.5945 - val_recall_m: 0.5826\n",
            "Epoch 20/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6796 - acc: 0.5673 - f1_m: 0.5653 - precision_m: 0.5680 - recall_m: 0.5640 - val_loss: 0.6786 - val_acc: 0.5758 - val_f1_m: 0.5945 - val_precision_m: 0.5961 - val_recall_m: 0.5944\n",
            "Epoch 21/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6791 - acc: 0.5684 - f1_m: 0.5698 - precision_m: 0.5680 - recall_m: 0.5727 - val_loss: 0.6783 - val_acc: 0.5765 - val_f1_m: 0.5921 - val_precision_m: 0.5972 - val_recall_m: 0.5885\n",
            "Epoch 22/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6787 - acc: 0.5699 - f1_m: 0.5688 - precision_m: 0.5702 - recall_m: 0.5688 - val_loss: 0.6777 - val_acc: 0.5782 - val_f1_m: 0.5959 - val_precision_m: 0.5978 - val_recall_m: 0.5953\n",
            "Epoch 23/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6783 - acc: 0.5714 - f1_m: 0.5735 - precision_m: 0.5708 - recall_m: 0.5778 - val_loss: 0.6775 - val_acc: 0.5791 - val_f1_m: 0.5943 - val_precision_m: 0.6012 - val_recall_m: 0.5894\n",
            "Epoch 24/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6779 - acc: 0.5724 - f1_m: 0.5725 - precision_m: 0.5721 - recall_m: 0.5742 - val_loss: 0.6771 - val_acc: 0.5803 - val_f1_m: 0.5951 - val_precision_m: 0.6024 - val_recall_m: 0.5898\n",
            "Epoch 25/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6775 - acc: 0.5736 - f1_m: 0.5751 - precision_m: 0.5727 - recall_m: 0.5785 - val_loss: 0.6769 - val_acc: 0.5809 - val_f1_m: 0.5932 - val_precision_m: 0.6043 - val_recall_m: 0.5843\n",
            "Epoch 26/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6772 - acc: 0.5743 - f1_m: 0.5739 - precision_m: 0.5743 - recall_m: 0.5747 - val_loss: 0.6764 - val_acc: 0.5826 - val_f1_m: 0.5978 - val_precision_m: 0.6055 - val_recall_m: 0.5924\n",
            "Epoch 27/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6768 - acc: 0.5749 - f1_m: 0.5756 - precision_m: 0.5747 - recall_m: 0.5779 - val_loss: 0.6761 - val_acc: 0.5827 - val_f1_m: 0.5972 - val_precision_m: 0.6059 - val_recall_m: 0.5908\n",
            "Epoch 28/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6765 - acc: 0.5753 - f1_m: 0.5754 - precision_m: 0.5751 - recall_m: 0.5770 - val_loss: 0.6758 - val_acc: 0.5832 - val_f1_m: 0.5986 - val_precision_m: 0.6060 - val_recall_m: 0.5932\n",
            "Epoch 29/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6762 - acc: 0.5759 - f1_m: 0.5776 - precision_m: 0.5751 - recall_m: 0.5813 - val_loss: 0.6756 - val_acc: 0.5840 - val_f1_m: 0.5977 - val_precision_m: 0.6077 - val_recall_m: 0.5899\n",
            "Epoch 30/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6759 - acc: 0.5769 - f1_m: 0.5769 - precision_m: 0.5767 - recall_m: 0.5783 - val_loss: 0.6752 - val_acc: 0.5848 - val_f1_m: 0.5998 - val_precision_m: 0.6076 - val_recall_m: 0.5942\n",
            "Epoch 31/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6757 - acc: 0.5776 - f1_m: 0.5771 - precision_m: 0.5773 - recall_m: 0.5781 - val_loss: 0.6749 - val_acc: 0.5844 - val_f1_m: 0.6011 - val_precision_m: 0.6064 - val_recall_m: 0.5978\n",
            "Epoch 32/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6754 - acc: 0.5780 - f1_m: 0.5802 - precision_m: 0.5770 - recall_m: 0.5847 - val_loss: 0.6747 - val_acc: 0.5853 - val_f1_m: 0.6003 - val_precision_m: 0.6083 - val_recall_m: 0.5946\n",
            "Epoch 33/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6752 - acc: 0.5791 - f1_m: 0.5788 - precision_m: 0.5790 - recall_m: 0.5798 - val_loss: 0.6743 - val_acc: 0.5864 - val_f1_m: 0.6043 - val_precision_m: 0.6077 - val_recall_m: 0.6029\n",
            "Epoch 34/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6749 - acc: 0.5793 - f1_m: 0.5815 - precision_m: 0.5781 - recall_m: 0.5859 - val_loss: 0.6742 - val_acc: 0.5872 - val_f1_m: 0.6033 - val_precision_m: 0.6095 - val_recall_m: 0.5992\n",
            "Epoch 35/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6747 - acc: 0.5795 - f1_m: 0.5792 - precision_m: 0.5793 - recall_m: 0.5803 - val_loss: 0.6738 - val_acc: 0.5876 - val_f1_m: 0.6064 - val_precision_m: 0.6083 - val_recall_m: 0.6066\n",
            "Epoch 36/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6745 - acc: 0.5805 - f1_m: 0.5816 - precision_m: 0.5797 - recall_m: 0.5847 - val_loss: 0.6736 - val_acc: 0.5885 - val_f1_m: 0.6072 - val_precision_m: 0.6091 - val_recall_m: 0.6074\n",
            "Epoch 37/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6743 - acc: 0.5808 - f1_m: 0.5828 - precision_m: 0.5798 - recall_m: 0.5868 - val_loss: 0.6735 - val_acc: 0.5882 - val_f1_m: 0.6055 - val_precision_m: 0.6096 - val_recall_m: 0.6035\n",
            "Epoch 38/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6741 - acc: 0.5816 - f1_m: 0.5817 - precision_m: 0.5815 - recall_m: 0.5835 - val_loss: 0.6732 - val_acc: 0.5890 - val_f1_m: 0.6078 - val_precision_m: 0.6086 - val_recall_m: 0.6092\n",
            "Epoch 39/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6739 - acc: 0.5821 - f1_m: 0.5828 - precision_m: 0.5817 - recall_m: 0.5849 - val_loss: 0.6730 - val_acc: 0.5895 - val_f1_m: 0.6088 - val_precision_m: 0.6087 - val_recall_m: 0.6111\n",
            "Epoch 40/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6737 - acc: 0.5824 - f1_m: 0.5835 - precision_m: 0.5819 - recall_m: 0.5862 - val_loss: 0.6728 - val_acc: 0.5900 - val_f1_m: 0.6108 - val_precision_m: 0.6096 - val_recall_m: 0.6146\n",
            "Epoch 41/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6735 - acc: 0.5827 - f1_m: 0.5852 - precision_m: 0.5818 - recall_m: 0.5901 - val_loss: 0.6727 - val_acc: 0.5898 - val_f1_m: 0.6086 - val_precision_m: 0.6105 - val_recall_m: 0.6094\n",
            "Epoch 42/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6734 - acc: 0.5834 - f1_m: 0.5851 - precision_m: 0.5827 - recall_m: 0.5885 - val_loss: 0.6726 - val_acc: 0.5902 - val_f1_m: 0.6086 - val_precision_m: 0.6112 - val_recall_m: 0.6087\n",
            "Epoch 43/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6732 - acc: 0.5841 - f1_m: 0.5836 - precision_m: 0.5840 - recall_m: 0.5845 - val_loss: 0.6722 - val_acc: 0.5912 - val_f1_m: 0.6119 - val_precision_m: 0.6108 - val_recall_m: 0.6157\n",
            "Epoch 44/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6730 - acc: 0.5838 - f1_m: 0.5868 - precision_m: 0.5823 - recall_m: 0.5925 - val_loss: 0.6723 - val_acc: 0.5920 - val_f1_m: 0.6099 - val_precision_m: 0.6131 - val_recall_m: 0.6093\n",
            "Epoch 45/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6729 - acc: 0.5846 - f1_m: 0.5865 - precision_m: 0.5839 - recall_m: 0.5904 - val_loss: 0.6722 - val_acc: 0.5910 - val_f1_m: 0.6081 - val_precision_m: 0.6126 - val_recall_m: 0.6064\n",
            "Epoch 46/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6727 - acc: 0.5843 - f1_m: 0.5844 - precision_m: 0.5841 - recall_m: 0.5861 - val_loss: 0.6719 - val_acc: 0.5909 - val_f1_m: 0.6101 - val_precision_m: 0.6114 - val_recall_m: 0.6115\n",
            "Epoch 47/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6726 - acc: 0.5845 - f1_m: 0.5851 - precision_m: 0.5840 - recall_m: 0.5874 - val_loss: 0.6716 - val_acc: 0.5923 - val_f1_m: 0.6122 - val_precision_m: 0.6113 - val_recall_m: 0.6161\n",
            "Epoch 48/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6724 - acc: 0.5848 - f1_m: 0.5858 - precision_m: 0.5840 - recall_m: 0.5888 - val_loss: 0.6714 - val_acc: 0.5933 - val_f1_m: 0.6141 - val_precision_m: 0.6116 - val_recall_m: 0.6196\n",
            "Epoch 49/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6723 - acc: 0.5850 - f1_m: 0.5887 - precision_m: 0.5833 - recall_m: 0.5955 - val_loss: 0.6715 - val_acc: 0.5924 - val_f1_m: 0.6103 - val_precision_m: 0.6127 - val_recall_m: 0.6110\n",
            "Epoch 50/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6722 - acc: 0.5852 - f1_m: 0.5859 - precision_m: 0.5849 - recall_m: 0.5885 - val_loss: 0.6713 - val_acc: 0.5933 - val_f1_m: 0.6127 - val_precision_m: 0.6126 - val_recall_m: 0.6159\n",
            "Epoch 51/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6720 - acc: 0.5857 - f1_m: 0.5873 - precision_m: 0.5845 - recall_m: 0.5914 - val_loss: 0.6711 - val_acc: 0.5955 - val_f1_m: 0.6146 - val_precision_m: 0.6146 - val_recall_m: 0.6179\n",
            "Epoch 52/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6719 - acc: 0.5861 - f1_m: 0.5902 - precision_m: 0.5840 - recall_m: 0.5977 - val_loss: 0.6713 - val_acc: 0.5921 - val_f1_m: 0.6081 - val_precision_m: 0.6144 - val_recall_m: 0.6048\n",
            "Epoch 53/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6718 - acc: 0.5864 - f1_m: 0.5889 - precision_m: 0.5853 - recall_m: 0.5940 - val_loss: 0.6712 - val_acc: 0.5930 - val_f1_m: 0.6079 - val_precision_m: 0.6160 - val_recall_m: 0.6029\n",
            "Epoch 54/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6716 - acc: 0.5863 - f1_m: 0.5861 - precision_m: 0.5859 - recall_m: 0.5876 - val_loss: 0.6708 - val_acc: 0.5950 - val_f1_m: 0.6133 - val_precision_m: 0.6147 - val_recall_m: 0.6150\n",
            "Epoch 55/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6715 - acc: 0.5867 - f1_m: 0.5881 - precision_m: 0.5860 - recall_m: 0.5915 - val_loss: 0.6706 - val_acc: 0.5958 - val_f1_m: 0.6151 - val_precision_m: 0.6147 - val_recall_m: 0.6185\n",
            "Epoch 56/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6714 - acc: 0.5864 - f1_m: 0.5886 - precision_m: 0.5851 - recall_m: 0.5934 - val_loss: 0.6705 - val_acc: 0.5965 - val_f1_m: 0.6158 - val_precision_m: 0.6152 - val_recall_m: 0.6195\n",
            "Epoch 57/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6713 - acc: 0.5869 - f1_m: 0.5899 - precision_m: 0.5855 - recall_m: 0.5957 - val_loss: 0.6705 - val_acc: 0.5951 - val_f1_m: 0.6130 - val_precision_m: 0.6150 - val_recall_m: 0.6140\n",
            "Epoch 58/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6712 - acc: 0.5869 - f1_m: 0.5879 - precision_m: 0.5863 - recall_m: 0.5908 - val_loss: 0.6702 - val_acc: 0.5974 - val_f1_m: 0.6178 - val_precision_m: 0.6153 - val_recall_m: 0.6233\n",
            "Epoch 59/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6711 - acc: 0.5870 - f1_m: 0.5889 - precision_m: 0.5859 - recall_m: 0.5931 - val_loss: 0.6701 - val_acc: 0.5983 - val_f1_m: 0.6193 - val_precision_m: 0.6156 - val_recall_m: 0.6261\n",
            "Epoch 60/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6710 - acc: 0.5880 - f1_m: 0.5937 - precision_m: 0.5853 - recall_m: 0.6038 - val_loss: 0.6704 - val_acc: 0.5932 - val_f1_m: 0.6079 - val_precision_m: 0.6161 - val_recall_m: 0.6029\n",
            "Epoch 61/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6709 - acc: 0.5882 - f1_m: 0.5893 - precision_m: 0.5878 - recall_m: 0.5922 - val_loss: 0.6701 - val_acc: 0.5955 - val_f1_m: 0.6129 - val_precision_m: 0.6164 - val_recall_m: 0.6123\n",
            "Epoch 62/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6708 - acc: 0.5881 - f1_m: 0.5907 - precision_m: 0.5868 - recall_m: 0.5960 - val_loss: 0.6700 - val_acc: 0.5948 - val_f1_m: 0.6119 - val_precision_m: 0.6161 - val_recall_m: 0.6107\n",
            "Epoch 63/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6707 - acc: 0.5879 - f1_m: 0.5898 - precision_m: 0.5867 - recall_m: 0.5939 - val_loss: 0.6698 - val_acc: 0.5963 - val_f1_m: 0.6149 - val_precision_m: 0.6164 - val_recall_m: 0.6162\n",
            "Epoch 64/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6706 - acc: 0.5886 - f1_m: 0.5906 - precision_m: 0.5877 - recall_m: 0.5949 - val_loss: 0.6697 - val_acc: 0.5963 - val_f1_m: 0.6157 - val_precision_m: 0.6159 - val_recall_m: 0.6183\n",
            "Epoch 65/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6705 - acc: 0.5890 - f1_m: 0.5940 - precision_m: 0.5866 - recall_m: 0.6028 - val_loss: 0.6699 - val_acc: 0.5941 - val_f1_m: 0.6090 - val_precision_m: 0.6168 - val_recall_m: 0.6044\n",
            "Epoch 66/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6704 - acc: 0.5891 - f1_m: 0.5903 - precision_m: 0.5883 - recall_m: 0.5936 - val_loss: 0.6696 - val_acc: 0.5960 - val_f1_m: 0.6128 - val_precision_m: 0.6173 - val_recall_m: 0.6112\n",
            "Epoch 67/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6703 - acc: 0.5888 - f1_m: 0.5918 - precision_m: 0.5876 - recall_m: 0.5976 - val_loss: 0.6696 - val_acc: 0.5951 - val_f1_m: 0.6112 - val_precision_m: 0.6170 - val_recall_m: 0.6084\n",
            "Epoch 68/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6702 - acc: 0.5896 - f1_m: 0.5919 - precision_m: 0.5884 - recall_m: 0.5967 - val_loss: 0.6695 - val_acc: 0.5957 - val_f1_m: 0.6124 - val_precision_m: 0.6171 - val_recall_m: 0.6105\n",
            "Epoch 69/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6701 - acc: 0.5895 - f1_m: 0.5915 - precision_m: 0.5883 - recall_m: 0.5961 - val_loss: 0.6693 - val_acc: 0.5962 - val_f1_m: 0.6140 - val_precision_m: 0.6169 - val_recall_m: 0.6139\n",
            "Epoch 70/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6700 - acc: 0.5897 - f1_m: 0.5921 - precision_m: 0.5883 - recall_m: 0.5972 - val_loss: 0.6692 - val_acc: 0.5964 - val_f1_m: 0.6148 - val_precision_m: 0.6165 - val_recall_m: 0.6158\n",
            "Epoch 71/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6699 - acc: 0.5902 - f1_m: 0.5934 - precision_m: 0.5885 - recall_m: 0.5997 - val_loss: 0.6692 - val_acc: 0.5964 - val_f1_m: 0.6134 - val_precision_m: 0.6174 - val_recall_m: 0.6121\n",
            "Epoch 72/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6698 - acc: 0.5896 - f1_m: 0.5926 - precision_m: 0.5881 - recall_m: 0.5987 - val_loss: 0.6691 - val_acc: 0.5960 - val_f1_m: 0.6124 - val_precision_m: 0.6174 - val_recall_m: 0.6101\n",
            "Epoch 73/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6698 - acc: 0.5905 - f1_m: 0.5946 - precision_m: 0.5884 - recall_m: 0.6022 - val_loss: 0.6692 - val_acc: 0.5949 - val_f1_m: 0.6092 - val_precision_m: 0.6178 - val_recall_m: 0.6038\n",
            "Epoch 74/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6697 - acc: 0.5906 - f1_m: 0.5919 - precision_m: 0.5898 - recall_m: 0.5956 - val_loss: 0.6689 - val_acc: 0.5966 - val_f1_m: 0.6134 - val_precision_m: 0.6178 - val_recall_m: 0.6119\n",
            "Epoch 75/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6696 - acc: 0.5908 - f1_m: 0.5931 - precision_m: 0.5896 - recall_m: 0.5977 - val_loss: 0.6688 - val_acc: 0.5960 - val_f1_m: 0.6137 - val_precision_m: 0.6166 - val_recall_m: 0.6137\n",
            "Epoch 76/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6695 - acc: 0.5905 - f1_m: 0.5951 - precision_m: 0.5880 - recall_m: 0.6035 - val_loss: 0.6690 - val_acc: 0.5955 - val_f1_m: 0.6103 - val_precision_m: 0.6179 - val_recall_m: 0.6057\n",
            "Epoch 77/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6694 - acc: 0.5904 - f1_m: 0.5929 - precision_m: 0.5894 - recall_m: 0.5975 - val_loss: 0.6688 - val_acc: 0.5964 - val_f1_m: 0.6121 - val_precision_m: 0.6181 - val_recall_m: 0.6091\n",
            "Epoch 78/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6694 - acc: 0.5911 - f1_m: 0.5931 - precision_m: 0.5901 - recall_m: 0.5976 - val_loss: 0.6686 - val_acc: 0.5961 - val_f1_m: 0.6140 - val_precision_m: 0.6165 - val_recall_m: 0.6142\n",
            "Epoch 79/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6693 - acc: 0.5907 - f1_m: 0.5938 - precision_m: 0.5890 - recall_m: 0.6000 - val_loss: 0.6685 - val_acc: 0.5957 - val_f1_m: 0.6131 - val_precision_m: 0.6165 - val_recall_m: 0.6125\n",
            "Epoch 80/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6692 - acc: 0.5911 - f1_m: 0.5962 - precision_m: 0.5890 - recall_m: 0.6050 - val_loss: 0.6688 - val_acc: 0.5956 - val_f1_m: 0.6097 - val_precision_m: 0.6194 - val_recall_m: 0.6029\n",
            "Epoch 81/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6692 - acc: 0.5919 - f1_m: 0.5923 - precision_m: 0.5915 - recall_m: 0.5943 - val_loss: 0.6683 - val_acc: 0.5970 - val_f1_m: 0.6156 - val_precision_m: 0.6169 - val_recall_m: 0.6170\n",
            "Epoch 82/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6691 - acc: 0.5916 - f1_m: 0.5950 - precision_m: 0.5899 - recall_m: 0.6016 - val_loss: 0.6683 - val_acc: 0.5973 - val_f1_m: 0.6156 - val_precision_m: 0.6174 - val_recall_m: 0.6165\n",
            "Epoch 83/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6690 - acc: 0.5918 - f1_m: 0.5959 - precision_m: 0.5896 - recall_m: 0.6039 - val_loss: 0.6684 - val_acc: 0.5962 - val_f1_m: 0.6122 - val_precision_m: 0.6189 - val_recall_m: 0.6083\n",
            "Epoch 84/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6689 - acc: 0.5921 - f1_m: 0.5946 - precision_m: 0.5905 - recall_m: 0.6000 - val_loss: 0.6683 - val_acc: 0.5976 - val_f1_m: 0.6145 - val_precision_m: 0.6196 - val_recall_m: 0.6119\n",
            "Epoch 85/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6689 - acc: 0.5919 - f1_m: 0.5950 - precision_m: 0.5903 - recall_m: 0.6013 - val_loss: 0.6682 - val_acc: 0.5977 - val_f1_m: 0.6146 - val_precision_m: 0.6197 - val_recall_m: 0.6121\n",
            "Epoch 86/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6688 - acc: 0.5924 - f1_m: 0.5950 - precision_m: 0.5908 - recall_m: 0.6003 - val_loss: 0.6681 - val_acc: 0.5978 - val_f1_m: 0.6153 - val_precision_m: 0.6194 - val_recall_m: 0.6137\n",
            "Epoch 87/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6687 - acc: 0.5922 - f1_m: 0.5952 - precision_m: 0.5909 - recall_m: 0.6010 - val_loss: 0.6680 - val_acc: 0.5989 - val_f1_m: 0.6169 - val_precision_m: 0.6199 - val_recall_m: 0.6163\n",
            "Epoch 88/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6687 - acc: 0.5923 - f1_m: 0.5950 - precision_m: 0.5909 - recall_m: 0.6004 - val_loss: 0.6679 - val_acc: 0.5991 - val_f1_m: 0.6182 - val_precision_m: 0.6195 - val_recall_m: 0.6193\n",
            "Epoch 89/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6686 - acc: 0.5928 - f1_m: 0.5965 - precision_m: 0.5907 - recall_m: 0.6033 - val_loss: 0.6679 - val_acc: 0.5990 - val_f1_m: 0.6172 - val_precision_m: 0.6200 - val_recall_m: 0.6168\n",
            "Epoch 90/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6686 - acc: 0.5927 - f1_m: 0.5950 - precision_m: 0.5909 - recall_m: 0.6002 - val_loss: 0.6677 - val_acc: 0.5996 - val_f1_m: 0.6187 - val_precision_m: 0.6199 - val_recall_m: 0.6200\n",
            "Epoch 91/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6685 - acc: 0.5930 - f1_m: 0.5966 - precision_m: 0.5907 - recall_m: 0.6034 - val_loss: 0.6677 - val_acc: 0.5990 - val_f1_m: 0.6174 - val_precision_m: 0.6198 - val_recall_m: 0.6175\n",
            "Epoch 92/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6684 - acc: 0.5928 - f1_m: 0.5966 - precision_m: 0.5911 - recall_m: 0.6038 - val_loss: 0.6678 - val_acc: 0.5985 - val_f1_m: 0.6155 - val_precision_m: 0.6202 - val_recall_m: 0.6133\n",
            "Epoch 93/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6684 - acc: 0.5935 - f1_m: 0.5954 - precision_m: 0.5923 - recall_m: 0.5997 - val_loss: 0.6675 - val_acc: 0.5990 - val_f1_m: 0.6183 - val_precision_m: 0.6193 - val_recall_m: 0.6196\n",
            "Epoch 94/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6683 - acc: 0.5933 - f1_m: 0.5942 - precision_m: 0.5926 - recall_m: 0.5976 - val_loss: 0.6672 - val_acc: 0.6003 - val_f1_m: 0.6228 - val_precision_m: 0.6183 - val_recall_m: 0.6296\n",
            "Epoch 95/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6683 - acc: 0.5930 - f1_m: 0.5979 - precision_m: 0.5904 - recall_m: 0.6068 - val_loss: 0.6674 - val_acc: 0.5993 - val_f1_m: 0.6190 - val_precision_m: 0.6193 - val_recall_m: 0.6210\n",
            "Epoch 96/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6682 - acc: 0.5929 - f1_m: 0.5969 - precision_m: 0.5909 - recall_m: 0.6040 - val_loss: 0.6674 - val_acc: 0.5995 - val_f1_m: 0.6179 - val_precision_m: 0.6203 - val_recall_m: 0.6179\n",
            "Epoch 97/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6681 - acc: 0.5932 - f1_m: 0.5975 - precision_m: 0.5910 - recall_m: 0.6053 - val_loss: 0.6675 - val_acc: 0.5988 - val_f1_m: 0.6152 - val_precision_m: 0.6208 - val_recall_m: 0.6122\n",
            "Epoch 98/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6681 - acc: 0.5931 - f1_m: 0.5964 - precision_m: 0.5915 - recall_m: 0.6026 - val_loss: 0.6675 - val_acc: 0.5989 - val_f1_m: 0.6156 - val_precision_m: 0.6208 - val_recall_m: 0.6129\n",
            "Epoch 99/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6680 - acc: 0.5938 - f1_m: 0.5955 - precision_m: 0.5927 - recall_m: 0.5996 - val_loss: 0.6672 - val_acc: 0.5999 - val_f1_m: 0.6195 - val_precision_m: 0.6198 - val_recall_m: 0.6217\n",
            "Epoch 100/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6680 - acc: 0.5936 - f1_m: 0.5974 - precision_m: 0.5914 - recall_m: 0.6049 - val_loss: 0.6672 - val_acc: 0.5998 - val_f1_m: 0.6179 - val_precision_m: 0.6207 - val_recall_m: 0.6175\n",
            "Epoch 101/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6679 - acc: 0.5933 - f1_m: 0.5970 - precision_m: 0.5916 - recall_m: 0.6040 - val_loss: 0.6672 - val_acc: 0.5995 - val_f1_m: 0.6168 - val_precision_m: 0.6209 - val_recall_m: 0.6152\n",
            "Epoch 102/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6679 - acc: 0.5938 - f1_m: 0.5966 - precision_m: 0.5920 - recall_m: 0.6024 - val_loss: 0.6671 - val_acc: 0.5999 - val_f1_m: 0.6176 - val_precision_m: 0.6209 - val_recall_m: 0.6167\n",
            "Epoch 103/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6678 - acc: 0.5938 - f1_m: 0.5961 - precision_m: 0.5925 - recall_m: 0.6011 - val_loss: 0.6670 - val_acc: 0.5999 - val_f1_m: 0.6192 - val_precision_m: 0.6200 - val_recall_m: 0.6208\n",
            "Epoch 104/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6678 - acc: 0.5939 - f1_m: 0.5961 - precision_m: 0.5927 - recall_m: 0.6009 - val_loss: 0.6668 - val_acc: 0.6002 - val_f1_m: 0.6214 - val_precision_m: 0.6191 - val_recall_m: 0.6262\n",
            "Epoch 105/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6677 - acc: 0.5938 - f1_m: 0.5987 - precision_m: 0.5911 - recall_m: 0.6077 - val_loss: 0.6670 - val_acc: 0.6000 - val_f1_m: 0.6181 - val_precision_m: 0.6208 - val_recall_m: 0.6178\n",
            "Epoch 106/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6677 - acc: 0.5942 - f1_m: 0.5987 - precision_m: 0.5920 - recall_m: 0.6071 - val_loss: 0.6671 - val_acc: 0.6002 - val_f1_m: 0.6156 - val_precision_m: 0.6226 - val_recall_m: 0.6111\n",
            "Epoch 107/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6676 - acc: 0.5941 - f1_m: 0.5957 - precision_m: 0.5929 - recall_m: 0.5994 - val_loss: 0.6668 - val_acc: 0.6002 - val_f1_m: 0.6189 - val_precision_m: 0.6206 - val_recall_m: 0.6196\n",
            "Epoch 108/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6676 - acc: 0.5939 - f1_m: 0.5967 - precision_m: 0.5922 - recall_m: 0.6026 - val_loss: 0.6667 - val_acc: 0.6006 - val_f1_m: 0.6198 - val_precision_m: 0.6206 - val_recall_m: 0.6213\n",
            "Epoch 109/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6675 - acc: 0.5940 - f1_m: 0.5977 - precision_m: 0.5922 - recall_m: 0.6048 - val_loss: 0.6667 - val_acc: 0.6008 - val_f1_m: 0.6192 - val_precision_m: 0.6213 - val_recall_m: 0.6196\n",
            "Epoch 110/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6675 - acc: 0.5944 - f1_m: 0.5966 - precision_m: 0.5931 - recall_m: 0.6012 - val_loss: 0.6665 - val_acc: 0.6003 - val_f1_m: 0.6207 - val_precision_m: 0.6196 - val_recall_m: 0.6241\n",
            "Epoch 111/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6674 - acc: 0.5941 - f1_m: 0.5988 - precision_m: 0.5919 - recall_m: 0.6072 - val_loss: 0.6667 - val_acc: 0.6007 - val_f1_m: 0.6182 - val_precision_m: 0.6217 - val_recall_m: 0.6171\n",
            "Epoch 112/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6674 - acc: 0.5942 - f1_m: 0.5989 - precision_m: 0.5918 - recall_m: 0.6075 - val_loss: 0.6669 - val_acc: 0.5997 - val_f1_m: 0.6145 - val_precision_m: 0.6224 - val_recall_m: 0.6092\n",
            "Epoch 113/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6673 - acc: 0.5943 - f1_m: 0.5948 - precision_m: 0.5938 - recall_m: 0.5972 - val_loss: 0.6665 - val_acc: 0.6011 - val_f1_m: 0.6206 - val_precision_m: 0.6209 - val_recall_m: 0.6227\n",
            "Epoch 114/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6673 - acc: 0.5946 - f1_m: 0.5974 - precision_m: 0.5930 - recall_m: 0.6030 - val_loss: 0.6663 - val_acc: 0.6012 - val_f1_m: 0.6216 - val_precision_m: 0.6204 - val_recall_m: 0.6252\n",
            "Epoch 115/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6672 - acc: 0.5945 - f1_m: 0.5994 - precision_m: 0.5922 - recall_m: 0.6081 - val_loss: 0.6665 - val_acc: 0.6005 - val_f1_m: 0.6179 - val_precision_m: 0.6215 - val_recall_m: 0.6167\n",
            "Epoch 116/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6672 - acc: 0.5943 - f1_m: 0.5975 - precision_m: 0.5925 - recall_m: 0.6037 - val_loss: 0.6665 - val_acc: 0.6008 - val_f1_m: 0.6184 - val_precision_m: 0.6217 - val_recall_m: 0.6174\n",
            "Epoch 117/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6671 - acc: 0.5944 - f1_m: 0.5985 - precision_m: 0.5925 - recall_m: 0.6058 - val_loss: 0.6666 - val_acc: 0.6009 - val_f1_m: 0.6171 - val_precision_m: 0.6226 - val_recall_m: 0.6141\n",
            "Epoch 118/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6671 - acc: 0.5943 - f1_m: 0.5971 - precision_m: 0.5930 - recall_m: 0.6027 - val_loss: 0.6664 - val_acc: 0.6008 - val_f1_m: 0.6183 - val_precision_m: 0.6218 - val_recall_m: 0.6172\n",
            "Epoch 119/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6671 - acc: 0.5946 - f1_m: 0.5981 - precision_m: 0.5931 - recall_m: 0.6044 - val_loss: 0.6664 - val_acc: 0.6008 - val_f1_m: 0.6181 - val_precision_m: 0.6219 - val_recall_m: 0.6169\n",
            "Epoch 120/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6670 - acc: 0.5947 - f1_m: 0.5960 - precision_m: 0.5939 - recall_m: 0.5992 - val_loss: 0.6661 - val_acc: 0.6019 - val_f1_m: 0.6226 - val_precision_m: 0.6209 - val_recall_m: 0.6266\n",
            "Epoch 121/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6670 - acc: 0.5946 - f1_m: 0.5992 - precision_m: 0.5923 - recall_m: 0.6076 - val_loss: 0.6662 - val_acc: 0.6004 - val_f1_m: 0.6184 - val_precision_m: 0.6212 - val_recall_m: 0.6181\n",
            "Epoch 122/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6669 - acc: 0.5948 - f1_m: 0.5998 - precision_m: 0.5925 - recall_m: 0.6087 - val_loss: 0.6665 - val_acc: 0.6009 - val_f1_m: 0.6161 - val_precision_m: 0.6233 - val_recall_m: 0.6116\n",
            "Epoch 123/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6669 - acc: 0.5946 - f1_m: 0.5974 - precision_m: 0.5929 - recall_m: 0.6032 - val_loss: 0.6664 - val_acc: 0.6012 - val_f1_m: 0.6170 - val_precision_m: 0.6233 - val_recall_m: 0.6133\n",
            "Epoch 124/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6669 - acc: 0.5948 - f1_m: 0.5964 - precision_m: 0.5936 - recall_m: 0.6003 - val_loss: 0.6661 - val_acc: 0.5999 - val_f1_m: 0.6183 - val_precision_m: 0.6205 - val_recall_m: 0.6185\n",
            "Epoch 125/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6668 - acc: 0.5950 - f1_m: 0.5980 - precision_m: 0.5931 - recall_m: 0.6042 - val_loss: 0.6661 - val_acc: 0.6003 - val_f1_m: 0.6187 - val_precision_m: 0.6208 - val_recall_m: 0.6190\n",
            "Epoch 126/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6668 - acc: 0.5946 - f1_m: 0.5988 - precision_m: 0.5926 - recall_m: 0.6067 - val_loss: 0.6661 - val_acc: 0.6000 - val_f1_m: 0.6169 - val_precision_m: 0.6215 - val_recall_m: 0.6147\n",
            "Epoch 127/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6667 - acc: 0.5952 - f1_m: 0.5963 - precision_m: 0.5946 - recall_m: 0.5995 - val_loss: 0.6658 - val_acc: 0.6020 - val_f1_m: 0.6225 - val_precision_m: 0.6210 - val_recall_m: 0.6262\n",
            "Epoch 128/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6667 - acc: 0.5950 - f1_m: 0.5999 - precision_m: 0.5926 - recall_m: 0.6087 - val_loss: 0.6660 - val_acc: 0.6004 - val_f1_m: 0.6178 - val_precision_m: 0.6215 - val_recall_m: 0.6165\n",
            "Epoch 129/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6666 - acc: 0.5952 - f1_m: 0.5971 - precision_m: 0.5941 - recall_m: 0.6013 - val_loss: 0.6658 - val_acc: 0.6016 - val_f1_m: 0.6215 - val_precision_m: 0.6210 - val_recall_m: 0.6244\n",
            "Epoch 130/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6666 - acc: 0.5951 - f1_m: 0.5981 - precision_m: 0.5933 - recall_m: 0.6044 - val_loss: 0.6658 - val_acc: 0.6015 - val_f1_m: 0.6214 - val_precision_m: 0.6210 - val_recall_m: 0.6240\n",
            "Epoch 131/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6666 - acc: 0.5952 - f1_m: 0.5992 - precision_m: 0.5933 - recall_m: 0.6066 - val_loss: 0.6658 - val_acc: 0.6019 - val_f1_m: 0.6207 - val_precision_m: 0.6220 - val_recall_m: 0.6216\n",
            "Epoch 132/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6665 - acc: 0.5952 - f1_m: 0.5997 - precision_m: 0.5930 - recall_m: 0.6076 - val_loss: 0.6660 - val_acc: 0.6004 - val_f1_m: 0.6165 - val_precision_m: 0.6224 - val_recall_m: 0.6132\n",
            "Epoch 133/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6665 - acc: 0.5954 - f1_m: 0.5964 - precision_m: 0.5948 - recall_m: 0.5993 - val_loss: 0.6656 - val_acc: 0.6018 - val_f1_m: 0.6216 - val_precision_m: 0.6212 - val_recall_m: 0.6244\n",
            "Epoch 134/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6665 - acc: 0.5954 - f1_m: 0.5987 - precision_m: 0.5935 - recall_m: 0.6052 - val_loss: 0.6656 - val_acc: 0.6019 - val_f1_m: 0.6215 - val_precision_m: 0.6214 - val_recall_m: 0.6238\n",
            "Epoch 135/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6664 - acc: 0.5955 - f1_m: 0.5991 - precision_m: 0.5935 - recall_m: 0.6058 - val_loss: 0.6657 - val_acc: 0.6019 - val_f1_m: 0.6202 - val_precision_m: 0.6223 - val_recall_m: 0.6205\n",
            "Epoch 136/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6664 - acc: 0.5952 - f1_m: 0.5990 - precision_m: 0.5931 - recall_m: 0.6061 - val_loss: 0.6658 - val_acc: 0.6023 - val_f1_m: 0.6191 - val_precision_m: 0.6236 - val_recall_m: 0.6170\n",
            "Epoch 137/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6664 - acc: 0.5954 - f1_m: 0.5981 - precision_m: 0.5941 - recall_m: 0.6034 - val_loss: 0.6656 - val_acc: 0.6021 - val_f1_m: 0.6201 - val_precision_m: 0.6226 - val_recall_m: 0.6200\n",
            "Epoch 138/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6663 - acc: 0.5952 - f1_m: 0.5981 - precision_m: 0.5933 - recall_m: 0.6042 - val_loss: 0.6656 - val_acc: 0.6025 - val_f1_m: 0.6204 - val_precision_m: 0.6231 - val_recall_m: 0.6200\n",
            "Epoch 139/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6663 - acc: 0.5953 - f1_m: 0.5983 - precision_m: 0.5935 - recall_m: 0.6043 - val_loss: 0.6656 - val_acc: 0.6025 - val_f1_m: 0.6205 - val_precision_m: 0.6230 - val_recall_m: 0.6204\n",
            "Epoch 140/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6662 - acc: 0.5958 - f1_m: 0.6003 - precision_m: 0.5935 - recall_m: 0.6085 - val_loss: 0.6658 - val_acc: 0.6017 - val_f1_m: 0.6173 - val_precision_m: 0.6239 - val_recall_m: 0.6132\n",
            "Epoch 141/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6662 - acc: 0.5959 - f1_m: 0.5972 - precision_m: 0.5954 - recall_m: 0.6003 - val_loss: 0.6655 - val_acc: 0.6022 - val_f1_m: 0.6204 - val_precision_m: 0.6226 - val_recall_m: 0.6205\n",
            "Epoch 142/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6662 - acc: 0.5957 - f1_m: 0.5976 - precision_m: 0.5945 - recall_m: 0.6019 - val_loss: 0.6653 - val_acc: 0.6015 - val_f1_m: 0.6222 - val_precision_m: 0.6205 - val_recall_m: 0.6262\n",
            "Epoch 143/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6661 - acc: 0.5954 - f1_m: 0.5984 - precision_m: 0.5939 - recall_m: 0.6044 - val_loss: 0.6653 - val_acc: 0.6020 - val_f1_m: 0.6223 - val_precision_m: 0.6211 - val_recall_m: 0.6257\n",
            "Epoch 144/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6661 - acc: 0.5962 - f1_m: 0.5992 - precision_m: 0.5948 - recall_m: 0.6051 - val_loss: 0.6652 - val_acc: 0.6018 - val_f1_m: 0.6222 - val_precision_m: 0.6210 - val_recall_m: 0.6257\n",
            "Epoch 145/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6661 - acc: 0.5958 - f1_m: 0.6011 - precision_m: 0.5932 - recall_m: 0.6104 - val_loss: 0.6656 - val_acc: 0.6024 - val_f1_m: 0.6181 - val_precision_m: 0.6244 - val_recall_m: 0.6144\n",
            "Epoch 146/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6660 - acc: 0.5959 - f1_m: 0.5969 - precision_m: 0.5952 - recall_m: 0.6000 - val_loss: 0.6652 - val_acc: 0.6024 - val_f1_m: 0.6221 - val_precision_m: 0.6218 - val_recall_m: 0.6248\n",
            "Epoch 147/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6660 - acc: 0.5961 - f1_m: 0.5990 - precision_m: 0.5948 - recall_m: 0.6047 - val_loss: 0.6652 - val_acc: 0.6021 - val_f1_m: 0.6221 - val_precision_m: 0.6214 - val_recall_m: 0.6250\n",
            "Epoch 148/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6660 - acc: 0.5957 - f1_m: 0.5990 - precision_m: 0.5941 - recall_m: 0.6053 - val_loss: 0.6652 - val_acc: 0.6025 - val_f1_m: 0.6218 - val_precision_m: 0.6222 - val_recall_m: 0.6238\n",
            "Epoch 149/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6659 - acc: 0.5959 - f1_m: 0.5982 - precision_m: 0.5948 - recall_m: 0.6028 - val_loss: 0.6650 - val_acc: 0.6023 - val_f1_m: 0.6231 - val_precision_m: 0.6211 - val_recall_m: 0.6275\n",
            "Epoch 150/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6659 - acc: 0.5956 - f1_m: 0.6000 - precision_m: 0.5936 - recall_m: 0.6076 - val_loss: 0.6652 - val_acc: 0.6028 - val_f1_m: 0.6214 - val_precision_m: 0.6229 - val_recall_m: 0.6224\n",
            "Epoch 151/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6659 - acc: 0.5959 - f1_m: 0.5995 - precision_m: 0.5941 - recall_m: 0.6063 - val_loss: 0.6652 - val_acc: 0.6031 - val_f1_m: 0.6208 - val_precision_m: 0.6237 - val_recall_m: 0.6204\n",
            "Epoch 152/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6659 - acc: 0.5958 - f1_m: 0.5992 - precision_m: 0.5942 - recall_m: 0.6054 - val_loss: 0.6652 - val_acc: 0.6030 - val_f1_m: 0.6202 - val_precision_m: 0.6239 - val_recall_m: 0.6190\n",
            "Epoch 153/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6658 - acc: 0.5963 - f1_m: 0.5988 - precision_m: 0.5950 - recall_m: 0.6042 - val_loss: 0.6651 - val_acc: 0.6031 - val_f1_m: 0.6212 - val_precision_m: 0.6235 - val_recall_m: 0.6214\n",
            "Epoch 154/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6658 - acc: 0.5963 - f1_m: 0.5986 - precision_m: 0.5949 - recall_m: 0.6036 - val_loss: 0.6650 - val_acc: 0.6028 - val_f1_m: 0.6220 - val_precision_m: 0.6225 - val_recall_m: 0.6238\n",
            "Epoch 155/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6658 - acc: 0.5960 - f1_m: 0.5999 - precision_m: 0.5939 - recall_m: 0.6075 - val_loss: 0.6651 - val_acc: 0.6034 - val_f1_m: 0.6205 - val_precision_m: 0.6243 - val_recall_m: 0.6190\n",
            "Epoch 156/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6657 - acc: 0.5963 - f1_m: 0.5985 - precision_m: 0.5950 - recall_m: 0.6033 - val_loss: 0.6650 - val_acc: 0.6026 - val_f1_m: 0.6212 - val_precision_m: 0.6228 - val_recall_m: 0.6220\n",
            "Epoch 157/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6657 - acc: 0.5963 - f1_m: 0.6011 - precision_m: 0.5940 - recall_m: 0.6096 - val_loss: 0.6653 - val_acc: 0.6038 - val_f1_m: 0.6186 - val_precision_m: 0.6262 - val_recall_m: 0.6135\n",
            "Epoch 158/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6657 - acc: 0.5961 - f1_m: 0.5976 - precision_m: 0.5954 - recall_m: 0.6012 - val_loss: 0.6650 - val_acc: 0.6029 - val_f1_m: 0.6206 - val_precision_m: 0.6236 - val_recall_m: 0.6201\n",
            "Epoch 159/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6656 - acc: 0.5962 - f1_m: 0.5998 - precision_m: 0.5944 - recall_m: 0.6069 - val_loss: 0.6651 - val_acc: 0.6034 - val_f1_m: 0.6199 - val_precision_m: 0.6249 - val_recall_m: 0.6174\n",
            "Epoch 160/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6656 - acc: 0.5966 - f1_m: 0.5980 - precision_m: 0.5952 - recall_m: 0.6020 - val_loss: 0.6649 - val_acc: 0.6037 - val_f1_m: 0.6226 - val_precision_m: 0.6235 - val_recall_m: 0.6241\n",
            "Epoch 161/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6656 - acc: 0.5967 - f1_m: 0.5986 - precision_m: 0.5955 - recall_m: 0.6033 - val_loss: 0.6647 - val_acc: 0.6035 - val_f1_m: 0.6247 - val_precision_m: 0.6219 - val_recall_m: 0.6297\n",
            "Epoch 162/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6655 - acc: 0.5969 - f1_m: 0.6018 - precision_m: 0.5945 - recall_m: 0.6103 - val_loss: 0.6649 - val_acc: 0.6031 - val_f1_m: 0.6204 - val_precision_m: 0.6240 - val_recall_m: 0.6193\n",
            "Epoch 163/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6655 - acc: 0.5964 - f1_m: 0.5984 - precision_m: 0.5955 - recall_m: 0.6031 - val_loss: 0.6647 - val_acc: 0.6046 - val_f1_m: 0.6238 - val_precision_m: 0.6240 - val_recall_m: 0.6261\n",
            "Epoch 164/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6655 - acc: 0.5964 - f1_m: 0.6000 - precision_m: 0.5944 - recall_m: 0.6070 - val_loss: 0.6649 - val_acc: 0.6034 - val_f1_m: 0.6208 - val_precision_m: 0.6242 - val_recall_m: 0.6200\n",
            "Epoch 165/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6655 - acc: 0.5965 - f1_m: 0.5991 - precision_m: 0.5951 - recall_m: 0.6044 - val_loss: 0.6648 - val_acc: 0.6041 - val_f1_m: 0.6222 - val_precision_m: 0.6244 - val_recall_m: 0.6226\n",
            "Epoch 166/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6654 - acc: 0.5966 - f1_m: 0.6012 - precision_m: 0.5941 - recall_m: 0.6100 - val_loss: 0.6651 - val_acc: 0.6035 - val_f1_m: 0.6181 - val_precision_m: 0.6262 - val_recall_m: 0.6127\n",
            "Epoch 167/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6654 - acc: 0.5965 - f1_m: 0.5986 - precision_m: 0.5950 - recall_m: 0.6033 - val_loss: 0.6649 - val_acc: 0.6037 - val_f1_m: 0.6195 - val_precision_m: 0.6257 - val_recall_m: 0.6159\n",
            "Epoch 168/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6654 - acc: 0.5969 - f1_m: 0.5987 - precision_m: 0.5958 - recall_m: 0.6029 - val_loss: 0.6647 - val_acc: 0.6042 - val_f1_m: 0.6219 - val_precision_m: 0.6248 - val_recall_m: 0.6215\n",
            "Epoch 169/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6654 - acc: 0.5971 - f1_m: 0.5997 - precision_m: 0.5956 - recall_m: 0.6054 - val_loss: 0.6647 - val_acc: 0.6047 - val_f1_m: 0.6229 - val_precision_m: 0.6248 - val_recall_m: 0.6235\n",
            "Epoch 170/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6653 - acc: 0.5967 - f1_m: 0.5994 - precision_m: 0.5955 - recall_m: 0.6048 - val_loss: 0.6646 - val_acc: 0.6045 - val_f1_m: 0.6236 - val_precision_m: 0.6240 - val_recall_m: 0.6256\n",
            "Epoch 171/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6653 - acc: 0.5969 - f1_m: 0.6004 - precision_m: 0.5950 - recall_m: 0.6072 - val_loss: 0.6647 - val_acc: 0.6046 - val_f1_m: 0.6224 - val_precision_m: 0.6251 - val_recall_m: 0.6222\n",
            "Epoch 172/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6653 - acc: 0.5973 - f1_m: 0.6008 - precision_m: 0.5954 - recall_m: 0.6077 - val_loss: 0.6647 - val_acc: 0.6040 - val_f1_m: 0.6206 - val_precision_m: 0.6253 - val_recall_m: 0.6185\n",
            "Epoch 173/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6652 - acc: 0.5971 - f1_m: 0.5981 - precision_m: 0.5967 - recall_m: 0.6010 - val_loss: 0.6644 - val_acc: 0.6042 - val_f1_m: 0.6240 - val_precision_m: 0.6233 - val_recall_m: 0.6271\n",
            "Epoch 174/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6652 - acc: 0.5972 - f1_m: 0.6002 - precision_m: 0.5955 - recall_m: 0.6064 - val_loss: 0.6644 - val_acc: 0.6047 - val_f1_m: 0.6241 - val_precision_m: 0.6240 - val_recall_m: 0.6266\n",
            "Epoch 175/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6652 - acc: 0.5972 - f1_m: 0.6009 - precision_m: 0.5950 - recall_m: 0.6080 - val_loss: 0.6645 - val_acc: 0.6052 - val_f1_m: 0.6236 - val_precision_m: 0.6250 - val_recall_m: 0.6247\n",
            "Epoch 176/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6652 - acc: 0.5967 - f1_m: 0.6010 - precision_m: 0.5944 - recall_m: 0.6090 - val_loss: 0.6647 - val_acc: 0.6039 - val_f1_m: 0.6199 - val_precision_m: 0.6258 - val_recall_m: 0.6166\n",
            "Epoch 177/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6651 - acc: 0.5968 - f1_m: 0.6000 - precision_m: 0.5952 - recall_m: 0.6063 - val_loss: 0.6647 - val_acc: 0.6044 - val_f1_m: 0.6198 - val_precision_m: 0.6266 - val_recall_m: 0.6157\n",
            "Epoch 178/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6651 - acc: 0.5970 - f1_m: 0.5970 - precision_m: 0.5967 - recall_m: 0.5986 - val_loss: 0.6643 - val_acc: 0.6040 - val_f1_m: 0.6243 - val_precision_m: 0.6229 - val_recall_m: 0.6280\n",
            "Epoch 179/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6651 - acc: 0.5968 - f1_m: 0.6024 - precision_m: 0.5941 - recall_m: 0.6125 - val_loss: 0.6647 - val_acc: 0.6041 - val_f1_m: 0.6199 - val_precision_m: 0.6261 - val_recall_m: 0.6163\n",
            "Epoch 180/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6651 - acc: 0.5973 - f1_m: 0.5976 - precision_m: 0.5971 - recall_m: 0.5996 - val_loss: 0.6642 - val_acc: 0.6045 - val_f1_m: 0.6243 - val_precision_m: 0.6236 - val_recall_m: 0.6273\n",
            "Epoch 181/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6650 - acc: 0.5974 - f1_m: 0.6014 - precision_m: 0.5952 - recall_m: 0.6092 - val_loss: 0.6644 - val_acc: 0.6051 - val_f1_m: 0.6227 - val_precision_m: 0.6257 - val_recall_m: 0.6222\n",
            "Epoch 182/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6650 - acc: 0.5977 - f1_m: 0.6003 - precision_m: 0.5961 - recall_m: 0.6059 - val_loss: 0.6644 - val_acc: 0.6052 - val_f1_m: 0.6230 - val_precision_m: 0.6256 - val_recall_m: 0.6229\n",
            "Epoch 183/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6650 - acc: 0.5973 - f1_m: 0.5992 - precision_m: 0.5965 - recall_m: 0.6034 - val_loss: 0.6642 - val_acc: 0.6046 - val_f1_m: 0.6243 - val_precision_m: 0.6237 - val_recall_m: 0.6273\n",
            "Epoch 184/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6650 - acc: 0.5977 - f1_m: 0.6014 - precision_m: 0.5954 - recall_m: 0.6089 - val_loss: 0.6644 - val_acc: 0.6051 - val_f1_m: 0.6228 - val_precision_m: 0.6256 - val_recall_m: 0.6226\n",
            "Epoch 185/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6649 - acc: 0.5978 - f1_m: 0.6004 - precision_m: 0.5962 - recall_m: 0.6059 - val_loss: 0.6643 - val_acc: 0.6054 - val_f1_m: 0.6234 - val_precision_m: 0.6256 - val_recall_m: 0.6236\n",
            "Epoch 186/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6649 - acc: 0.5977 - f1_m: 0.6006 - precision_m: 0.5960 - recall_m: 0.6062 - val_loss: 0.6642 - val_acc: 0.6053 - val_f1_m: 0.6237 - val_precision_m: 0.6252 - val_recall_m: 0.6245\n",
            "Epoch 187/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6649 - acc: 0.5979 - f1_m: 0.6011 - precision_m: 0.5961 - recall_m: 0.6075 - val_loss: 0.6643 - val_acc: 0.6046 - val_f1_m: 0.6223 - val_precision_m: 0.6252 - val_recall_m: 0.6218\n",
            "Epoch 188/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6649 - acc: 0.5978 - f1_m: 0.5996 - precision_m: 0.5963 - recall_m: 0.6041 - val_loss: 0.6641 - val_acc: 0.6051 - val_f1_m: 0.6241 - val_precision_m: 0.6246 - val_recall_m: 0.6259\n",
            "Epoch 189/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6648 - acc: 0.5974 - f1_m: 0.6004 - precision_m: 0.5957 - recall_m: 0.6067 - val_loss: 0.6642 - val_acc: 0.6049 - val_f1_m: 0.6233 - val_precision_m: 0.6249 - val_recall_m: 0.6242\n",
            "Epoch 190/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6648 - acc: 0.5978 - f1_m: 0.6006 - precision_m: 0.5961 - recall_m: 0.6062 - val_loss: 0.6641 - val_acc: 0.6045 - val_f1_m: 0.6229 - val_precision_m: 0.6245 - val_recall_m: 0.6236\n",
            "Epoch 191/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6648 - acc: 0.5976 - f1_m: 0.6006 - precision_m: 0.5958 - recall_m: 0.6067 - val_loss: 0.6642 - val_acc: 0.6045 - val_f1_m: 0.6225 - val_precision_m: 0.6249 - val_recall_m: 0.6225\n",
            "Epoch 192/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6648 - acc: 0.5979 - f1_m: 0.6000 - precision_m: 0.5967 - recall_m: 0.6044 - val_loss: 0.6640 - val_acc: 0.6048 - val_f1_m: 0.6240 - val_precision_m: 0.6243 - val_recall_m: 0.6261\n",
            "Epoch 193/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6647 - acc: 0.5977 - f1_m: 0.6008 - precision_m: 0.5958 - recall_m: 0.6070 - val_loss: 0.6641 - val_acc: 0.6044 - val_f1_m: 0.6225 - val_precision_m: 0.6247 - val_recall_m: 0.6227\n",
            "Epoch 194/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6647 - acc: 0.5981 - f1_m: 0.6004 - precision_m: 0.5967 - recall_m: 0.6054 - val_loss: 0.6640 - val_acc: 0.6044 - val_f1_m: 0.6229 - val_precision_m: 0.6243 - val_recall_m: 0.6240\n",
            "Epoch 195/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6647 - acc: 0.5974 - f1_m: 0.5992 - precision_m: 0.5964 - recall_m: 0.6034 - val_loss: 0.6638 - val_acc: 0.6045 - val_f1_m: 0.6252 - val_precision_m: 0.6231 - val_recall_m: 0.6297\n",
            "Epoch 196/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6647 - acc: 0.5979 - f1_m: 0.6015 - precision_m: 0.5963 - recall_m: 0.6080 - val_loss: 0.6640 - val_acc: 0.6047 - val_f1_m: 0.6237 - val_precision_m: 0.6244 - val_recall_m: 0.6254\n",
            "Epoch 197/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6646 - acc: 0.5982 - f1_m: 0.6004 - precision_m: 0.5969 - recall_m: 0.6051 - val_loss: 0.6639 - val_acc: 0.6040 - val_f1_m: 0.6240 - val_precision_m: 0.6231 - val_recall_m: 0.6274\n",
            "Epoch 198/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6646 - acc: 0.5979 - f1_m: 0.6006 - precision_m: 0.5964 - recall_m: 0.6062 - val_loss: 0.6638 - val_acc: 0.6042 - val_f1_m: 0.6240 - val_precision_m: 0.6234 - val_recall_m: 0.6270\n",
            "Epoch 199/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6646 - acc: 0.5978 - f1_m: 0.6012 - precision_m: 0.5958 - recall_m: 0.6079 - val_loss: 0.6640 - val_acc: 0.6048 - val_f1_m: 0.6232 - val_precision_m: 0.6248 - val_recall_m: 0.6240\n",
            "Epoch 200/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6646 - acc: 0.5978 - f1_m: 0.5989 - precision_m: 0.5969 - recall_m: 0.6025 - val_loss: 0.6637 - val_acc: 0.6043 - val_f1_m: 0.6256 - val_precision_m: 0.6226 - val_recall_m: 0.6311\n",
            "Epoch 201/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6646 - acc: 0.5981 - f1_m: 0.6026 - precision_m: 0.5961 - recall_m: 0.6109 - val_loss: 0.6640 - val_acc: 0.6050 - val_f1_m: 0.6227 - val_precision_m: 0.6255 - val_recall_m: 0.6224\n",
            "Epoch 202/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6645 - acc: 0.5979 - f1_m: 0.6009 - precision_m: 0.5959 - recall_m: 0.6071 - val_loss: 0.6640 - val_acc: 0.6052 - val_f1_m: 0.6220 - val_precision_m: 0.6263 - val_recall_m: 0.6202\n",
            "Epoch 203/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6645 - acc: 0.5982 - f1_m: 0.6010 - precision_m: 0.5968 - recall_m: 0.6067 - val_loss: 0.6640 - val_acc: 0.6055 - val_f1_m: 0.6222 - val_precision_m: 0.6267 - val_recall_m: 0.6203\n",
            "Epoch 204/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6645 - acc: 0.5979 - f1_m: 0.6001 - precision_m: 0.5966 - recall_m: 0.6049 - val_loss: 0.6639 - val_acc: 0.6048 - val_f1_m: 0.6225 - val_precision_m: 0.6253 - val_recall_m: 0.6222\n",
            "Epoch 205/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6645 - acc: 0.5979 - f1_m: 0.6002 - precision_m: 0.5967 - recall_m: 0.6050 - val_loss: 0.6638 - val_acc: 0.6051 - val_f1_m: 0.6234 - val_precision_m: 0.6252 - val_recall_m: 0.6240\n",
            "Epoch 206/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6644 - acc: 0.5983 - f1_m: 0.5998 - precision_m: 0.5973 - recall_m: 0.6036 - val_loss: 0.6636 - val_acc: 0.6040 - val_f1_m: 0.6251 - val_precision_m: 0.6225 - val_recall_m: 0.6302\n",
            "Epoch 207/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6644 - acc: 0.5981 - f1_m: 0.6018 - precision_m: 0.5961 - recall_m: 0.6086 - val_loss: 0.6637 - val_acc: 0.6046 - val_f1_m: 0.6236 - val_precision_m: 0.6243 - val_recall_m: 0.6254\n",
            "Epoch 208/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6644 - acc: 0.5983 - f1_m: 0.6015 - precision_m: 0.5963 - recall_m: 0.6079 - val_loss: 0.6638 - val_acc: 0.6050 - val_f1_m: 0.6227 - val_precision_m: 0.6255 - val_recall_m: 0.6224\n",
            "Epoch 209/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6644 - acc: 0.5985 - f1_m: 0.6007 - precision_m: 0.5972 - recall_m: 0.6056 - val_loss: 0.6637 - val_acc: 0.6050 - val_f1_m: 0.6238 - val_precision_m: 0.6248 - val_recall_m: 0.6252\n",
            "Epoch 210/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6644 - acc: 0.5983 - f1_m: 0.6017 - precision_m: 0.5963 - recall_m: 0.6088 - val_loss: 0.6639 - val_acc: 0.6059 - val_f1_m: 0.6225 - val_precision_m: 0.6272 - val_recall_m: 0.6203\n",
            "Epoch 211/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6643 - acc: 0.5981 - f1_m: 0.5991 - precision_m: 0.5969 - recall_m: 0.6024 - val_loss: 0.6636 - val_acc: 0.6044 - val_f1_m: 0.6236 - val_precision_m: 0.6240 - val_recall_m: 0.6257\n",
            "Epoch 212/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6643 - acc: 0.5983 - f1_m: 0.6009 - precision_m: 0.5967 - recall_m: 0.6063 - val_loss: 0.6636 - val_acc: 0.6041 - val_f1_m: 0.6234 - val_precision_m: 0.6237 - val_recall_m: 0.6255\n",
            "Epoch 213/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6643 - acc: 0.5984 - f1_m: 0.6003 - precision_m: 0.5971 - recall_m: 0.6047 - val_loss: 0.6634 - val_acc: 0.6038 - val_f1_m: 0.6251 - val_precision_m: 0.6223 - val_recall_m: 0.6303\n",
            "Epoch 214/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6643 - acc: 0.5981 - f1_m: 0.6019 - precision_m: 0.5962 - recall_m: 0.6091 - val_loss: 0.6636 - val_acc: 0.6047 - val_f1_m: 0.6234 - val_precision_m: 0.6246 - val_recall_m: 0.6247\n",
            "Epoch 215/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6642 - acc: 0.5985 - f1_m: 0.6026 - precision_m: 0.5962 - recall_m: 0.6105 - val_loss: 0.6638 - val_acc: 0.6062 - val_f1_m: 0.6222 - val_precision_m: 0.6279 - val_recall_m: 0.6191\n",
            "Epoch 216/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6642 - acc: 0.5984 - f1_m: 0.5998 - precision_m: 0.5976 - recall_m: 0.6033 - val_loss: 0.6636 - val_acc: 0.6043 - val_f1_m: 0.6232 - val_precision_m: 0.6242 - val_recall_m: 0.6247\n",
            "Epoch 217/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6642 - acc: 0.5982 - f1_m: 0.6003 - precision_m: 0.5968 - recall_m: 0.6049 - val_loss: 0.6635 - val_acc: 0.6039 - val_f1_m: 0.6237 - val_precision_m: 0.6232 - val_recall_m: 0.6266\n",
            "Epoch 218/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6642 - acc: 0.5982 - f1_m: 0.6023 - precision_m: 0.5960 - recall_m: 0.6096 - val_loss: 0.6637 - val_acc: 0.6057 - val_f1_m: 0.6223 - val_precision_m: 0.6271 - val_recall_m: 0.6201\n",
            "Epoch 219/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6642 - acc: 0.5985 - f1_m: 0.6011 - precision_m: 0.5972 - recall_m: 0.6066 - val_loss: 0.6637 - val_acc: 0.6056 - val_f1_m: 0.6223 - val_precision_m: 0.6267 - val_recall_m: 0.6205\n",
            "Epoch 220/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6641 - acc: 0.5985 - f1_m: 0.5999 - precision_m: 0.5975 - recall_m: 0.6035 - val_loss: 0.6634 - val_acc: 0.6042 - val_f1_m: 0.6244 - val_precision_m: 0.6232 - val_recall_m: 0.6280\n",
            "Epoch 221/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6641 - acc: 0.5984 - f1_m: 0.6018 - precision_m: 0.5965 - recall_m: 0.6082 - val_loss: 0.6635 - val_acc: 0.6048 - val_f1_m: 0.6233 - val_precision_m: 0.6249 - val_recall_m: 0.6242\n",
            "Epoch 222/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6641 - acc: 0.5980 - f1_m: 0.6014 - precision_m: 0.5961 - recall_m: 0.6079 - val_loss: 0.6636 - val_acc: 0.6054 - val_f1_m: 0.6220 - val_precision_m: 0.6267 - val_recall_m: 0.6200\n",
            "Epoch 223/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6641 - acc: 0.5981 - f1_m: 0.6003 - precision_m: 0.5970 - recall_m: 0.6051 - val_loss: 0.6635 - val_acc: 0.6048 - val_f1_m: 0.6229 - val_precision_m: 0.6252 - val_recall_m: 0.6231\n",
            "Epoch 224/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6641 - acc: 0.5987 - f1_m: 0.6017 - precision_m: 0.5969 - recall_m: 0.6076 - val_loss: 0.6636 - val_acc: 0.6055 - val_f1_m: 0.6221 - val_precision_m: 0.6267 - val_recall_m: 0.6201\n",
            "Epoch 225/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6640 - acc: 0.5982 - f1_m: 0.5998 - precision_m: 0.5973 - recall_m: 0.6036 - val_loss: 0.6633 - val_acc: 0.6047 - val_f1_m: 0.6242 - val_precision_m: 0.6241 - val_recall_m: 0.6268\n",
            "Epoch 226/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6640 - acc: 0.5984 - f1_m: 0.6003 - precision_m: 0.5973 - recall_m: 0.6046 - val_loss: 0.6632 - val_acc: 0.6037 - val_f1_m: 0.6250 - val_precision_m: 0.6222 - val_recall_m: 0.6303\n",
            "Epoch 227/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6640 - acc: 0.5985 - f1_m: 0.6017 - precision_m: 0.5966 - recall_m: 0.6080 - val_loss: 0.6632 - val_acc: 0.6045 - val_f1_m: 0.6246 - val_precision_m: 0.6235 - val_recall_m: 0.6282\n",
            "Epoch 228/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6640 - acc: 0.5981 - f1_m: 0.6020 - precision_m: 0.5960 - recall_m: 0.6091 - val_loss: 0.6634 - val_acc: 0.6054 - val_f1_m: 0.6228 - val_precision_m: 0.6262 - val_recall_m: 0.6219\n",
            "Epoch 229/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6639 - acc: 0.5989 - f1_m: 0.6025 - precision_m: 0.5971 - recall_m: 0.6093 - val_loss: 0.6636 - val_acc: 0.6055 - val_f1_m: 0.6212 - val_precision_m: 0.6274 - val_recall_m: 0.6177\n",
            "Epoch 230/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6639 - acc: 0.5986 - f1_m: 0.6009 - precision_m: 0.5971 - recall_m: 0.6057 - val_loss: 0.6635 - val_acc: 0.6058 - val_f1_m: 0.6225 - val_precision_m: 0.6271 - val_recall_m: 0.6205\n",
            "Epoch 231/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6639 - acc: 0.5982 - f1_m: 0.6008 - precision_m: 0.5967 - recall_m: 0.6062 - val_loss: 0.6634 - val_acc: 0.6057 - val_f1_m: 0.6226 - val_precision_m: 0.6269 - val_recall_m: 0.6208\n",
            "Epoch 232/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6639 - acc: 0.5983 - f1_m: 0.5994 - precision_m: 0.5974 - recall_m: 0.6024 - val_loss: 0.6631 - val_acc: 0.6041 - val_f1_m: 0.6247 - val_precision_m: 0.6229 - val_recall_m: 0.6289\n",
            "Epoch 233/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6639 - acc: 0.5982 - f1_m: 0.6021 - precision_m: 0.5959 - recall_m: 0.6094 - val_loss: 0.6633 - val_acc: 0.6052 - val_f1_m: 0.6228 - val_precision_m: 0.6258 - val_recall_m: 0.6224\n",
            "Epoch 234/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6639 - acc: 0.5984 - f1_m: 0.6015 - precision_m: 0.5968 - recall_m: 0.6076 - val_loss: 0.6633 - val_acc: 0.6057 - val_f1_m: 0.6230 - val_precision_m: 0.6265 - val_recall_m: 0.6220\n",
            "Epoch 235/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6638 - acc: 0.5986 - f1_m: 0.6012 - precision_m: 0.5971 - recall_m: 0.6065 - val_loss: 0.6633 - val_acc: 0.6058 - val_f1_m: 0.6231 - val_precision_m: 0.6267 - val_recall_m: 0.6220\n",
            "Epoch 236/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6638 - acc: 0.5983 - f1_m: 0.6002 - precision_m: 0.5973 - recall_m: 0.6045 - val_loss: 0.6631 - val_acc: 0.6047 - val_f1_m: 0.6240 - val_precision_m: 0.6243 - val_recall_m: 0.6262\n",
            "Epoch 237/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6638 - acc: 0.5983 - f1_m: 0.6015 - precision_m: 0.5968 - recall_m: 0.6075 - val_loss: 0.6631 - val_acc: 0.6049 - val_f1_m: 0.6238 - val_precision_m: 0.6247 - val_recall_m: 0.6254\n",
            "Epoch 238/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6638 - acc: 0.5982 - f1_m: 0.6021 - precision_m: 0.5958 - recall_m: 0.6097 - val_loss: 0.6633 - val_acc: 0.6062 - val_f1_m: 0.6225 - val_precision_m: 0.6277 - val_recall_m: 0.6199\n",
            "Epoch 239/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6638 - acc: 0.5982 - f1_m: 0.6004 - precision_m: 0.5971 - recall_m: 0.6047 - val_loss: 0.6632 - val_acc: 0.6056 - val_f1_m: 0.6238 - val_precision_m: 0.6257 - val_recall_m: 0.6243\n",
            "Epoch 240/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6637 - acc: 0.5982 - f1_m: 0.6009 - precision_m: 0.5965 - recall_m: 0.6066 - val_loss: 0.6632 - val_acc: 0.6057 - val_f1_m: 0.6235 - val_precision_m: 0.6262 - val_recall_m: 0.6234\n",
            "Epoch 241/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6637 - acc: 0.5984 - f1_m: 0.6007 - precision_m: 0.5968 - recall_m: 0.6057 - val_loss: 0.6631 - val_acc: 0.6056 - val_f1_m: 0.6239 - val_precision_m: 0.6256 - val_recall_m: 0.6248\n",
            "Epoch 242/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6637 - acc: 0.5986 - f1_m: 0.6024 - precision_m: 0.5965 - recall_m: 0.6097 - val_loss: 0.6633 - val_acc: 0.6062 - val_f1_m: 0.6222 - val_precision_m: 0.6278 - val_recall_m: 0.6192\n",
            "Epoch 243/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6637 - acc: 0.5984 - f1_m: 0.6007 - precision_m: 0.5967 - recall_m: 0.6058 - val_loss: 0.6632 - val_acc: 0.6063 - val_f1_m: 0.6232 - val_precision_m: 0.6273 - val_recall_m: 0.6217\n",
            "Epoch 244/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6637 - acc: 0.5983 - f1_m: 0.6014 - precision_m: 0.5966 - recall_m: 0.6076 - val_loss: 0.6632 - val_acc: 0.6067 - val_f1_m: 0.6232 - val_precision_m: 0.6279 - val_recall_m: 0.6210\n",
            "Epoch 245/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5987 - f1_m: 0.5995 - precision_m: 0.5976 - recall_m: 0.6024 - val_loss: 0.6629 - val_acc: 0.6049 - val_f1_m: 0.6251 - val_precision_m: 0.6239 - val_recall_m: 0.6287\n",
            "Epoch 246/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5991 - f1_m: 0.6024 - precision_m: 0.5975 - recall_m: 0.6087 - val_loss: 0.6630 - val_acc: 0.6046 - val_f1_m: 0.6240 - val_precision_m: 0.6242 - val_recall_m: 0.6263\n",
            "Epoch 247/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5987 - f1_m: 0.6008 - precision_m: 0.5970 - recall_m: 0.6057 - val_loss: 0.6629 - val_acc: 0.6048 - val_f1_m: 0.6248 - val_precision_m: 0.6239 - val_recall_m: 0.6282\n",
            "Epoch 248/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5988 - f1_m: 0.6026 - precision_m: 0.5967 - recall_m: 0.6097 - val_loss: 0.6630 - val_acc: 0.6056 - val_f1_m: 0.6233 - val_precision_m: 0.6260 - val_recall_m: 0.6231\n",
            "Epoch 249/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5985 - f1_m: 0.6012 - precision_m: 0.5972 - recall_m: 0.6068 - val_loss: 0.6630 - val_acc: 0.6057 - val_f1_m: 0.6234 - val_precision_m: 0.6261 - val_recall_m: 0.6231\n",
            "Epoch 250/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6636 - acc: 0.5994 - f1_m: 0.6020 - precision_m: 0.5979 - recall_m: 0.6075 - val_loss: 0.6630 - val_acc: 0.6060 - val_f1_m: 0.6242 - val_precision_m: 0.6262 - val_recall_m: 0.6247\n",
            "Epoch 251/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5988 - f1_m: 0.6016 - precision_m: 0.5971 - recall_m: 0.6072 - val_loss: 0.6629 - val_acc: 0.6058 - val_f1_m: 0.6241 - val_precision_m: 0.6259 - val_recall_m: 0.6248\n",
            "Epoch 252/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5990 - f1_m: 0.6030 - precision_m: 0.5968 - recall_m: 0.6105 - val_loss: 0.6632 - val_acc: 0.6057 - val_f1_m: 0.6213 - val_precision_m: 0.6274 - val_recall_m: 0.6178\n",
            "Epoch 253/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5990 - f1_m: 0.6013 - precision_m: 0.5974 - recall_m: 0.6064 - val_loss: 0.6631 - val_acc: 0.6059 - val_f1_m: 0.6226 - val_precision_m: 0.6271 - val_recall_m: 0.6206\n",
            "Epoch 254/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5988 - f1_m: 0.6008 - precision_m: 0.5977 - recall_m: 0.6054 - val_loss: 0.6629 - val_acc: 0.6058 - val_f1_m: 0.6243 - val_precision_m: 0.6258 - val_recall_m: 0.6252\n",
            "Epoch 255/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5992 - f1_m: 0.6018 - precision_m: 0.5979 - recall_m: 0.6068 - val_loss: 0.6628 - val_acc: 0.6061 - val_f1_m: 0.6255 - val_precision_m: 0.6255 - val_recall_m: 0.6280\n",
            "Epoch 256/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6635 - acc: 0.5987 - f1_m: 0.6022 - precision_m: 0.5969 - recall_m: 0.6090 - val_loss: 0.6629 - val_acc: 0.6059 - val_f1_m: 0.6241 - val_precision_m: 0.6261 - val_recall_m: 0.6247\n",
            "Epoch 257/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6634 - acc: 0.5990 - f1_m: 0.6015 - precision_m: 0.5976 - recall_m: 0.6066 - val_loss: 0.6628 - val_acc: 0.6059 - val_f1_m: 0.6248 - val_precision_m: 0.6257 - val_recall_m: 0.6264\n",
            "Epoch 258/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6634 - acc: 0.5989 - f1_m: 0.6014 - precision_m: 0.5975 - recall_m: 0.6068 - val_loss: 0.6627 - val_acc: 0.6064 - val_f1_m: 0.6260 - val_precision_m: 0.6256 - val_recall_m: 0.6289\n",
            "Epoch 259/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6634 - acc: 0.5989 - f1_m: 0.6022 - precision_m: 0.5971 - recall_m: 0.6085 - val_loss: 0.6628 - val_acc: 0.6062 - val_f1_m: 0.6246 - val_precision_m: 0.6262 - val_recall_m: 0.6256\n",
            "Epoch 260/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6634 - acc: 0.5992 - f1_m: 0.6024 - precision_m: 0.5973 - recall_m: 0.6086 - val_loss: 0.6628 - val_acc: 0.6060 - val_f1_m: 0.6238 - val_precision_m: 0.6264 - val_recall_m: 0.6238\n",
            "Epoch 261/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6634 - acc: 0.5994 - f1_m: 0.6029 - precision_m: 0.5973 - recall_m: 0.6098 - val_loss: 0.6629 - val_acc: 0.6059 - val_f1_m: 0.6225 - val_precision_m: 0.6271 - val_recall_m: 0.6205\n",
            "Epoch 262/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5992 - f1_m: 0.6037 - precision_m: 0.5971 - recall_m: 0.6116 - val_loss: 0.6632 - val_acc: 0.6049 - val_f1_m: 0.6180 - val_precision_m: 0.6286 - val_recall_m: 0.6103\n",
            "Epoch 263/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5993 - f1_m: 0.6007 - precision_m: 0.5985 - recall_m: 0.6042 - val_loss: 0.6629 - val_acc: 0.6060 - val_f1_m: 0.6228 - val_precision_m: 0.6271 - val_recall_m: 0.6210\n",
            "Epoch 264/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5993 - f1_m: 0.6028 - precision_m: 0.5974 - recall_m: 0.6097 - val_loss: 0.6630 - val_acc: 0.6053 - val_f1_m: 0.6206 - val_precision_m: 0.6274 - val_recall_m: 0.6165\n",
            "Epoch 265/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5997 - f1_m: 0.6012 - precision_m: 0.5988 - recall_m: 0.6051 - val_loss: 0.6627 - val_acc: 0.6060 - val_f1_m: 0.6241 - val_precision_m: 0.6262 - val_recall_m: 0.6247\n",
            "Epoch 266/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5997 - f1_m: 0.6013 - precision_m: 0.5981 - recall_m: 0.6058 - val_loss: 0.6625 - val_acc: 0.6059 - val_f1_m: 0.6266 - val_precision_m: 0.6244 - val_recall_m: 0.6312\n",
            "Epoch 267/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6633 - acc: 0.5992 - f1_m: 0.6031 - precision_m: 0.5971 - recall_m: 0.6105 - val_loss: 0.6627 - val_acc: 0.6064 - val_f1_m: 0.6248 - val_precision_m: 0.6263 - val_recall_m: 0.6257\n",
            "Epoch 268/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6632 - acc: 0.5996 - f1_m: 0.6021 - precision_m: 0.5979 - recall_m: 0.6079 - val_loss: 0.6626 - val_acc: 0.6068 - val_f1_m: 0.6259 - val_precision_m: 0.6262 - val_recall_m: 0.6280\n",
            "Epoch 269/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6632 - acc: 0.5995 - f1_m: 0.6013 - precision_m: 0.5984 - recall_m: 0.6057 - val_loss: 0.6624 - val_acc: 0.6062 - val_f1_m: 0.6277 - val_precision_m: 0.6242 - val_recall_m: 0.6338\n",
            "Epoch 270/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6632 - acc: 0.5997 - f1_m: 0.6044 - precision_m: 0.5972 - recall_m: 0.6130 - val_loss: 0.6627 - val_acc: 0.6053 - val_f1_m: 0.6227 - val_precision_m: 0.6260 - val_recall_m: 0.6219\n",
            "Epoch 271/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6632 - acc: 0.5999 - f1_m: 0.6034 - precision_m: 0.5981 - recall_m: 0.6101 - val_loss: 0.6628 - val_acc: 0.6059 - val_f1_m: 0.6222 - val_precision_m: 0.6273 - val_recall_m: 0.6198\n",
            "Epoch 272/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6632 - acc: 0.5997 - f1_m: 0.6004 - precision_m: 0.5991 - recall_m: 0.6031 - val_loss: 0.6624 - val_acc: 0.6065 - val_f1_m: 0.6278 - val_precision_m: 0.6245 - val_recall_m: 0.6334\n",
            "Epoch 273/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.5993 - f1_m: 0.6046 - precision_m: 0.5965 - recall_m: 0.6142 - val_loss: 0.6628 - val_acc: 0.6057 - val_f1_m: 0.6216 - val_precision_m: 0.6274 - val_recall_m: 0.6185\n",
            "Epoch 274/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.5997 - f1_m: 0.6009 - precision_m: 0.5987 - recall_m: 0.6045 - val_loss: 0.6625 - val_acc: 0.6077 - val_f1_m: 0.6267 - val_precision_m: 0.6269 - val_recall_m: 0.6290\n",
            "Epoch 275/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.6001 - f1_m: 0.6039 - precision_m: 0.5983 - recall_m: 0.6111 - val_loss: 0.6626 - val_acc: 0.6058 - val_f1_m: 0.6237 - val_precision_m: 0.6262 - val_recall_m: 0.6238\n",
            "Epoch 276/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.5995 - f1_m: 0.6031 - precision_m: 0.5974 - recall_m: 0.6101 - val_loss: 0.6627 - val_acc: 0.6057 - val_f1_m: 0.6223 - val_precision_m: 0.6269 - val_recall_m: 0.6203\n",
            "Epoch 277/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.5999 - f1_m: 0.6025 - precision_m: 0.5986 - recall_m: 0.6072 - val_loss: 0.6626 - val_acc: 0.6065 - val_f1_m: 0.6244 - val_precision_m: 0.6267 - val_recall_m: 0.6247\n",
            "Epoch 278/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6631 - acc: 0.6000 - f1_m: 0.6037 - precision_m: 0.5981 - recall_m: 0.6104 - val_loss: 0.6626 - val_acc: 0.6050 - val_f1_m: 0.6221 - val_precision_m: 0.6259 - val_recall_m: 0.6208\n",
            "Epoch 279/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.5993 - f1_m: 0.6012 - precision_m: 0.5985 - recall_m: 0.6054 - val_loss: 0.6624 - val_acc: 0.6080 - val_f1_m: 0.6275 - val_precision_m: 0.6268 - val_recall_m: 0.6306\n",
            "Epoch 280/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.5993 - f1_m: 0.6028 - precision_m: 0.5971 - recall_m: 0.6097 - val_loss: 0.6625 - val_acc: 0.6071 - val_f1_m: 0.6254 - val_precision_m: 0.6269 - val_recall_m: 0.6264\n",
            "Epoch 281/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.5996 - f1_m: 0.6028 - precision_m: 0.5978 - recall_m: 0.6088 - val_loss: 0.6625 - val_acc: 0.6063 - val_f1_m: 0.6243 - val_precision_m: 0.6265 - val_recall_m: 0.6245\n",
            "Epoch 282/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.5995 - f1_m: 0.6031 - precision_m: 0.5976 - recall_m: 0.6101 - val_loss: 0.6626 - val_acc: 0.6060 - val_f1_m: 0.6224 - val_precision_m: 0.6274 - val_recall_m: 0.6200\n",
            "Epoch 283/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.5999 - f1_m: 0.6026 - precision_m: 0.5985 - recall_m: 0.6084 - val_loss: 0.6625 - val_acc: 0.6055 - val_f1_m: 0.6228 - val_precision_m: 0.6262 - val_recall_m: 0.6220\n",
            "Epoch 284/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6630 - acc: 0.6001 - f1_m: 0.6032 - precision_m: 0.5983 - recall_m: 0.6093 - val_loss: 0.6625 - val_acc: 0.6058 - val_f1_m: 0.6229 - val_precision_m: 0.6267 - val_recall_m: 0.6215\n",
            "Epoch 285/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.5999 - f1_m: 0.6015 - precision_m: 0.5987 - recall_m: 0.6056 - val_loss: 0.6623 - val_acc: 0.6080 - val_f1_m: 0.6274 - val_precision_m: 0.6270 - val_recall_m: 0.6303\n",
            "Epoch 286/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.6000 - f1_m: 0.6038 - precision_m: 0.5976 - recall_m: 0.6113 - val_loss: 0.6625 - val_acc: 0.6059 - val_f1_m: 0.6236 - val_precision_m: 0.6263 - val_recall_m: 0.6234\n",
            "Epoch 287/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.6002 - f1_m: 0.6021 - precision_m: 0.5986 - recall_m: 0.6069 - val_loss: 0.6623 - val_acc: 0.6076 - val_f1_m: 0.6266 - val_precision_m: 0.6269 - val_recall_m: 0.6287\n",
            "Epoch 288/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.5997 - f1_m: 0.6027 - precision_m: 0.5980 - recall_m: 0.6089 - val_loss: 0.6623 - val_acc: 0.6075 - val_f1_m: 0.6264 - val_precision_m: 0.6268 - val_recall_m: 0.6285\n",
            "Epoch 289/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.5996 - f1_m: 0.6037 - precision_m: 0.5974 - recall_m: 0.6116 - val_loss: 0.6625 - val_acc: 0.6059 - val_f1_m: 0.6223 - val_precision_m: 0.6273 - val_recall_m: 0.6200\n",
            "Epoch 290/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.5994 - f1_m: 0.6029 - precision_m: 0.5974 - recall_m: 0.6096 - val_loss: 0.6626 - val_acc: 0.6057 - val_f1_m: 0.6214 - val_precision_m: 0.6276 - val_recall_m: 0.6179\n",
            "Epoch 291/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6629 - acc: 0.6001 - f1_m: 0.6022 - precision_m: 0.5989 - recall_m: 0.6069 - val_loss: 0.6624 - val_acc: 0.6069 - val_f1_m: 0.6247 - val_precision_m: 0.6272 - val_recall_m: 0.6247\n",
            "Epoch 292/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.5995 - f1_m: 0.6022 - precision_m: 0.5978 - recall_m: 0.6079 - val_loss: 0.6623 - val_acc: 0.6070 - val_f1_m: 0.6251 - val_precision_m: 0.6270 - val_recall_m: 0.6257\n",
            "Epoch 293/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.5997 - f1_m: 0.6030 - precision_m: 0.5978 - recall_m: 0.6097 - val_loss: 0.6624 - val_acc: 0.6074 - val_f1_m: 0.6251 - val_precision_m: 0.6276 - val_recall_m: 0.6252\n",
            "Epoch 294/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.6002 - f1_m: 0.6019 - precision_m: 0.5992 - recall_m: 0.6060 - val_loss: 0.6621 - val_acc: 0.6075 - val_f1_m: 0.6276 - val_precision_m: 0.6260 - val_recall_m: 0.6317\n",
            "Epoch 295/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.5995 - f1_m: 0.6041 - precision_m: 0.5971 - recall_m: 0.6126 - val_loss: 0.6624 - val_acc: 0.6066 - val_f1_m: 0.6238 - val_precision_m: 0.6273 - val_recall_m: 0.6229\n",
            "Epoch 296/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.6000 - f1_m: 0.6023 - precision_m: 0.5988 - recall_m: 0.6071 - val_loss: 0.6622 - val_acc: 0.6077 - val_f1_m: 0.6265 - val_precision_m: 0.6270 - val_recall_m: 0.6283\n",
            "Epoch 297/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6628 - acc: 0.5998 - f1_m: 0.6038 - precision_m: 0.5978 - recall_m: 0.6112 - val_loss: 0.6624 - val_acc: 0.6062 - val_f1_m: 0.6232 - val_precision_m: 0.6272 - val_recall_m: 0.6217\n",
            "Epoch 298/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.5998 - f1_m: 0.6035 - precision_m: 0.5979 - recall_m: 0.6105 - val_loss: 0.6625 - val_acc: 0.6057 - val_f1_m: 0.6213 - val_precision_m: 0.6277 - val_recall_m: 0.6175\n",
            "Epoch 299/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.6002 - f1_m: 0.6030 - precision_m: 0.5987 - recall_m: 0.6088 - val_loss: 0.6624 - val_acc: 0.6059 - val_f1_m: 0.6223 - val_precision_m: 0.6273 - val_recall_m: 0.6200\n",
            "Epoch 300/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.6002 - f1_m: 0.6021 - precision_m: 0.5992 - recall_m: 0.6063 - val_loss: 0.6621 - val_acc: 0.6075 - val_f1_m: 0.6265 - val_precision_m: 0.6267 - val_recall_m: 0.6289\n",
            "Epoch 301/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.5998 - f1_m: 0.6029 - precision_m: 0.5976 - recall_m: 0.6093 - val_loss: 0.6622 - val_acc: 0.6076 - val_f1_m: 0.6263 - val_precision_m: 0.6270 - val_recall_m: 0.6281\n",
            "Epoch 302/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.6001 - f1_m: 0.6031 - precision_m: 0.5978 - recall_m: 0.6097 - val_loss: 0.6622 - val_acc: 0.6076 - val_f1_m: 0.6255 - val_precision_m: 0.6276 - val_recall_m: 0.6259\n",
            "Epoch 303/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6627 - acc: 0.5999 - f1_m: 0.6042 - precision_m: 0.5975 - recall_m: 0.6125 - val_loss: 0.6624 - val_acc: 0.6057 - val_f1_m: 0.6211 - val_precision_m: 0.6278 - val_recall_m: 0.6170\n",
            "Epoch 304/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6007 - f1_m: 0.6022 - precision_m: 0.5994 - recall_m: 0.6063 - val_loss: 0.6622 - val_acc: 0.6074 - val_f1_m: 0.6254 - val_precision_m: 0.6273 - val_recall_m: 0.6260\n",
            "Epoch 305/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6003 - f1_m: 0.6032 - precision_m: 0.5987 - recall_m: 0.6093 - val_loss: 0.6622 - val_acc: 0.6073 - val_f1_m: 0.6254 - val_precision_m: 0.6272 - val_recall_m: 0.6260\n",
            "Epoch 306/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6004 - f1_m: 0.6046 - precision_m: 0.5983 - recall_m: 0.6124 - val_loss: 0.6623 - val_acc: 0.6063 - val_f1_m: 0.6224 - val_precision_m: 0.6279 - val_recall_m: 0.6195\n",
            "Epoch 307/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6007 - f1_m: 0.6012 - precision_m: 0.6004 - recall_m: 0.6031 - val_loss: 0.6618 - val_acc: 0.6074 - val_f1_m: 0.6289 - val_precision_m: 0.6250 - val_recall_m: 0.6352\n",
            "Epoch 308/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6002 - f1_m: 0.6045 - precision_m: 0.5975 - recall_m: 0.6127 - val_loss: 0.6621 - val_acc: 0.6075 - val_f1_m: 0.6262 - val_precision_m: 0.6270 - val_recall_m: 0.6278\n",
            "Epoch 309/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6626 - acc: 0.6001 - f1_m: 0.6034 - precision_m: 0.5982 - recall_m: 0.6100 - val_loss: 0.6621 - val_acc: 0.6072 - val_f1_m: 0.6254 - val_precision_m: 0.6270 - val_recall_m: 0.6262\n",
            "Epoch 310/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6626 - acc: 0.6000 - f1_m: 0.6040 - precision_m: 0.5978 - recall_m: 0.6115 - val_loss: 0.6622 - val_acc: 0.6068 - val_f1_m: 0.6233 - val_precision_m: 0.6280 - val_recall_m: 0.6212\n",
            "Epoch 311/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.6003 - f1_m: 0.6037 - precision_m: 0.5985 - recall_m: 0.6102 - val_loss: 0.6623 - val_acc: 0.6064 - val_f1_m: 0.6223 - val_precision_m: 0.6281 - val_recall_m: 0.6191\n",
            "Epoch 312/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.6006 - f1_m: 0.6020 - precision_m: 0.5998 - recall_m: 0.6058 - val_loss: 0.6620 - val_acc: 0.6077 - val_f1_m: 0.6271 - val_precision_m: 0.6266 - val_recall_m: 0.6299\n",
            "Epoch 313/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.5998 - f1_m: 0.6041 - precision_m: 0.5974 - recall_m: 0.6124 - val_loss: 0.6622 - val_acc: 0.6067 - val_f1_m: 0.6232 - val_precision_m: 0.6279 - val_recall_m: 0.6212\n",
            "Epoch 314/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.6008 - f1_m: 0.6022 - precision_m: 0.5999 - recall_m: 0.6056 - val_loss: 0.6619 - val_acc: 0.6084 - val_f1_m: 0.6283 - val_precision_m: 0.6270 - val_recall_m: 0.6320\n",
            "Epoch 315/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.6000 - f1_m: 0.6042 - precision_m: 0.5981 - recall_m: 0.6121 - val_loss: 0.6621 - val_acc: 0.6064 - val_f1_m: 0.6243 - val_precision_m: 0.6266 - val_recall_m: 0.6245\n",
            "Epoch 316/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6625 - acc: 0.6000 - f1_m: 0.6034 - precision_m: 0.5983 - recall_m: 0.6098 - val_loss: 0.6621 - val_acc: 0.6064 - val_f1_m: 0.6241 - val_precision_m: 0.6268 - val_recall_m: 0.6238\n",
            "Epoch 317/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6002 - f1_m: 0.6035 - precision_m: 0.5984 - recall_m: 0.6099 - val_loss: 0.6621 - val_acc: 0.6064 - val_f1_m: 0.6241 - val_precision_m: 0.6268 - val_recall_m: 0.6238\n",
            "Epoch 318/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6008 - f1_m: 0.6029 - precision_m: 0.5997 - recall_m: 0.6075 - val_loss: 0.6618 - val_acc: 0.6080 - val_f1_m: 0.6278 - val_precision_m: 0.6267 - val_recall_m: 0.6314\n",
            "Epoch 319/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6001 - f1_m: 0.6047 - precision_m: 0.5976 - recall_m: 0.6130 - val_loss: 0.6621 - val_acc: 0.6068 - val_f1_m: 0.6234 - val_precision_m: 0.6279 - val_recall_m: 0.6215\n",
            "Epoch 320/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6006 - f1_m: 0.6030 - precision_m: 0.5994 - recall_m: 0.6080 - val_loss: 0.6620 - val_acc: 0.6071 - val_f1_m: 0.6256 - val_precision_m: 0.6268 - val_recall_m: 0.6268\n",
            "Epoch 321/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6006 - f1_m: 0.6036 - precision_m: 0.5987 - recall_m: 0.6099 - val_loss: 0.6619 - val_acc: 0.6073 - val_f1_m: 0.6260 - val_precision_m: 0.6268 - val_recall_m: 0.6276\n",
            "Epoch 322/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6007 - f1_m: 0.6044 - precision_m: 0.5985 - recall_m: 0.6118 - val_loss: 0.6620 - val_acc: 0.6064 - val_f1_m: 0.6239 - val_precision_m: 0.6269 - val_recall_m: 0.6233\n",
            "Epoch 323/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6624 - acc: 0.6005 - f1_m: 0.6033 - precision_m: 0.5985 - recall_m: 0.6096 - val_loss: 0.6620 - val_acc: 0.6070 - val_f1_m: 0.6249 - val_precision_m: 0.6271 - val_recall_m: 0.6250\n",
            "Epoch 324/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6004 - f1_m: 0.6033 - precision_m: 0.5987 - recall_m: 0.6092 - val_loss: 0.6619 - val_acc: 0.6068 - val_f1_m: 0.6252 - val_precision_m: 0.6266 - val_recall_m: 0.6262\n",
            "Epoch 325/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6007 - f1_m: 0.6038 - precision_m: 0.5990 - recall_m: 0.6099 - val_loss: 0.6619 - val_acc: 0.6067 - val_f1_m: 0.6251 - val_precision_m: 0.6265 - val_recall_m: 0.6261\n",
            "Epoch 326/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6005 - f1_m: 0.6049 - precision_m: 0.5978 - recall_m: 0.6132 - val_loss: 0.6621 - val_acc: 0.6066 - val_f1_m: 0.6225 - val_precision_m: 0.6293 - val_recall_m: 0.6180\n",
            "Epoch 327/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6008 - f1_m: 0.6036 - precision_m: 0.5992 - recall_m: 0.6094 - val_loss: 0.6621 - val_acc: 0.6067 - val_f1_m: 0.6229 - val_precision_m: 0.6280 - val_recall_m: 0.6204\n",
            "Epoch 328/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6010 - f1_m: 0.6039 - precision_m: 0.5991 - recall_m: 0.6098 - val_loss: 0.6620 - val_acc: 0.6067 - val_f1_m: 0.6240 - val_precision_m: 0.6272 - val_recall_m: 0.6233\n",
            "Epoch 329/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6623 - acc: 0.6003 - f1_m: 0.6037 - precision_m: 0.5985 - recall_m: 0.6104 - val_loss: 0.6620 - val_acc: 0.6068 - val_f1_m: 0.6235 - val_precision_m: 0.6278 - val_recall_m: 0.6217\n",
            "Epoch 330/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6009 - f1_m: 0.6024 - precision_m: 0.5998 - recall_m: 0.6062 - val_loss: 0.6617 - val_acc: 0.6080 - val_f1_m: 0.6278 - val_precision_m: 0.6268 - val_recall_m: 0.6312\n",
            "Epoch 331/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6007 - f1_m: 0.6039 - precision_m: 0.5990 - recall_m: 0.6101 - val_loss: 0.6617 - val_acc: 0.6078 - val_f1_m: 0.6273 - val_precision_m: 0.6267 - val_recall_m: 0.6303\n",
            "Epoch 332/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6004 - f1_m: 0.6043 - precision_m: 0.5980 - recall_m: 0.6119 - val_loss: 0.6618 - val_acc: 0.6068 - val_f1_m: 0.6251 - val_precision_m: 0.6267 - val_recall_m: 0.6259\n",
            "Epoch 333/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6007 - f1_m: 0.6034 - precision_m: 0.5989 - recall_m: 0.6089 - val_loss: 0.6617 - val_acc: 0.6070 - val_f1_m: 0.6258 - val_precision_m: 0.6265 - val_recall_m: 0.6276\n",
            "Epoch 334/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6006 - f1_m: 0.6038 - precision_m: 0.5987 - recall_m: 0.6101 - val_loss: 0.6618 - val_acc: 0.6066 - val_f1_m: 0.6253 - val_precision_m: 0.6262 - val_recall_m: 0.6268\n",
            "Epoch 335/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6005 - f1_m: 0.6053 - precision_m: 0.5978 - recall_m: 0.6141 - val_loss: 0.6621 - val_acc: 0.6065 - val_f1_m: 0.6222 - val_precision_m: 0.6305 - val_recall_m: 0.6160\n",
            "Epoch 336/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6012 - f1_m: 0.6036 - precision_m: 0.5996 - recall_m: 0.6087 - val_loss: 0.6619 - val_acc: 0.6074 - val_f1_m: 0.6243 - val_precision_m: 0.6281 - val_recall_m: 0.6229\n",
            "Epoch 337/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6011 - f1_m: 0.6044 - precision_m: 0.5992 - recall_m: 0.6111 - val_loss: 0.6619 - val_acc: 0.6075 - val_f1_m: 0.6243 - val_precision_m: 0.6282 - val_recall_m: 0.6229\n",
            "Epoch 338/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6010 - f1_m: 0.6034 - precision_m: 0.5996 - recall_m: 0.6088 - val_loss: 0.6618 - val_acc: 0.6072 - val_f1_m: 0.6250 - val_precision_m: 0.6273 - val_recall_m: 0.6252\n",
            "Epoch 339/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6008 - f1_m: 0.6047 - precision_m: 0.5987 - recall_m: 0.6119 - val_loss: 0.6619 - val_acc: 0.6076 - val_f1_m: 0.6247 - val_precision_m: 0.6292 - val_recall_m: 0.6224\n",
            "Epoch 340/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6013 - f1_m: 0.6048 - precision_m: 0.5993 - recall_m: 0.6115 - val_loss: 0.6619 - val_acc: 0.6075 - val_f1_m: 0.6244 - val_precision_m: 0.6293 - val_recall_m: 0.6217\n",
            "Epoch 341/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6014 - f1_m: 0.6037 - precision_m: 0.5999 - recall_m: 0.6089 - val_loss: 0.6617 - val_acc: 0.6069 - val_f1_m: 0.6248 - val_precision_m: 0.6270 - val_recall_m: 0.6250\n",
            "Epoch 342/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6010 - f1_m: 0.6042 - precision_m: 0.5992 - recall_m: 0.6106 - val_loss: 0.6617 - val_acc: 0.6069 - val_f1_m: 0.6247 - val_precision_m: 0.6271 - val_recall_m: 0.6248\n",
            "Epoch 343/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.6009 - f1_m: 0.6044 - precision_m: 0.5988 - recall_m: 0.6113 - val_loss: 0.6617 - val_acc: 0.6070 - val_f1_m: 0.6246 - val_precision_m: 0.6273 - val_recall_m: 0.6243\n",
            "Epoch 344/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6011 - f1_m: 0.6042 - precision_m: 0.5993 - recall_m: 0.6105 - val_loss: 0.6617 - val_acc: 0.6072 - val_f1_m: 0.6248 - val_precision_m: 0.6274 - val_recall_m: 0.6247\n",
            "Epoch 345/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6012 - f1_m: 0.6044 - precision_m: 0.5996 - recall_m: 0.6104 - val_loss: 0.6617 - val_acc: 0.6070 - val_f1_m: 0.6250 - val_precision_m: 0.6270 - val_recall_m: 0.6254\n",
            "Epoch 346/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6011 - f1_m: 0.6050 - precision_m: 0.5987 - recall_m: 0.6126 - val_loss: 0.6618 - val_acc: 0.6072 - val_f1_m: 0.6242 - val_precision_m: 0.6289 - val_recall_m: 0.6217\n",
            "Epoch 347/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6011 - f1_m: 0.6037 - precision_m: 0.5994 - recall_m: 0.6091 - val_loss: 0.6617 - val_acc: 0.6071 - val_f1_m: 0.6246 - val_precision_m: 0.6274 - val_recall_m: 0.6243\n",
            "Epoch 348/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6011 - f1_m: 0.6029 - precision_m: 0.5999 - recall_m: 0.6070 - val_loss: 0.6614 - val_acc: 0.6080 - val_f1_m: 0.6283 - val_precision_m: 0.6262 - val_recall_m: 0.6327\n",
            "Epoch 349/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6007 - f1_m: 0.6053 - precision_m: 0.5980 - recall_m: 0.6140 - val_loss: 0.6617 - val_acc: 0.6069 - val_f1_m: 0.6245 - val_precision_m: 0.6283 - val_recall_m: 0.6227\n",
            "Epoch 350/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6017 - f1_m: 0.6049 - precision_m: 0.5999 - recall_m: 0.6113 - val_loss: 0.6617 - val_acc: 0.6071 - val_f1_m: 0.6247 - val_precision_m: 0.6285 - val_recall_m: 0.6230\n",
            "Epoch 351/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6620 - acc: 0.6010 - f1_m: 0.6032 - precision_m: 0.5996 - recall_m: 0.6080 - val_loss: 0.6615 - val_acc: 0.6080 - val_f1_m: 0.6272 - val_precision_m: 0.6269 - val_recall_m: 0.6299\n",
            "Epoch 352/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6013 - f1_m: 0.6057 - precision_m: 0.5990 - recall_m: 0.6137 - val_loss: 0.6617 - val_acc: 0.6069 - val_f1_m: 0.6244 - val_precision_m: 0.6284 - val_recall_m: 0.6225\n",
            "Epoch 353/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6013 - f1_m: 0.6048 - precision_m: 0.5995 - recall_m: 0.6114 - val_loss: 0.6617 - val_acc: 0.6071 - val_f1_m: 0.6248 - val_precision_m: 0.6295 - val_recall_m: 0.6220\n",
            "Epoch 354/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6014 - f1_m: 0.6037 - precision_m: 0.5997 - recall_m: 0.6090 - val_loss: 0.6615 - val_acc: 0.6082 - val_f1_m: 0.6267 - val_precision_m: 0.6277 - val_recall_m: 0.6281\n",
            "Epoch 355/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6011 - f1_m: 0.6054 - precision_m: 0.5988 - recall_m: 0.6135 - val_loss: 0.6617 - val_acc: 0.6073 - val_f1_m: 0.6245 - val_precision_m: 0.6299 - val_recall_m: 0.6211\n",
            "Epoch 356/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6015 - f1_m: 0.6022 - precision_m: 0.6008 - recall_m: 0.6051 - val_loss: 0.6613 - val_acc: 0.6083 - val_f1_m: 0.6294 - val_precision_m: 0.6260 - val_recall_m: 0.6353\n",
            "Epoch 357/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6009 - f1_m: 0.6050 - precision_m: 0.5985 - recall_m: 0.6130 - val_loss: 0.6615 - val_acc: 0.6080 - val_f1_m: 0.6268 - val_precision_m: 0.6273 - val_recall_m: 0.6287\n",
            "Epoch 358/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6619 - acc: 0.6012 - f1_m: 0.6052 - precision_m: 0.5992 - recall_m: 0.6127 - val_loss: 0.6616 - val_acc: 0.6074 - val_f1_m: 0.6256 - val_precision_m: 0.6282 - val_recall_m: 0.6249\n",
            "Epoch 359/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6015 - f1_m: 0.6043 - precision_m: 0.5996 - recall_m: 0.6102 - val_loss: 0.6615 - val_acc: 0.6080 - val_f1_m: 0.6266 - val_precision_m: 0.6276 - val_recall_m: 0.6280\n",
            "Epoch 360/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6011 - f1_m: 0.6040 - precision_m: 0.5992 - recall_m: 0.6101 - val_loss: 0.6614 - val_acc: 0.6079 - val_f1_m: 0.6268 - val_precision_m: 0.6271 - val_recall_m: 0.6288\n",
            "Epoch 361/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6013 - f1_m: 0.6046 - precision_m: 0.5995 - recall_m: 0.6112 - val_loss: 0.6614 - val_acc: 0.6079 - val_f1_m: 0.6268 - val_precision_m: 0.6271 - val_recall_m: 0.6288\n",
            "Epoch 362/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6015 - f1_m: 0.6058 - precision_m: 0.5990 - recall_m: 0.6140 - val_loss: 0.6616 - val_acc: 0.6077 - val_f1_m: 0.6251 - val_precision_m: 0.6302 - val_recall_m: 0.6220\n",
            "Epoch 363/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6017 - f1_m: 0.6052 - precision_m: 0.5998 - recall_m: 0.6121 - val_loss: 0.6617 - val_acc: 0.6075 - val_f1_m: 0.6243 - val_precision_m: 0.6305 - val_recall_m: 0.6200\n",
            "Epoch 364/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6016 - f1_m: 0.6042 - precision_m: 0.6000 - recall_m: 0.6096 - val_loss: 0.6615 - val_acc: 0.6075 - val_f1_m: 0.6261 - val_precision_m: 0.6292 - val_recall_m: 0.6249\n",
            "Epoch 365/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6618 - acc: 0.6019 - f1_m: 0.6046 - precision_m: 0.6002 - recall_m: 0.6102 - val_loss: 0.6614 - val_acc: 0.6078 - val_f1_m: 0.6259 - val_precision_m: 0.6275 - val_recall_m: 0.6267\n",
            "Epoch 366/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6617 - acc: 0.6015 - f1_m: 0.6057 - precision_m: 0.5993 - recall_m: 0.6134 - val_loss: 0.6616 - val_acc: 0.6078 - val_f1_m: 0.6255 - val_precision_m: 0.6300 - val_recall_m: 0.6230\n",
            "Epoch 367/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6617 - acc: 0.6011 - f1_m: 0.6046 - precision_m: 0.5989 - recall_m: 0.6115 - val_loss: 0.6617 - val_acc: 0.6080 - val_f1_m: 0.6245 - val_precision_m: 0.6311 - val_recall_m: 0.6198\n",
            "Epoch 368/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6617 - acc: 0.6018 - f1_m: 0.6030 - precision_m: 0.6003 - recall_m: 0.6070 - val_loss: 0.6613 - val_acc: 0.6085 - val_f1_m: 0.6276 - val_precision_m: 0.6275 - val_recall_m: 0.6300\n",
            "Epoch 369/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6617 - acc: 0.6015 - f1_m: 0.6057 - precision_m: 0.5989 - recall_m: 0.6139 - val_loss: 0.6616 - val_acc: 0.6080 - val_f1_m: 0.6249 - val_precision_m: 0.6308 - val_recall_m: 0.6208\n",
            "Epoch 370/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6617 - acc: 0.6015 - f1_m: 0.6040 - precision_m: 0.6002 - recall_m: 0.6091 - val_loss: 0.6614 - val_acc: 0.6079 - val_f1_m: 0.6267 - val_precision_m: 0.6294 - val_recall_m: 0.6258\n",
            "Epoch 371/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6617 - acc: 0.6016 - f1_m: 0.6045 - precision_m: 0.5997 - recall_m: 0.6104 - val_loss: 0.6613 - val_acc: 0.6078 - val_f1_m: 0.6265 - val_precision_m: 0.6271 - val_recall_m: 0.6281\n",
            "Epoch 372/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6617 - acc: 0.6018 - f1_m: 0.6057 - precision_m: 0.5997 - recall_m: 0.6132 - val_loss: 0.6615 - val_acc: 0.6079 - val_f1_m: 0.6260 - val_precision_m: 0.6298 - val_recall_m: 0.6240\n",
            "Epoch 373/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6617 - acc: 0.6014 - f1_m: 0.6050 - precision_m: 0.5994 - recall_m: 0.6119 - val_loss: 0.6615 - val_acc: 0.6074 - val_f1_m: 0.6247 - val_precision_m: 0.6300 - val_recall_m: 0.6214\n",
            "Epoch 374/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6616 - acc: 0.6026 - f1_m: 0.6056 - precision_m: 0.6010 - recall_m: 0.6115 - val_loss: 0.6615 - val_acc: 0.6080 - val_f1_m: 0.6259 - val_precision_m: 0.6300 - val_recall_m: 0.6237\n",
            "Epoch 375/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6019 - f1_m: 0.6040 - precision_m: 0.6002 - recall_m: 0.6090 - val_loss: 0.6613 - val_acc: 0.6082 - val_f1_m: 0.6271 - val_precision_m: 0.6274 - val_recall_m: 0.6292\n",
            "Epoch 376/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6013 - f1_m: 0.6050 - precision_m: 0.5991 - recall_m: 0.6121 - val_loss: 0.6614 - val_acc: 0.6079 - val_f1_m: 0.6267 - val_precision_m: 0.6294 - val_recall_m: 0.6258\n",
            "Epoch 377/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6017 - f1_m: 0.6052 - precision_m: 0.5999 - recall_m: 0.6120 - val_loss: 0.6614 - val_acc: 0.6082 - val_f1_m: 0.6265 - val_precision_m: 0.6301 - val_recall_m: 0.6247\n",
            "Epoch 378/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6019 - f1_m: 0.6048 - precision_m: 0.6003 - recall_m: 0.6106 - val_loss: 0.6613 - val_acc: 0.6084 - val_f1_m: 0.6279 - val_precision_m: 0.6294 - val_recall_m: 0.6283\n",
            "Epoch 379/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6019 - f1_m: 0.6044 - precision_m: 0.6007 - recall_m: 0.6093 - val_loss: 0.6611 - val_acc: 0.6086 - val_f1_m: 0.6286 - val_precision_m: 0.6270 - val_recall_m: 0.6325\n",
            "Epoch 380/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6015 - f1_m: 0.6072 - precision_m: 0.5983 - recall_m: 0.6180 - val_loss: 0.6616 - val_acc: 0.6078 - val_f1_m: 0.6235 - val_precision_m: 0.6314 - val_recall_m: 0.6177\n",
            "Epoch 381/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6616 - acc: 0.6020 - f1_m: 0.6038 - precision_m: 0.6010 - recall_m: 0.6077 - val_loss: 0.6613 - val_acc: 0.6081 - val_f1_m: 0.6276 - val_precision_m: 0.6292 - val_recall_m: 0.6279\n",
            "Epoch 382/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6023 - f1_m: 0.6076 - precision_m: 0.5991 - recall_m: 0.6178 - val_loss: 0.6617 - val_acc: 0.6070 - val_f1_m: 0.6212 - val_precision_m: 0.6317 - val_recall_m: 0.6130\n",
            "Epoch 383/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6027 - f1_m: 0.6045 - precision_m: 0.6013 - recall_m: 0.6089 - val_loss: 0.6615 - val_acc: 0.6081 - val_f1_m: 0.6251 - val_precision_m: 0.6309 - val_recall_m: 0.6212\n",
            "Epoch 384/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6022 - f1_m: 0.6034 - precision_m: 0.6015 - recall_m: 0.6069 - val_loss: 0.6610 - val_acc: 0.6086 - val_f1_m: 0.6286 - val_precision_m: 0.6270 - val_recall_m: 0.6325\n",
            "Epoch 385/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6019 - f1_m: 0.6068 - precision_m: 0.5990 - recall_m: 0.6159 - val_loss: 0.6614 - val_acc: 0.6079 - val_f1_m: 0.6253 - val_precision_m: 0.6303 - val_recall_m: 0.6223\n",
            "Epoch 386/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6023 - f1_m: 0.6049 - precision_m: 0.6008 - recall_m: 0.6102 - val_loss: 0.6613 - val_acc: 0.6080 - val_f1_m: 0.6267 - val_precision_m: 0.6295 - val_recall_m: 0.6258\n",
            "Epoch 387/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6018 - f1_m: 0.6052 - precision_m: 0.5998 - recall_m: 0.6121 - val_loss: 0.6613 - val_acc: 0.6080 - val_f1_m: 0.6257 - val_precision_m: 0.6302 - val_recall_m: 0.6231\n",
            "Epoch 388/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6027 - f1_m: 0.6045 - precision_m: 0.6019 - recall_m: 0.6084 - val_loss: 0.6610 - val_acc: 0.6082 - val_f1_m: 0.6284 - val_precision_m: 0.6265 - val_recall_m: 0.6327\n",
            "Epoch 389/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6615 - acc: 0.6021 - f1_m: 0.6059 - precision_m: 0.5997 - recall_m: 0.6135 - val_loss: 0.6611 - val_acc: 0.6086 - val_f1_m: 0.6287 - val_precision_m: 0.6292 - val_recall_m: 0.6299\n",
            "Epoch 390/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6021 - f1_m: 0.6055 - precision_m: 0.6000 - recall_m: 0.6123 - val_loss: 0.6612 - val_acc: 0.6085 - val_f1_m: 0.6278 - val_precision_m: 0.6296 - val_recall_m: 0.6277\n",
            "Epoch 391/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6018 - f1_m: 0.6049 - precision_m: 0.6003 - recall_m: 0.6110 - val_loss: 0.6611 - val_acc: 0.6087 - val_f1_m: 0.6285 - val_precision_m: 0.6295 - val_recall_m: 0.6293\n",
            "Epoch 392/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6020 - f1_m: 0.6055 - precision_m: 0.5999 - recall_m: 0.6125 - val_loss: 0.6612 - val_acc: 0.6084 - val_f1_m: 0.6274 - val_precision_m: 0.6297 - val_recall_m: 0.6268\n",
            "Epoch 393/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6023 - f1_m: 0.6063 - precision_m: 0.6000 - recall_m: 0.6139 - val_loss: 0.6613 - val_acc: 0.6082 - val_f1_m: 0.6256 - val_precision_m: 0.6307 - val_recall_m: 0.6224\n",
            "Epoch 394/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6614 - acc: 0.6023 - f1_m: 0.6055 - precision_m: 0.6003 - recall_m: 0.6121 - val_loss: 0.6613 - val_acc: 0.6080 - val_f1_m: 0.6254 - val_precision_m: 0.6306 - val_recall_m: 0.6221\n",
            "Epoch 395/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6025 - f1_m: 0.6060 - precision_m: 0.6005 - recall_m: 0.6131 - val_loss: 0.6614 - val_acc: 0.6076 - val_f1_m: 0.6239 - val_precision_m: 0.6308 - val_recall_m: 0.6191\n",
            "Epoch 396/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6614 - acc: 0.6026 - f1_m: 0.6054 - precision_m: 0.6010 - recall_m: 0.6111 - val_loss: 0.6613 - val_acc: 0.6082 - val_f1_m: 0.6256 - val_precision_m: 0.6307 - val_recall_m: 0.6224\n",
            "Epoch 397/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6613 - acc: 0.6030 - f1_m: 0.6044 - precision_m: 0.6019 - recall_m: 0.6082 - val_loss: 0.6609 - val_acc: 0.6084 - val_f1_m: 0.6284 - val_precision_m: 0.6267 - val_recall_m: 0.6323\n",
            "Epoch 398/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6613 - acc: 0.6023 - f1_m: 0.6065 - precision_m: 0.5996 - recall_m: 0.6146 - val_loss: 0.6612 - val_acc: 0.6090 - val_f1_m: 0.6275 - val_precision_m: 0.6306 - val_recall_m: 0.6263\n",
            "Epoch 399/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6613 - acc: 0.6024 - f1_m: 0.6056 - precision_m: 0.6005 - recall_m: 0.6122 - val_loss: 0.6612 - val_acc: 0.6087 - val_f1_m: 0.6270 - val_precision_m: 0.6304 - val_recall_m: 0.6254\n",
            "Epoch 400/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6613 - acc: 0.6024 - f1_m: 0.6064 - precision_m: 0.6002 - recall_m: 0.6142 - val_loss: 0.6613 - val_acc: 0.6080 - val_f1_m: 0.6245 - val_precision_m: 0.6310 - val_recall_m: 0.6199\n",
            "Epoch 401/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6613 - acc: 0.6027 - f1_m: 0.6046 - precision_m: 0.6015 - recall_m: 0.6089 - val_loss: 0.6611 - val_acc: 0.6090 - val_f1_m: 0.6283 - val_precision_m: 0.6301 - val_recall_m: 0.6283\n",
            "Epoch 402/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6613 - acc: 0.6025 - f1_m: 0.6070 - precision_m: 0.5997 - recall_m: 0.6155 - val_loss: 0.6613 - val_acc: 0.6080 - val_f1_m: 0.6245 - val_precision_m: 0.6309 - val_recall_m: 0.6201\n",
            "Epoch 403/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6612 - acc: 0.6031 - f1_m: 0.6033 - precision_m: 0.6027 - recall_m: 0.6051 - val_loss: 0.6608 - val_acc: 0.6079 - val_f1_m: 0.6289 - val_precision_m: 0.6256 - val_recall_m: 0.6344\n",
            "Epoch 404/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6613 - acc: 0.6019 - f1_m: 0.6057 - precision_m: 0.5997 - recall_m: 0.6131 - val_loss: 0.6609 - val_acc: 0.6089 - val_f1_m: 0.6295 - val_precision_m: 0.6291 - val_recall_m: 0.6316\n",
            "Epoch 405/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6025 - f1_m: 0.6066 - precision_m: 0.6002 - recall_m: 0.6146 - val_loss: 0.6610 - val_acc: 0.6091 - val_f1_m: 0.6284 - val_precision_m: 0.6301 - val_recall_m: 0.6286\n",
            "Epoch 406/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6027 - f1_m: 0.6062 - precision_m: 0.6010 - recall_m: 0.6131 - val_loss: 0.6611 - val_acc: 0.6091 - val_f1_m: 0.6278 - val_precision_m: 0.6305 - val_recall_m: 0.6269\n",
            "Epoch 407/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6030 - f1_m: 0.6065 - precision_m: 0.6010 - recall_m: 0.6133 - val_loss: 0.6611 - val_acc: 0.6081 - val_f1_m: 0.6259 - val_precision_m: 0.6303 - val_recall_m: 0.6233\n",
            "Epoch 408/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6033 - f1_m: 0.6055 - precision_m: 0.6019 - recall_m: 0.6103 - val_loss: 0.6609 - val_acc: 0.6091 - val_f1_m: 0.6291 - val_precision_m: 0.6296 - val_recall_m: 0.6304\n",
            "Epoch 409/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6025 - f1_m: 0.6069 - precision_m: 0.6000 - recall_m: 0.6150 - val_loss: 0.6611 - val_acc: 0.6081 - val_f1_m: 0.6257 - val_precision_m: 0.6305 - val_recall_m: 0.6228\n",
            "Epoch 410/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6032 - f1_m: 0.6058 - precision_m: 0.6011 - recall_m: 0.6117 - val_loss: 0.6611 - val_acc: 0.6080 - val_f1_m: 0.6261 - val_precision_m: 0.6300 - val_recall_m: 0.6240\n",
            "Epoch 411/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6612 - acc: 0.6035 - f1_m: 0.6058 - precision_m: 0.6020 - recall_m: 0.6110 - val_loss: 0.6609 - val_acc: 0.6091 - val_f1_m: 0.6294 - val_precision_m: 0.6294 - val_recall_m: 0.6311\n",
            "Epoch 412/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6612 - acc: 0.6031 - f1_m: 0.6065 - precision_m: 0.6009 - recall_m: 0.6135 - val_loss: 0.6609 - val_acc: 0.6094 - val_f1_m: 0.6290 - val_precision_m: 0.6302 - val_recall_m: 0.6295\n",
            "Epoch 413/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6611 - acc: 0.6029 - f1_m: 0.6061 - precision_m: 0.6010 - recall_m: 0.6125 - val_loss: 0.6609 - val_acc: 0.6090 - val_f1_m: 0.6289 - val_precision_m: 0.6296 - val_recall_m: 0.6300\n",
            "Epoch 414/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6611 - acc: 0.6034 - f1_m: 0.6067 - precision_m: 0.6018 - recall_m: 0.6132 - val_loss: 0.6609 - val_acc: 0.6090 - val_f1_m: 0.6289 - val_precision_m: 0.6296 - val_recall_m: 0.6300\n",
            "Epoch 415/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6611 - acc: 0.6027 - f1_m: 0.6072 - precision_m: 0.6003 - recall_m: 0.6152 - val_loss: 0.6611 - val_acc: 0.6080 - val_f1_m: 0.6255 - val_precision_m: 0.6302 - val_recall_m: 0.6226\n",
            "Epoch 416/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6611 - acc: 0.6031 - f1_m: 0.6055 - precision_m: 0.6014 - recall_m: 0.6108 - val_loss: 0.6609 - val_acc: 0.6093 - val_f1_m: 0.6281 - val_precision_m: 0.6305 - val_recall_m: 0.6275\n",
            "Epoch 417/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6611 - acc: 0.6032 - f1_m: 0.6057 - precision_m: 0.6015 - recall_m: 0.6111 - val_loss: 0.6609 - val_acc: 0.6091 - val_f1_m: 0.6290 - val_precision_m: 0.6297 - val_recall_m: 0.6300\n",
            "Epoch 418/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6611 - acc: 0.6032 - f1_m: 0.6069 - precision_m: 0.6010 - recall_m: 0.6143 - val_loss: 0.6610 - val_acc: 0.6086 - val_f1_m: 0.6270 - val_precision_m: 0.6303 - val_recall_m: 0.6256\n",
            "Epoch 419/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6611 - acc: 0.6037 - f1_m: 0.6072 - precision_m: 0.6017 - recall_m: 0.6140 - val_loss: 0.6610 - val_acc: 0.6081 - val_f1_m: 0.6261 - val_precision_m: 0.6302 - val_recall_m: 0.6239\n",
            "Epoch 420/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6611 - acc: 0.6033 - f1_m: 0.6056 - precision_m: 0.6016 - recall_m: 0.6112 - val_loss: 0.6609 - val_acc: 0.6093 - val_f1_m: 0.6282 - val_precision_m: 0.6305 - val_recall_m: 0.6277\n",
            "Epoch 421/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6610 - acc: 0.6031 - f1_m: 0.6067 - precision_m: 0.6009 - recall_m: 0.6138 - val_loss: 0.6610 - val_acc: 0.6084 - val_f1_m: 0.6268 - val_precision_m: 0.6302 - val_recall_m: 0.6253\n",
            "Epoch 422/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6610 - acc: 0.6034 - f1_m: 0.6069 - precision_m: 0.6014 - recall_m: 0.6136 - val_loss: 0.6610 - val_acc: 0.6082 - val_f1_m: 0.6263 - val_precision_m: 0.6302 - val_recall_m: 0.6242\n",
            "Epoch 423/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6610 - acc: 0.6039 - f1_m: 0.6065 - precision_m: 0.6021 - recall_m: 0.6122 - val_loss: 0.6609 - val_acc: 0.6092 - val_f1_m: 0.6281 - val_precision_m: 0.6305 - val_recall_m: 0.6275\n",
            "Epoch 424/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6610 - acc: 0.6035 - f1_m: 0.6069 - precision_m: 0.6016 - recall_m: 0.6136 - val_loss: 0.6609 - val_acc: 0.6088 - val_f1_m: 0.6275 - val_precision_m: 0.6303 - val_recall_m: 0.6265\n",
            "Epoch 425/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6610 - acc: 0.6036 - f1_m: 0.6070 - precision_m: 0.6013 - recall_m: 0.6140 - val_loss: 0.6610 - val_acc: 0.6080 - val_f1_m: 0.6259 - val_precision_m: 0.6301 - val_recall_m: 0.6236\n",
            "Epoch 426/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6610 - acc: 0.6032 - f1_m: 0.6070 - precision_m: 0.6007 - recall_m: 0.6145 - val_loss: 0.6611 - val_acc: 0.6067 - val_f1_m: 0.6228 - val_precision_m: 0.6301 - val_recall_m: 0.6175\n",
            "Epoch 427/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6610 - acc: 0.6034 - f1_m: 0.6062 - precision_m: 0.6018 - recall_m: 0.6120 - val_loss: 0.6610 - val_acc: 0.6075 - val_f1_m: 0.6244 - val_precision_m: 0.6302 - val_recall_m: 0.6206\n",
            "Epoch 428/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6610 - acc: 0.6036 - f1_m: 0.6067 - precision_m: 0.6016 - recall_m: 0.6130 - val_loss: 0.6610 - val_acc: 0.6075 - val_f1_m: 0.6247 - val_precision_m: 0.6301 - val_recall_m: 0.6211\n",
            "Epoch 429/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6609 - acc: 0.6035 - f1_m: 0.6056 - precision_m: 0.6021 - recall_m: 0.6102 - val_loss: 0.6608 - val_acc: 0.6096 - val_f1_m: 0.6289 - val_precision_m: 0.6305 - val_recall_m: 0.6291\n",
            "Epoch 430/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6609 - acc: 0.6039 - f1_m: 0.6072 - precision_m: 0.6021 - recall_m: 0.6135 - val_loss: 0.6608 - val_acc: 0.6087 - val_f1_m: 0.6278 - val_precision_m: 0.6299 - val_recall_m: 0.6274\n",
            "Epoch 431/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6609 - acc: 0.6038 - f1_m: 0.6071 - precision_m: 0.6016 - recall_m: 0.6139 - val_loss: 0.6608 - val_acc: 0.6087 - val_f1_m: 0.6275 - val_precision_m: 0.6301 - val_recall_m: 0.6266\n",
            "Epoch 432/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6609 - acc: 0.6037 - f1_m: 0.6081 - precision_m: 0.6015 - recall_m: 0.6162 - val_loss: 0.6611 - val_acc: 0.6070 - val_f1_m: 0.6230 - val_precision_m: 0.6304 - val_recall_m: 0.6177\n",
            "Epoch 433/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6609 - acc: 0.6040 - f1_m: 0.6056 - precision_m: 0.6028 - recall_m: 0.6099 - val_loss: 0.6608 - val_acc: 0.6090 - val_f1_m: 0.6283 - val_precision_m: 0.6300 - val_recall_m: 0.6282\n",
            "Epoch 434/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6609 - acc: 0.6036 - f1_m: 0.6072 - precision_m: 0.6017 - recall_m: 0.6140 - val_loss: 0.6608 - val_acc: 0.6084 - val_f1_m: 0.6273 - val_precision_m: 0.6298 - val_recall_m: 0.6266\n",
            "Epoch 435/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6609 - acc: 0.6040 - f1_m: 0.6071 - precision_m: 0.6019 - recall_m: 0.6137 - val_loss: 0.6608 - val_acc: 0.6083 - val_f1_m: 0.6271 - val_precision_m: 0.6297 - val_recall_m: 0.6263\n",
            "Epoch 436/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6609 - acc: 0.6039 - f1_m: 0.6069 - precision_m: 0.6020 - recall_m: 0.6131 - val_loss: 0.6608 - val_acc: 0.6083 - val_f1_m: 0.6272 - val_precision_m: 0.6297 - val_recall_m: 0.6264\n",
            "Epoch 437/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6036 - f1_m: 0.6082 - precision_m: 0.6009 - recall_m: 0.6170 - val_loss: 0.6611 - val_acc: 0.6070 - val_f1_m: 0.6225 - val_precision_m: 0.6308 - val_recall_m: 0.6163\n",
            "Epoch 438/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6608 - acc: 0.6045 - f1_m: 0.6071 - precision_m: 0.6026 - recall_m: 0.6131 - val_loss: 0.6610 - val_acc: 0.6071 - val_f1_m: 0.6231 - val_precision_m: 0.6305 - val_recall_m: 0.6177\n",
            "Epoch 439/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6041 - f1_m: 0.6050 - precision_m: 0.6033 - recall_m: 0.6078 - val_loss: 0.6606 - val_acc: 0.6092 - val_f1_m: 0.6301 - val_precision_m: 0.6292 - val_recall_m: 0.6328\n",
            "Epoch 440/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6038 - f1_m: 0.6077 - precision_m: 0.6015 - recall_m: 0.6153 - val_loss: 0.6607 - val_acc: 0.6080 - val_f1_m: 0.6271 - val_precision_m: 0.6293 - val_recall_m: 0.6266\n",
            "Epoch 441/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6042 - f1_m: 0.6071 - precision_m: 0.6024 - recall_m: 0.6130 - val_loss: 0.6606 - val_acc: 0.6092 - val_f1_m: 0.6289 - val_precision_m: 0.6299 - val_recall_m: 0.6296\n",
            "Epoch 442/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6608 - acc: 0.6042 - f1_m: 0.6072 - precision_m: 0.6024 - recall_m: 0.6131 - val_loss: 0.6606 - val_acc: 0.6093 - val_f1_m: 0.6292 - val_precision_m: 0.6299 - val_recall_m: 0.6301\n",
            "Epoch 443/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6608 - acc: 0.6039 - f1_m: 0.6060 - precision_m: 0.6022 - recall_m: 0.6111 - val_loss: 0.6605 - val_acc: 0.6096 - val_f1_m: 0.6309 - val_precision_m: 0.6292 - val_recall_m: 0.6342\n",
            "Epoch 444/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6608 - acc: 0.6039 - f1_m: 0.6071 - precision_m: 0.6023 - recall_m: 0.6134 - val_loss: 0.6605 - val_acc: 0.6096 - val_f1_m: 0.6308 - val_precision_m: 0.6293 - val_recall_m: 0.6341\n",
            "Epoch 445/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6608 - acc: 0.6036 - f1_m: 0.6081 - precision_m: 0.6012 - recall_m: 0.6166 - val_loss: 0.6608 - val_acc: 0.6077 - val_f1_m: 0.6258 - val_precision_m: 0.6296 - val_recall_m: 0.6238\n",
            "Epoch 446/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6607 - acc: 0.6043 - f1_m: 0.6069 - precision_m: 0.6027 - recall_m: 0.6123 - val_loss: 0.6606 - val_acc: 0.6083 - val_f1_m: 0.6277 - val_precision_m: 0.6293 - val_recall_m: 0.6278\n",
            "Epoch 447/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6607 - acc: 0.6038 - f1_m: 0.6078 - precision_m: 0.6014 - recall_m: 0.6156 - val_loss: 0.6608 - val_acc: 0.6077 - val_f1_m: 0.6251 - val_precision_m: 0.6302 - val_recall_m: 0.6219\n",
            "Epoch 448/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6607 - acc: 0.6043 - f1_m: 0.6064 - precision_m: 0.6028 - recall_m: 0.6115 - val_loss: 0.6606 - val_acc: 0.6083 - val_f1_m: 0.6278 - val_precision_m: 0.6293 - val_recall_m: 0.6280\n",
            "Epoch 449/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6607 - acc: 0.6045 - f1_m: 0.6067 - precision_m: 0.6030 - recall_m: 0.6116 - val_loss: 0.6605 - val_acc: 0.6096 - val_f1_m: 0.6302 - val_precision_m: 0.6298 - val_recall_m: 0.6323\n",
            "Epoch 450/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6607 - acc: 0.6042 - f1_m: 0.6085 - precision_m: 0.6019 - recall_m: 0.6165 - val_loss: 0.6607 - val_acc: 0.6078 - val_f1_m: 0.6263 - val_precision_m: 0.6294 - val_recall_m: 0.6250\n",
            "Epoch 451/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6607 - acc: 0.6044 - f1_m: 0.6075 - precision_m: 0.6026 - recall_m: 0.6138 - val_loss: 0.6607 - val_acc: 0.6075 - val_f1_m: 0.6259 - val_precision_m: 0.6292 - val_recall_m: 0.6245\n",
            "Epoch 452/500\n",
            "139/139 [==============================] - 2s 11ms/step - loss: 0.6607 - acc: 0.6040 - f1_m: 0.6082 - precision_m: 0.6014 - recall_m: 0.6165 - val_loss: 0.6610 - val_acc: 0.6073 - val_f1_m: 0.6226 - val_precision_m: 0.6312 - val_recall_m: 0.6161\n",
            "Epoch 453/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6607 - acc: 0.6043 - f1_m: 0.6061 - precision_m: 0.6028 - recall_m: 0.6105 - val_loss: 0.6607 - val_acc: 0.6080 - val_f1_m: 0.6258 - val_precision_m: 0.6302 - val_recall_m: 0.6233\n",
            "Epoch 454/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6043 - f1_m: 0.6086 - precision_m: 0.6018 - recall_m: 0.6169 - val_loss: 0.6610 - val_acc: 0.6072 - val_f1_m: 0.6215 - val_precision_m: 0.6319 - val_recall_m: 0.6133\n",
            "Epoch 455/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6606 - acc: 0.6045 - f1_m: 0.6051 - precision_m: 0.6039 - recall_m: 0.6075 - val_loss: 0.6605 - val_acc: 0.6090 - val_f1_m: 0.6287 - val_precision_m: 0.6297 - val_recall_m: 0.6294\n",
            "Epoch 456/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6606 - acc: 0.6046 - f1_m: 0.6072 - precision_m: 0.6029 - recall_m: 0.6128 - val_loss: 0.6604 - val_acc: 0.6095 - val_f1_m: 0.6303 - val_precision_m: 0.6294 - val_recall_m: 0.6329\n",
            "Epoch 457/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6046 - f1_m: 0.6085 - precision_m: 0.6024 - recall_m: 0.6160 - val_loss: 0.6605 - val_acc: 0.6081 - val_f1_m: 0.6274 - val_precision_m: 0.6293 - val_recall_m: 0.6273\n",
            "Epoch 458/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6045 - f1_m: 0.6067 - precision_m: 0.6030 - recall_m: 0.6116 - val_loss: 0.6604 - val_acc: 0.6092 - val_f1_m: 0.6299 - val_precision_m: 0.6292 - val_recall_m: 0.6322\n",
            "Epoch 459/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6047 - f1_m: 0.6089 - precision_m: 0.6023 - recall_m: 0.6169 - val_loss: 0.6606 - val_acc: 0.6080 - val_f1_m: 0.6267 - val_precision_m: 0.6296 - val_recall_m: 0.6255\n",
            "Epoch 460/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6042 - f1_m: 0.6068 - precision_m: 0.6026 - recall_m: 0.6123 - val_loss: 0.6605 - val_acc: 0.6083 - val_f1_m: 0.6276 - val_precision_m: 0.6295 - val_recall_m: 0.6275\n",
            "Epoch 461/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6041 - f1_m: 0.6075 - precision_m: 0.6018 - recall_m: 0.6147 - val_loss: 0.6606 - val_acc: 0.6079 - val_f1_m: 0.6258 - val_precision_m: 0.6299 - val_recall_m: 0.6236\n",
            "Epoch 462/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6606 - acc: 0.6046 - f1_m: 0.6070 - precision_m: 0.6032 - recall_m: 0.6125 - val_loss: 0.6605 - val_acc: 0.6078 - val_f1_m: 0.6267 - val_precision_m: 0.6292 - val_recall_m: 0.6260\n",
            "Epoch 463/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6048 - f1_m: 0.6066 - precision_m: 0.6035 - recall_m: 0.6112 - val_loss: 0.6603 - val_acc: 0.6100 - val_f1_m: 0.6313 - val_precision_m: 0.6294 - val_recall_m: 0.6349\n",
            "Epoch 464/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6045 - f1_m: 0.6077 - precision_m: 0.6023 - recall_m: 0.6144 - val_loss: 0.6603 - val_acc: 0.6100 - val_f1_m: 0.6308 - val_precision_m: 0.6297 - val_recall_m: 0.6337\n",
            "Epoch 465/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6044 - f1_m: 0.6085 - precision_m: 0.6019 - recall_m: 0.6163 - val_loss: 0.6605 - val_acc: 0.6078 - val_f1_m: 0.6264 - val_precision_m: 0.6294 - val_recall_m: 0.6252\n",
            "Epoch 466/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6044 - f1_m: 0.6076 - precision_m: 0.6023 - recall_m: 0.6140 - val_loss: 0.6605 - val_acc: 0.6079 - val_f1_m: 0.6267 - val_precision_m: 0.6294 - val_recall_m: 0.6257\n",
            "Epoch 467/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6046 - f1_m: 0.6075 - precision_m: 0.6029 - recall_m: 0.6135 - val_loss: 0.6605 - val_acc: 0.6083 - val_f1_m: 0.6275 - val_precision_m: 0.6295 - val_recall_m: 0.6271\n",
            "Epoch 468/500\n",
            "139/139 [==============================] - 1s 11ms/step - loss: 0.6605 - acc: 0.6049 - f1_m: 0.6080 - precision_m: 0.6028 - recall_m: 0.6143 - val_loss: 0.6605 - val_acc: 0.6084 - val_f1_m: 0.6276 - val_precision_m: 0.6296 - val_recall_m: 0.6273\n",
            "Epoch 469/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6043 - f1_m: 0.6087 - precision_m: 0.6017 - recall_m: 0.6170 - val_loss: 0.6607 - val_acc: 0.6083 - val_f1_m: 0.6247 - val_precision_m: 0.6315 - val_recall_m: 0.6200\n",
            "Epoch 470/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6047 - f1_m: 0.6060 - precision_m: 0.6037 - recall_m: 0.6093 - val_loss: 0.6603 - val_acc: 0.6095 - val_f1_m: 0.6302 - val_precision_m: 0.6294 - val_recall_m: 0.6326\n",
            "Epoch 471/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6605 - acc: 0.6047 - f1_m: 0.6088 - precision_m: 0.6021 - recall_m: 0.6168 - val_loss: 0.6605 - val_acc: 0.6083 - val_f1_m: 0.6268 - val_precision_m: 0.6300 - val_recall_m: 0.6254\n",
            "Epoch 472/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6047 - f1_m: 0.6072 - precision_m: 0.6032 - recall_m: 0.6127 - val_loss: 0.6604 - val_acc: 0.6084 - val_f1_m: 0.6278 - val_precision_m: 0.6294 - val_recall_m: 0.6280\n",
            "Epoch 473/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6049 - f1_m: 0.6083 - precision_m: 0.6029 - recall_m: 0.6147 - val_loss: 0.6604 - val_acc: 0.6082 - val_f1_m: 0.6274 - val_precision_m: 0.6295 - val_recall_m: 0.6271\n",
            "Epoch 474/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6050 - f1_m: 0.6089 - precision_m: 0.6028 - recall_m: 0.6164 - val_loss: 0.6606 - val_acc: 0.6082 - val_f1_m: 0.6259 - val_precision_m: 0.6305 - val_recall_m: 0.6231\n",
            "Epoch 475/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6048 - f1_m: 0.6075 - precision_m: 0.6030 - recall_m: 0.6135 - val_loss: 0.6605 - val_acc: 0.6084 - val_f1_m: 0.6267 - val_precision_m: 0.6303 - val_recall_m: 0.6249\n",
            "Epoch 476/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6047 - f1_m: 0.6083 - precision_m: 0.6030 - recall_m: 0.6149 - val_loss: 0.6606 - val_acc: 0.6084 - val_f1_m: 0.6259 - val_precision_m: 0.6308 - val_recall_m: 0.6229\n",
            "Epoch 477/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6050 - f1_m: 0.6069 - precision_m: 0.6039 - recall_m: 0.6112 - val_loss: 0.6603 - val_acc: 0.6086 - val_f1_m: 0.6281 - val_precision_m: 0.6295 - val_recall_m: 0.6285\n",
            "Epoch 478/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6048 - f1_m: 0.6090 - precision_m: 0.6024 - recall_m: 0.6169 - val_loss: 0.6605 - val_acc: 0.6087 - val_f1_m: 0.6264 - val_precision_m: 0.6309 - val_recall_m: 0.6238\n",
            "Epoch 479/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6049 - f1_m: 0.6082 - precision_m: 0.6028 - recall_m: 0.6149 - val_loss: 0.6605 - val_acc: 0.6083 - val_f1_m: 0.6256 - val_precision_m: 0.6309 - val_recall_m: 0.6222\n",
            "Epoch 480/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6604 - acc: 0.6046 - f1_m: 0.6068 - precision_m: 0.6031 - recall_m: 0.6116 - val_loss: 0.6603 - val_acc: 0.6083 - val_f1_m: 0.6275 - val_precision_m: 0.6296 - val_recall_m: 0.6271\n",
            "Epoch 481/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6048 - f1_m: 0.6066 - precision_m: 0.6037 - recall_m: 0.6108 - val_loss: 0.6601 - val_acc: 0.6090 - val_f1_m: 0.6313 - val_precision_m: 0.6280 - val_recall_m: 0.6363\n",
            "Epoch 482/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6050 - f1_m: 0.6088 - precision_m: 0.6027 - recall_m: 0.6162 - val_loss: 0.6603 - val_acc: 0.6092 - val_f1_m: 0.6291 - val_precision_m: 0.6297 - val_recall_m: 0.6303\n",
            "Epoch 483/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6048 - f1_m: 0.6079 - precision_m: 0.6029 - recall_m: 0.6145 - val_loss: 0.6602 - val_acc: 0.6089 - val_f1_m: 0.6291 - val_precision_m: 0.6294 - val_recall_m: 0.6305\n",
            "Epoch 484/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6047 - f1_m: 0.6079 - precision_m: 0.6029 - recall_m: 0.6140 - val_loss: 0.6602 - val_acc: 0.6095 - val_f1_m: 0.6299 - val_precision_m: 0.6296 - val_recall_m: 0.6321\n",
            "Epoch 485/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6052 - f1_m: 0.6091 - precision_m: 0.6032 - recall_m: 0.6164 - val_loss: 0.6603 - val_acc: 0.6084 - val_f1_m: 0.6273 - val_precision_m: 0.6299 - val_recall_m: 0.6264\n",
            "Epoch 486/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6053 - f1_m: 0.6086 - precision_m: 0.6036 - recall_m: 0.6149 - val_loss: 0.6604 - val_acc: 0.6085 - val_f1_m: 0.6270 - val_precision_m: 0.6302 - val_recall_m: 0.6255\n",
            "Epoch 487/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6051 - f1_m: 0.6064 - precision_m: 0.6040 - recall_m: 0.6102 - val_loss: 0.6601 - val_acc: 0.6091 - val_f1_m: 0.6309 - val_precision_m: 0.6284 - val_recall_m: 0.6351\n",
            "Epoch 488/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6049 - f1_m: 0.6099 - precision_m: 0.6019 - recall_m: 0.6193 - val_loss: 0.6605 - val_acc: 0.6092 - val_f1_m: 0.6266 - val_precision_m: 0.6316 - val_recall_m: 0.6235\n",
            "Epoch 489/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6603 - acc: 0.6049 - f1_m: 0.6074 - precision_m: 0.6036 - recall_m: 0.6126 - val_loss: 0.6603 - val_acc: 0.6089 - val_f1_m: 0.6279 - val_precision_m: 0.6302 - val_recall_m: 0.6273\n",
            "Epoch 490/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6051 - f1_m: 0.6082 - precision_m: 0.6031 - recall_m: 0.6147 - val_loss: 0.6603 - val_acc: 0.6092 - val_f1_m: 0.6276 - val_precision_m: 0.6308 - val_recall_m: 0.6261\n",
            "Epoch 491/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6055 - f1_m: 0.6071 - precision_m: 0.6043 - recall_m: 0.6110 - val_loss: 0.6601 - val_acc: 0.6096 - val_f1_m: 0.6307 - val_precision_m: 0.6293 - val_recall_m: 0.6338\n",
            "Epoch 492/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6048 - f1_m: 0.6099 - precision_m: 0.6020 - recall_m: 0.6193 - val_loss: 0.6605 - val_acc: 0.6091 - val_f1_m: 0.6261 - val_precision_m: 0.6318 - val_recall_m: 0.6224\n",
            "Epoch 493/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6051 - f1_m: 0.6077 - precision_m: 0.6033 - recall_m: 0.6134 - val_loss: 0.6604 - val_acc: 0.6092 - val_f1_m: 0.6267 - val_precision_m: 0.6315 - val_recall_m: 0.6238\n",
            "Epoch 494/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6055 - f1_m: 0.6087 - precision_m: 0.6036 - recall_m: 0.6151 - val_loss: 0.6604 - val_acc: 0.6091 - val_f1_m: 0.6267 - val_precision_m: 0.6314 - val_recall_m: 0.6238\n",
            "Epoch 495/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6051 - f1_m: 0.6078 - precision_m: 0.6033 - recall_m: 0.6136 - val_loss: 0.6603 - val_acc: 0.6092 - val_f1_m: 0.6275 - val_precision_m: 0.6309 - val_recall_m: 0.6259\n",
            "Epoch 496/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6053 - f1_m: 0.6087 - precision_m: 0.6031 - recall_m: 0.6156 - val_loss: 0.6604 - val_acc: 0.6092 - val_f1_m: 0.6263 - val_precision_m: 0.6318 - val_recall_m: 0.6228\n",
            "Epoch 497/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6602 - acc: 0.6048 - f1_m: 0.6068 - precision_m: 0.6037 - recall_m: 0.6112 - val_loss: 0.6602 - val_acc: 0.6088 - val_f1_m: 0.6281 - val_precision_m: 0.6298 - val_recall_m: 0.6282\n",
            "Epoch 498/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6601 - acc: 0.6048 - f1_m: 0.6097 - precision_m: 0.6022 - recall_m: 0.6185 - val_loss: 0.6605 - val_acc: 0.6080 - val_f1_m: 0.6240 - val_precision_m: 0.6314 - val_recall_m: 0.6187\n",
            "Epoch 499/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6601 - acc: 0.6058 - f1_m: 0.6069 - precision_m: 0.6054 - recall_m: 0.6098 - val_loss: 0.6600 - val_acc: 0.6093 - val_f1_m: 0.6301 - val_precision_m: 0.6293 - val_recall_m: 0.6326\n",
            "Epoch 500/500\n",
            "139/139 [==============================] - 1s 10ms/step - loss: 0.6601 - acc: 0.6054 - f1_m: 0.6090 - precision_m: 0.6033 - recall_m: 0.6160 - val_loss: 0.6601 - val_acc: 0.6088 - val_f1_m: 0.6284 - val_precision_m: 0.6296 - val_recall_m: 0.6289\n",
            "nodes=10  loss=0.661  accuracy=0.606  F1-score=0.621  Precision=0.634  Recall=0.622\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Test activations\n",
        "activations = ['tanh']\n",
        "save_history = pd.DataFrame()\n",
        "for param in activations:\n",
        "    \n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', StandardScaler(), one_hot=True)\n",
        "    X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "    \n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = basic_model(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation=param,opt=SGD, lr=0.0002) #lr = 0.0002\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle(str(param))\n",
        "\n",
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580a1880",
      "metadata": {
        "id": "580a1880"
      },
      "outputs": [],
      "source": [
        "# Test optimizers\n",
        "save_history = pd.DataFrame()\n",
        "for param in optimizers:\n",
        "    \n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', StandardScaler(), one_hot=True)\n",
        "    X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "    \n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = basic_model(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='tanh',opt=param, lr=0.000001)\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle(str(param))\n",
        "\n",
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learning Rate Schedulers"
      ],
      "metadata": {
        "id": "m-XAdsqrw8HZ"
      },
      "id": "m-XAdsqrw8HZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate schedule\n",
        "def step_decay(epoch):\n",
        "\tinitial_lrate = 0.1\n",
        "\tdrop = 0.5\n",
        "\tepochs_drop = 10.0\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate"
      ],
      "metadata": {
        "id": "zNwMETydw7Ay"
      },
      "id": "zNwMETydw7Ay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test opt opt params\n",
        "\n",
        "learning_rates = [1e-3, 1e-4, 1e-6, some_changing_thing()]\n",
        "beta1, beta2 = [1,2,3,4], [1,2,3,4]\n",
        "save_history = pd.DataFrame()\n",
        "for param in optimizers:\n",
        "    \n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=True)\n",
        "    X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "    \n",
        "    # evaluate model with a given number of nodes\n",
        "    history, result = basic_model(n_nodes, X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch, activation='relu',opt=param, lr=0.000001)\n",
        "    save_history['loss_'+str(n_nodes)]=history.history[\"loss\"]\n",
        "    save_history['acc_'+str(n_nodes)]=history.history[\"acc\"]\n",
        "    save_history['f1'+str(n_nodes)]=history.history[\"f1_m\"]\n",
        "    save_history['recall'+str(n_nodes)]=history.history[\"recall_m\"]\n",
        "\n",
        "\n",
        "    # summarize final test set accuracy\n",
        "    print('nodes=%d  loss=%.3f  accuracy=%.3f  F1-score=%.3f  Precision=%.3f  Recall=%.3f' % (n_nodes, result[0], result[1], result[2], result[3], result[4]))\n",
        "    \n",
        "    # plot learning curves\n",
        "    plot_history(history)\n",
        "    plt.suptitle(str(param))\n",
        "\n",
        "plot_compare(save_history.loc[:,save_history.columns.to_list()[3::4]], 'Recall')"
      ],
      "metadata": {
        "id": "dAaElnGDpvzg"
      },
      "id": "dAaElnGDpvzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7132839",
      "metadata": {
        "id": "b7132839"
      },
      "source": [
        "### Build MLP Model with Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f78ee8",
      "metadata": {
        "id": "32f78ee8"
      },
      "outputs": [],
      "source": [
        "def build_categorical_inputs(features):\n",
        "\n",
        "    initial_inputs = {}\n",
        "    cat_input_layers={}\n",
        "    \n",
        "    train_test_cat_features = pd.concat([X_train[categorical_cols], X_test[categorical_cols]])\n",
        "    \n",
        "    for feature in features:\n",
        "        no_of_unique_cats  = train_test_cat_features[feature].nunique()\n",
        "        embedding_size = int(min(np.ceil((no_of_unique_cats)/2), 50))\n",
        "        categories  = no_of_unique_cats + 1\n",
        "\n",
        "        initial_inputs[feature] = Input(shape=(1,))\n",
        "        embedding_layer = Embedding(categories, \n",
        "                                    embedding_size,\n",
        "                                    embeddings_regularizer=regularizers.l2(0.01),\n",
        "                                    input_length=1)(initial_inputs[feature])\n",
        "        cat_input_layers[feature] = Reshape(target_shape=(embedding_size,))(embedding_layer)\n",
        "\n",
        "    return initial_inputs, cat_input_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a231b6",
      "metadata": {
        "id": "58a231b6"
      },
      "outputs": [],
      "source": [
        "def build_model( X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch):\n",
        "    \n",
        "    models = []\n",
        "    for categorical_var in categorical_cols :\n",
        "        model = Sequential()\n",
        "        no_of_unique_cat  = X_train[categorical_var].nunique()\n",
        "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
        "        embedding_size = int(embedding_size)\n",
        "        vocab  = no_of_unique_cat+1\n",
        "        model.add( Embedding(vocab ,embedding_size, input_length = 1 ))\n",
        "        model.add(Reshape(target_shape=(embedding_size,)))\n",
        "        models.append( model )\n",
        "        \n",
        "    model_rest = Sequential()\n",
        "    model_rest.add(Dense(16, input_dim= X_train[numerical_cols+continuous_cols].shape[1]))\n",
        "    models.append(model_rest)\n",
        "\n",
        "    full_model = Sequential()\n",
        "    full_model.add(Concatenate(models))\n",
        "    full_model.add(Dense(1000))\n",
        "    full_model.add(Activation('relu'))\n",
        "    full_model.add(Dense(400))\n",
        "    full_model.add(Activation('relu'))\n",
        "    full_model.add(Dense(1))\n",
        "    full_model.add(Activation('sigmoid'))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=Adam(learning_rate=0.000001),\n",
        "        metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    # fit model on train set\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val, y_val),\n",
        "    )\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return history, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df20d86",
      "metadata": {
        "id": "7df20d86"
      },
      "outputs": [],
      "source": [
        "def build_model( X_train, X_val, X_test ,y_train, y_val, y_test, epochs, batch):\n",
        "    \n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    for col in categorical_cols:\n",
        "        no_of_unique_cat  = X_train[col].nunique()\n",
        "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50)\n",
        "        embedding_size = int(embedding_size)\n",
        "        vocab  = no_of_unique_cat+1\n",
        "        model.add(Embedding(input_dim=no_of_unique_cat, output_dim=embedding_size, input_shape=(X_train.shape[1],)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50, activation=tf.keras.activations.gelu ))\n",
        "    model.add(Dense(1, activation=tf.nn.sigmoid))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=Adam(learning_rate=0.000001),\n",
        "        metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    # fit model on train set\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch,\n",
        "        epochs=epochs,\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val, y_val),\n",
        "    )\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return history, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7504bd9e",
      "metadata": {
        "id": "7504bd9e",
        "outputId": "461b0328-9c50-49ca-c506-82ac05ebb67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_179_input'), name='embedding_179_input', description=\"created by layer 'embedding_179_input'\"), but it was called on an input with incompatible shape (None, 61).\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 111, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer \"reshape_149\" (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [61, 1], output_shape = [1]\n    \n    Call arguments received by layer \"reshape_149\" (type Reshape):\n       inputs=tf.Tensor(shape=(None, 61, 1), dtype=float32)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33300/3241727813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'outcome_arrhythmia'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuantileTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_distribution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mresample_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'under'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33300/2518285737.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(X_train, X_val, y_val, y_train, X_test, y_test, epochs, batch)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# fit model on train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 111, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer \"reshape_149\" (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [61, 1], output_shape = [1]\n    \n    Call arguments received by layer \"reshape_149\" (type Reshape):\n       inputs=tf.Tensor(shape=(None, 61, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = process_features(df, 'outcome_arrhythmia', QuantileTransformer(output_distribution='uniform'), one_hot=False)\n",
        "X_train, y_train= resample_data(X_train, y_train, 'under')\n",
        "model=build_model(X_train, X_val, X_test ,y_train, y_val, y_test, 400, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fac49e1",
      "metadata": {
        "id": "1fac49e1"
      },
      "outputs": [],
      "source": [
        "initial_inputs, input_layers = build_categorical_inputs(categorical_cols)\n",
        "\n",
        "no_of_num_features = len(X_train.columns) - len(categorical_cols)\n",
        "\n",
        "initial_inputs['numerical_features'] = Input(shape=(no_of_num_features,))\n",
        "input_layers['numerical_features'] = initial_inputs['numerical_features']\n",
        "\n",
        "inputs = Concatenate(axis=-1)([layer for layer in input_layers.values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ccdade",
      "metadata": {
        "id": "40ccdade"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "\n",
        "model_rest = Sequential()\n",
        "model_rest.add(Dense(100, input_dim= len(numerical_cols+continuous_cols) ))\n",
        "models.append(model_rest)\n",
        "\n",
        "for categorical_var in categorical_cols :\n",
        "     \n",
        "    model = Sequential()\n",
        "    no_of_unique_cat  = X_train[categorical_var].nunique()\n",
        "    \n",
        "    # jeremy howard rule\n",
        "    embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
        "    embedding_size = int(embedding_size)\n",
        "    vocab  = no_of_unique_cat+1\n",
        "    model.add( Embedding(vocab ,embedding_size, input_length = 1 ))\n",
        "    model.add(Reshape(target_shape=(embedding_size,)))\n",
        "    models.append( model )\n",
        "\n",
        "\n",
        "full_model = Sequential()\n",
        "full_model.add(Concatenate(models))\n",
        "full_model.add(Dense(1000))\n",
        "full_model.add(Activation('relu'))\n",
        "full_model.add(Dense(400))\n",
        "full_model.add(Activation('relu'))\n",
        "full_model.add(Dense(200))\n",
        "full_model.add(Activation('sigmoid'))\n",
        "full_model.add(Dense(1))\n",
        "full_model.add(Activation('sigmoid'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc',f1_m,precision_m, recall_m])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987f57a0",
      "metadata": {
        "id": "987f57a0",
        "outputId": "d565f829-b10a-4bf0-eae3-9a584de3dd05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_607_input'), name='embedding_607_input', description=\"created by layer 'embedding_607_input'\"), but it was called on an input with incompatible shape (None, 61).\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 111, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer \"reshape_603\" (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [61, 1], output_shape = [1]\n    \n    Call arguments received by layer \"reshape_603\" (type Reshape):\n       inputs=tf.Tensor(shape=(None, 61, 1), dtype=float32)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31388/13893326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# fit model on train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\anali\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 111, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer \"reshape_603\" (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [61, 1], output_shape = [1]\n    \n    Call arguments received by layer \"reshape_603\" (type Reshape):\n       inputs=tf.Tensor(shape=(None, 61, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "batch=1000\n",
        "epochs=100\n",
        "# fit model on train set\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch,\n",
        "    epochs=epochs,\n",
        "#     shuffle=True,\n",
        "    verbose=1,\n",
        "    validation_data=(X_val, y_val),\n",
        ")\n",
        "score = model.evaluate(X_test, y_test, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0276b8",
      "metadata": {
        "id": "6a0276b8"
      },
      "source": [
        "### K-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436b62e0",
      "metadata": {
        "id": "436b62e0"
      },
      "outputs": [],
      "source": [
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((input_train, input_test), axis=0)\n",
        "targets = np.concatenate((target_train, target_test), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=loss_function,\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity)\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779c9624",
      "metadata": {
        "id": "779c9624"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00dd4df5",
      "metadata": {
        "id": "00dd4df5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "02-basic_model.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}