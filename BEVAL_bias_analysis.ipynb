{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4e0ad4",
   "metadata": {},
   "source": [
    "## Bias Evaluation : AIF360\n",
    "\n",
    "Quantification of model bias in terms of fairness against protected groups before and after implementation of mitigation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8aefc",
   "metadata": {},
   "source": [
    "If the application follows the WAE worldview, then the demographic parity metrics should be used: disparate_impact and statistical_parity_difference.  If the application follows the WYSIWYG worldview, then the equality of odds metrics should be used: average_odds_difference and average_abs_odds_difference.  Other group fairness metrics (some are often labeled equality of opportunity) lie in-between the two worldviews and may be used appropriately: false_negative_rate_ratio, false_negative_rate_difference, false_positive_rate_ratio, false_positive_rate_difference, false_discovery_rate_ratio, false_discovery_rate_difference, false_omission_rate_ratio, false_omission_rate_difference, error_rate_ratio, and error_rate_difference.  To choose among these, the right side of the decision tree here may be consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='whitegrid')\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from aif360.sklearn.datasets import fetch_compas\n",
    "from aif360.sklearn.metrics import mdss_bias_scan, mdss_bias_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbce7a",
   "metadata": {},
   "source": [
    "### Preparation.\n",
    "\n",
    "Define privileged and unprivileged groups for each protected attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebb8a2",
   "metadata": {},
   "source": [
    "#### Sex. Privileged group: Males (1) Unprivileged group: Females (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe06926",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = original_output[original_output['Sex']==1]\n",
    "num_privileged = male_df.shape[0]\n",
    "female_df = original_output[original_output['Sex']==0]\n",
    "num_unprivileged = female_df.shape[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f6ca0",
   "metadata": {},
   "source": [
    "#### Gender. Privileged group: White (1) Unprivileged group: Non-white (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd031f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a8d94e",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16c676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce54f40a",
   "metadata": {},
   "source": [
    "### Method 1. Disparate Impact Ratio\n",
    "\n",
    "The ***disparate impact ratio*** is defined as the ratio of the proportion of positive predictions (y'=1) for facet d over the proportion of positive predicitons (y'=1) for facet a.\n",
    "\n",
    "Industry generally considers the four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privileged group it is considered a disparate impact violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_outcomes = female_df[female_df['prediction']==1].shape[0]\n",
    "unprivileged_ratio = unprivileged_outcomes/num_unprivileged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02958264",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_outcomes = male_df[male_df['prediction']==1].shape[0]\n",
    "privileged_ratio = privileged_outcomes/num_privileged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ccb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparate_impact = unprivileged_ratio / privileged_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daf8c2",
   "metadata": {},
   "source": [
    "### Method 2. Group Fairness: Ratios vs Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42c5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
