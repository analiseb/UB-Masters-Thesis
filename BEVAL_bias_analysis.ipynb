{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4e0ad4",
   "metadata": {},
   "source": [
    "# Bias Evaluation : AIF360\n",
    "***\n",
    "\n",
    "Quantification of model bias in terms of fairness against protected groups before and after implementation of mitigation methods\n",
    "\n",
    "## Terminology\n",
    "***\n",
    "\n",
    "***Favorable label:*** A label whose value corresponds to an outcome that provides an advantage to the recipient (such as receiving a loan, being hired for a job, not being arrested)\n",
    "\n",
    "***Protected attribute:*** An attribute that partitions a population into groups whose outcomes should have parity (such as race, gender, caste, and religion)\n",
    "\n",
    "***Privileged value (of a protected attribute):*** A protected attribute value indicating a group that has historically been at a systemic advantage\n",
    "\n",
    "***Fairness metric:*** A quantification of unwanted bias in training data or models\n",
    "\n",
    "***Discrimination/unwanted bias:*** Although bias can refer to any form of preference, fair or unfair, our focus is on undesirable bias or discrimination, which is when specific privileged groups are placed at a systematic advantage and specific unprivileged groups are placed at a systematic disadvantage. This relates to attributes such as race, gender, age, and sexual orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033fa56",
   "metadata": {},
   "source": [
    "\n",
    "## Structure of Evaluation & Intervention\n",
    "***\n",
    "<img src=\"images/aif360_pipeline.png\" width=\"700\" height=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819f924",
   "metadata": {},
   "source": [
    "### Three Perspectivs of Fairness in ML algorithms\n",
    "***\n",
    "\n",
    "[linkedin article](https://www.linkedin.com/pulse/whats-new-deep-learning-research-reducing-bias-models-jesus-rodriguez/)\n",
    "\n",
    "***Data vs Mode***\n",
    "\n",
    "Fairness may be quantified in the training dataset or in the learned model\n",
    "\n",
    "***Group vs Individual***\n",
    "\n",
    "Group fairness partitions a population into groups defined by protected attributes and seeks for some statistical measure to be equal across all groups. Individual fairness seeks for similar individuals to be treated similarly.\n",
    "\n",
    "\n",
    "***WAE vs WYSIWYG (We are all equal vs What you see is what you get)***\n",
    "\n",
    "WAE says that fairness is an equal distirbution of skills and opportunities among the participants in an ML task, attributing differences in outcome distributions to structural bias and not a difference in distribution to ability. WYSIWYG says that observations reflect ability with respect to a task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8aefc",
   "metadata": {},
   "source": [
    "If the application follows the WAE worldview, then the demographic parity metrics should be used: disparate_impact and statistical_parity_difference.  If the application follows the WYSIWYG worldview, then the equality of odds metrics should be used: average_odds_difference and average_abs_odds_difference.  Other group fairness metrics (some are often labeled equality of opportunity) lie in-between the two worldviews and may be used appropriately: false_negative_rate_ratio, false_negative_rate_difference, false_positive_rate_ratio, false_positive_rate_difference, false_discovery_rate_ratio, false_discovery_rate_difference, false_omission_rate_ratio, false_omission_rate_difference, error_rate_ratio, and error_rate_difference.  To choose among these, the right side of the decision tree here may be consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f5c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='whitegrid')\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from aif360.sklearn.metrics import mdss_bias_scan, mdss_bias_score\n",
    "import aif360\n",
    "import utilities\n",
    "import global_variables as gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73176f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(gv.data_link)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a82e39",
   "metadata": {},
   "source": [
    "### 1. Data Bias Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1be05",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4feb30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c5149",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774b603",
   "metadata": {},
   "source": [
    "### 2. Evaluation of Bias in Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbce7a",
   "metadata": {},
   "source": [
    "### Preparation.\n",
    "\n",
    "Define privileged and unprivileged groups for each protected attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebb8a2",
   "metadata": {},
   "source": [
    "#### Sex. Privileged group: Males (1) Unprivileged group: Females (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe06926",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = original_output[original_output['Sex']==1]\n",
    "num_privileged = male_df.shape[0]\n",
    "female_df = original_output[original_output['Sex']==0]\n",
    "num_unprivileged = female_df.shape[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f6ca0",
   "metadata": {},
   "source": [
    "#### Gender. Privileged group: White (1) Unprivileged group: Non-white (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd031f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a8d94e",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16c676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce54f40a",
   "metadata": {},
   "source": [
    "### Metric 1. Disparate Impact Ratio\n",
    "\n",
    "The ***disparate impact ratio*** is defined as the ratio of the proportion of positive predictions (y'=1) for facet d over the proportion of positive predicitons (y'=1) for facet a.\n",
    "\n",
    "Industry generally considers the four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privileged group it is considered a disparate impact violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_outcomes = female_df[female_df['prediction']==1].shape[0]\n",
    "unprivileged_ratio = unprivileged_outcomes/num_unprivileged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02958264",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_outcomes = male_df[male_df['prediction']==1].shape[0]\n",
    "privileged_ratio = privileged_outcomes/num_privileged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ccb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparate_impact = unprivileged_ratio / privileged_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3daf8c2",
   "metadata": {},
   "source": [
    "### Metric 2. Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42c5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8c1c509",
   "metadata": {},
   "source": [
    "### Metric 3. Equal Opportunity Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514aad29",
   "metadata": {},
   "source": [
    "### Metric 4. Average Odds Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ad60f",
   "metadata": {},
   "source": [
    "### Theil Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773ff20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
