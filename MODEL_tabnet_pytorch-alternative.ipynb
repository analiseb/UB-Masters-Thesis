{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385002dd",
   "metadata": {},
   "source": [
    "## TabNet Implentation for Tabular Data\n",
    "\n",
    "TabNet is proposed in [this article] (https://arxiv.org/abs/1908.07442) as a neural network architecture capable of learning a canonical representation of tabular data. This architecture has shown to perform well against the current gold-standard gradient boosting models for learning on tabular data.\n",
    "\n",
    "TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful\n",
    "features to process at each decision step. Instance-wise feature selection enables efficient learning as the model capacity is fully used for the most salient features, and also yields\n",
    "more interpretable decision making via visualization of selection masks. \n",
    "\n",
    "\n",
    "This implementation closely follows [the TabNet implementation in PyTorch linked here](https://github.com/dreamquark-ai/tabnet/tree/b6e1ebaf694f37ad40a6ba525aa016fd3cec15da). \n",
    "\n",
    "<img src=\"images/tabnet_schematic2.jpg\" width=\"1000\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "#### GLU Block\n",
    "\n",
    "Gated Linear Units act as an attention mechanism where the gates formed involve taking two dense layer outputs, applying a sigmoid to one of them, and then multiplying them together\n",
    "\n",
    "Following GLU blcok contains two dense layers, two ghost batch normalization layers, identity and sigmoid activation functions and multiplication operation.\n",
    "\n",
    "\n",
    "### Feature Transformer Block\n",
    "\n",
    "Builds two GLU blocks with a skip connection from the output of the first\n",
    "\n",
    "<img src=\"images/tabnet_feature_transformer.jpg\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "#### Attentive Transformer Block\n",
    "\n",
    "Use TabNet prior as an input to layer and reserve to handle prior updates in TabNet step layer\n",
    "\n",
    "> *prior is used to encourage orthogonal feature selection across decision steps, tell us what we know about features and how we have used them in the previous step\n",
    "\n",
    "<img src=\"images/tabnet_attentive_transformer.jpg\" width=\"200\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y-XevvaSe6_T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-XevvaSe6_T",
    "outputId": "8544313a-d02d-485e-cf73-736002fac06b"
   },
   "outputs": [],
   "source": [
    "# ! pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "_hHbyvL7Ub7X",
   "metadata": {
    "id": "_hHbyvL7Ub7X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import global_variables as gv\n",
    "import utilities\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0f3b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "5e0f3b0b",
    "outputId": "ee150c2f-7353-4144-8f21-99e54d86cac1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1319-0.0</th>\n",
       "      <th>1408-0.0</th>\n",
       "      <th>1329-0.0</th>\n",
       "      <th>1448-0.0</th>\n",
       "      <th>1538-0.0</th>\n",
       "      <th>6142-0.0</th>\n",
       "      <th>2050-0.0</th>\n",
       "      <th>1508-0.0</th>\n",
       "      <th>1339-0.0</th>\n",
       "      <th>30710-0.0</th>\n",
       "      <th>1349-0.0</th>\n",
       "      <th>30750-0.0</th>\n",
       "      <th>1468-0.0</th>\n",
       "      <th>20117-0.0</th>\n",
       "      <th>30740-0.0</th>\n",
       "      <th>1160-0.0</th>\n",
       "      <th>2090-0.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>1488-0.0</th>\n",
       "      <th>30850-0.0</th>\n",
       "      <th>4080-0.0</th>\n",
       "      <th>1369-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>1200-0.0</th>\n",
       "      <th>1289-0.0</th>\n",
       "      <th>30790-0.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>30630-0.0</th>\n",
       "      <th>1299-0.0</th>\n",
       "      <th>1220-0.0</th>\n",
       "      <th>1548-0.0</th>\n",
       "      <th>1528-0.0</th>\n",
       "      <th>23099-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>30690-0.0</th>\n",
       "      <th>1389-0.0</th>\n",
       "      <th>2654-0.0</th>\n",
       "      <th>1249-0.0</th>\n",
       "      <th>1309-0.0</th>\n",
       "      <th>1379-0.0</th>\n",
       "      <th>1239-0.0</th>\n",
       "      <th>21003-0.0</th>\n",
       "      <th>30780-0.0</th>\n",
       "      <th>1438-0.0</th>\n",
       "      <th>30870-0.0</th>\n",
       "      <th>1359-0.0</th>\n",
       "      <th>30770-0.0</th>\n",
       "      <th>21001-0.0</th>\n",
       "      <th>1458-0.0</th>\n",
       "      <th>23100-0.0</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>1418-0.0</th>\n",
       "      <th>1478-0.0</th>\n",
       "      <th>4079-0.0</th>\n",
       "      <th>30760-0.0</th>\n",
       "      <th>23101-0.0</th>\n",
       "      <th>2100-0.0</th>\n",
       "      <th>1428-0.0</th>\n",
       "      <th>30640-0.0</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>outcome_cardiomyopathies</th>\n",
       "      <th>outcome_ischemic_heart_disease</th>\n",
       "      <th>outcome_heart_failure</th>\n",
       "      <th>outcome_peripheral_vascular_disease</th>\n",
       "      <th>outcome_cardiac_arrest</th>\n",
       "      <th>outcome_cerebral_infarction</th>\n",
       "      <th>outcome_arrhythmia</th>\n",
       "      <th>outcome_myocardial_infarction</th>\n",
       "      <th>CVD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.937</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.622</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.508</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54.4035</td>\n",
       "      <td>20.90</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.593</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>35.6</td>\n",
       "      <td>102.0</td>\n",
       "      <td>6.477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.888</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.977</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.339</td>\n",
       "      <td>24.5790</td>\n",
       "      <td>3.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.706</td>\n",
       "      <td>45.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.900</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.052</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.088</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.47</td>\n",
       "      <td>36.5</td>\n",
       "      <td>113.0</td>\n",
       "      <td>5.512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.520</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.701</td>\n",
       "      <td>35.0861</td>\n",
       "      <td>7.00</td>\n",
       "      <td>42.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.173</td>\n",
       "      <td>74.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.310</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.515</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.1000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>7.079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.227</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.693</td>\n",
       "      <td>19.3835</td>\n",
       "      <td>7.00</td>\n",
       "      <td>15.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.490</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.300</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.449</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.675</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.5620</td>\n",
       "      <td>18.00</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.474</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>28.5</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.041</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.108</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.317</td>\n",
       "      <td>35.1281</td>\n",
       "      <td>7.00</td>\n",
       "      <td>31.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.169</td>\n",
       "      <td>79.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.616</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.04</td>\n",
       "      <td>20.162</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.1100</td>\n",
       "      <td>22.38</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>24.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.983</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.523</td>\n",
       "      <td>25.8866</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.053</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1319-0.0  1408-0.0  1329-0.0  1448-0.0  1538-0.0  6142-0.0  2050-0.0  \\\n",
       "0       0.0       1.0       2.0       3.0       2.0       1.0       2.0   \n",
       "1       0.0       3.0       2.0       1.0       0.0       1.0       1.0   \n",
       "2       0.0       3.0       3.0       2.0       1.0       2.0       1.0   \n",
       "3       3.0       3.0       3.0       3.0       0.0       2.0       1.0   \n",
       "4       0.0       3.0       2.0       1.0       0.0       5.0       2.0   \n",
       "\n",
       "   1508-0.0  1339-0.0  30710-0.0  1349-0.0  30750-0.0  1468-0.0  20117-0.0  \\\n",
       "0       3.0       2.0       0.34       1.0     34.937       3.0        2.0   \n",
       "1       2.0       2.0       3.94       4.0     40.900       5.0        2.0   \n",
       "2       2.0       2.0       0.55       1.0     40.000       1.0        0.0   \n",
       "3       2.0       2.0       0.45       2.0     37.300       4.0        2.0   \n",
       "4       2.0       2.0       0.75       2.0     32.200       1.0        2.0   \n",
       "\n",
       "   30740-0.0  1160-0.0  2090-0.0  31-0.0  1488-0.0  30850-0.0  4080-0.0  \\\n",
       "0      5.622       7.0       1.0     0.0      6.00      0.508     110.0   \n",
       "1      5.052       9.0       0.0     1.0      2.00     13.088     166.0   \n",
       "2      5.310       5.0       0.0     0.0      0.00      0.515     132.0   \n",
       "3      4.449       7.0       0.0     1.0      5.00      4.675     178.0   \n",
       "4      4.616       6.0       0.0     1.0      3.04     20.162     178.0   \n",
       "\n",
       "   1369-0.0  21000-0.0  1200-0.0  1289-0.0  30790-0.0  845-0.0  48-0.0  \\\n",
       "0       1.0     1001.0       3.0       6.0    54.4035    20.90    74.0   \n",
       "1       2.0     1001.0       2.0       2.0    15.4000    16.00   120.0   \n",
       "2       1.0     1001.0       3.0       2.0    32.1000    16.00    66.0   \n",
       "3       2.0     1001.0       1.0       3.0    43.5620    18.00   110.0   \n",
       "4       1.0     1001.0       3.0       1.0    71.1100    22.38    94.0   \n",
       "\n",
       "   30630-0.0  1299-0.0  1220-0.0  1548-0.0  1528-0.0  23099-0.0  49-0.0  \\\n",
       "0      1.593      10.0       0.0       2.0      2.00       35.6   102.0   \n",
       "1      1.390       2.0       0.0       2.0      2.47       36.5   113.0   \n",
       "2      2.005       4.0       0.0       1.0      1.00       29.5    88.0   \n",
       "3      1.474       2.0       0.0       1.0      2.00       28.5   117.0   \n",
       "4      2.149       1.0       0.0       2.0      2.00       24.8   100.0   \n",
       "\n",
       "   30690-0.0  1389-0.0  2654-0.0  1249-0.0  1309-0.0  1379-0.0  1239-0.0  \\\n",
       "0      6.477       1.0       6.0       1.0       2.0       1.0       0.0   \n",
       "1      5.512       1.0       7.0       1.0       1.0       2.0       0.0   \n",
       "2      7.079       1.0       7.0       3.0       4.0       2.0       0.0   \n",
       "3      5.028       0.0       7.0       1.0       1.0       2.0       1.0   \n",
       "4      7.958       1.0       7.0       2.0       1.0       1.0       0.0   \n",
       "\n",
       "   21003-0.0  30780-0.0  1438-0.0  30870-0.0  1359-0.0  30770-0.0  21001-0.0  \\\n",
       "0       54.0      3.888      10.0      0.977       2.0     26.339    24.5790   \n",
       "1       65.0      3.520      12.0      2.358       3.0     10.701    35.0861   \n",
       "2       69.0      4.227       8.0      0.655       2.0     10.693    19.3835   \n",
       "3       66.0      3.041      10.0      3.108       2.0     25.317    35.1281   \n",
       "4       48.0      4.983       8.0      1.173       1.0     26.523    25.8866   \n",
       "\n",
       "   1458-0.0  23100-0.0  6138-0.0  1418-0.0  1478-0.0  4079-0.0  30760-0.0  \\\n",
       "0      3.86       25.0       1.0       3.0       1.0      77.0      1.706   \n",
       "1      7.00       42.9       3.0       2.0       1.0      91.0      1.173   \n",
       "2      7.00       15.2       3.0       2.0       1.0      67.0      2.490   \n",
       "3      7.00       31.7       3.0       2.0       1.0      84.0      1.169   \n",
       "4      1.00       20.1       1.0       2.0       1.0      88.0      2.053   \n",
       "\n",
       "   23101-0.0  2100-0.0  1428-0.0  30640-0.0  hypertension  \\\n",
       "0       45.2       1.0       0.0      1.211             0   \n",
       "1       74.6       0.0       1.0      1.019             1   \n",
       "2       36.3       0.0       1.0      1.097             0   \n",
       "3       79.6       0.0       3.0      0.923             0   \n",
       "4       61.0       0.0       3.0      1.443             0   \n",
       "\n",
       "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
       "0                         0                               0   \n",
       "1                         0                               1   \n",
       "2                         0                               0   \n",
       "3                         0                               0   \n",
       "4                         0                               0   \n",
       "\n",
       "   outcome_heart_failure  outcome_peripheral_vascular_disease  \\\n",
       "0                      0                                    0   \n",
       "1                      0                                    0   \n",
       "2                      0                                    0   \n",
       "3                      0                                    0   \n",
       "4                      0                                    0   \n",
       "\n",
       "   outcome_cardiac_arrest  outcome_cerebral_infarction  outcome_arrhythmia  \\\n",
       "0                       0                            0                   1   \n",
       "1                       0                            0                   0   \n",
       "2                       0                            0                   0   \n",
       "3                       0                            0                   0   \n",
       "4                       0                            0                   0   \n",
       "\n",
       "   outcome_myocardial_infarction  CVD  \n",
       "0                              0    1  \n",
       "1                              1    0  \n",
       "2                              0    0  \n",
       "3                              0    0  \n",
       "4                              0    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/entire_imputed.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iiRxbosArp5N",
   "metadata": {
    "id": "iiRxbosArp5N"
   },
   "source": [
    "### Test TabNet Binary Classifier out-of-the-box (predicting Ischemic Heart Disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2d28ea4",
   "metadata": {
    "id": "c2d28ea4"
   },
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_test1, y_train1, y_val1, y_test1 = utilities.process_features(df, 'CVD', StandardScaler(), one_hot=False)\n",
    "X_train1, y_train1= utilities.resample_data(X_train1, y_train1, 'under')\n",
    "\n",
    "X_train= X_train1.to_numpy()\n",
    "X_val= X_val1.to_numpy()\n",
    "X_test= X_test1.to_numpy()\n",
    "\n",
    "y_train= y_train1.to_numpy()\n",
    "y_val= y_val1.to_numpy()\n",
    "y_test= y_test1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from keras import backend as K\n",
    "class my_recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return recall_score(y_true, y_score[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b9469f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b9469f1",
    "outputId": "68a9b50c-5aa6-4aaa-dd91-8bd3cc426dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 0.65793 | val_0_auc: 0.70643 |  0:00:06s\n",
      "epoch 1  | loss: 0.60939 | val_0_auc: 0.74607 |  0:00:12s\n",
      "epoch 2  | loss: 0.60229 | val_0_auc: 0.74914 |  0:00:18s\n",
      "epoch 3  | loss: 0.59971 | val_0_auc: 0.7559  |  0:00:25s\n",
      "epoch 4  | loss: 0.59702 | val_0_auc: 0.75689 |  0:00:31s\n",
      "epoch 5  | loss: 0.59724 | val_0_auc: 0.75719 |  0:00:38s\n",
      "epoch 6  | loss: 0.59738 | val_0_auc: 0.75714 |  0:00:44s\n",
      "epoch 7  | loss: 0.59574 | val_0_auc: 0.75604 |  0:00:50s\n",
      "epoch 8  | loss: 0.59653 | val_0_auc: 0.75653 |  0:00:56s\n",
      "epoch 9  | loss: 0.59542 | val_0_auc: 0.75806 |  0:01:03s\n",
      "epoch 10 | loss: 0.59546 | val_0_auc: 0.75766 |  0:01:08s\n",
      "epoch 11 | loss: 0.59546 | val_0_auc: 0.75845 |  0:01:14s\n",
      "epoch 12 | loss: 0.59478 | val_0_auc: 0.75922 |  0:01:21s\n",
      "epoch 13 | loss: 0.59488 | val_0_auc: 0.75999 |  0:01:28s\n",
      "epoch 14 | loss: 0.59451 | val_0_auc: 0.76046 |  0:01:35s\n",
      "epoch 15 | loss: 0.59383 | val_0_auc: 0.76063 |  0:01:46s\n",
      "epoch 16 | loss: 0.59582 | val_0_auc: 0.75933 |  0:01:55s\n",
      "epoch 17 | loss: 0.5939  | val_0_auc: 0.75995 |  0:02:04s\n",
      "epoch 18 | loss: 0.59409 | val_0_auc: 0.76035 |  0:02:12s\n",
      "epoch 19 | loss: 0.59395 | val_0_auc: 0.75973 |  0:02:19s\n",
      "epoch 20 | loss: 0.59351 | val_0_auc: 0.76056 |  0:02:26s\n",
      "epoch 21 | loss: 0.59427 | val_0_auc: 0.75998 |  0:02:32s\n",
      "epoch 22 | loss: 0.59331 | val_0_auc: 0.75862 |  0:02:39s\n",
      "epoch 23 | loss: 0.5935  | val_0_auc: 0.75784 |  0:02:46s\n",
      "epoch 24 | loss: 0.59322 | val_0_auc: 0.76076 |  0:02:53s\n",
      "epoch 25 | loss: 0.59231 | val_0_auc: 0.7599  |  0:03:00s\n",
      "epoch 26 | loss: 0.59342 | val_0_auc: 0.76003 |  0:03:06s\n",
      "epoch 27 | loss: 0.5935  | val_0_auc: 0.75981 |  0:03:13s\n",
      "epoch 28 | loss: 0.59322 | val_0_auc: 0.75956 |  0:03:20s\n",
      "epoch 29 | loss: 0.59311 | val_0_auc: 0.7601  |  0:03:27s\n",
      "epoch 30 | loss: 0.59401 | val_0_auc: 0.75928 |  0:03:34s\n",
      "epoch 31 | loss: 0.59358 | val_0_auc: 0.76046 |  0:03:41s\n",
      "epoch 32 | loss: 0.59299 | val_0_auc: 0.7602  |  0:03:49s\n",
      "epoch 33 | loss: 0.59293 | val_0_auc: 0.76063 |  0:03:57s\n",
      "epoch 34 | loss: 0.59379 | val_0_auc: 0.7596  |  0:04:03s\n",
      "\n",
      "Early stopping occured at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.76076\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier()  \n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "  eval_set=[(X_val, y_val)],\n",
    "  eval_metric=[\"auc\"]\n",
    ")\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "IJSsVVNmUEmR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "IJSsVVNmUEmR",
    "outputId": "526bce84-c3c0-4716-dd1e-2406bbcd5903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26ab57c9910>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(clf.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b6c65",
   "metadata": {},
   "source": [
    "### Global Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ouDy6aHNUHL5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ouDy6aHNUHL5",
    "outputId": "74754b86-04fe-420b-bdf1-4626a3540228",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_weights = clf.feature_importances_\n",
    "\n",
    "# zip to feature names\n",
    "input_cols = X_train1.columns.to_list()\n",
    "feat_dict = dict(zip(input_cols, feat_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "428e4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_d = dict( sorted(feat_dict.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcc8a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Contritbuting Features : \n",
      "['hypertension_1', 'age', 'whole body fat-free mass', '21000-0.0_3.0', 'testosterone', '6142-0.0_7.0', '6142-0.0_2.0', 'oily fish intake', 'waist circumference', '2100-0.0_1.0', '1508-0.0_3.0']\n"
     ]
    }
   ],
   "source": [
    "top = dict()\n",
    "# Iterate over all the items in dictionary and filter items which has even keys\n",
    "for (key, value) in sorted_d.items():\n",
    "   # Check if key is even then add pair to new dictionary\n",
    "   if value >= 0.01:\n",
    "        top[key] = value\n",
    "print('Top Contritbuting Features : ')\n",
    "replaced_list = [x if x not in gv.input_mapping else gv.input_mapping[x] for x in list(top.keys()) ]\n",
    "print(replaced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937288fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_contribution = dict()\n",
    "# Iterate over all the items in dictionary and filter items which has even keys\n",
    "for (key, value) in sorted_d.items():\n",
    "   # Check if key is even then add pair to new dictionary\n",
    "   if value >= 0.01:\n",
    "        c[key] = value\n",
    "print('Non-contritbuting Features : ')\n",
    "replaced_list2 = [x if x not in gv.input_mapping else gv.input_mapping[x] for x in list(v.keys()) ]\n",
    "print(replaced_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77063295",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'xlabels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27312/255482275.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Global Feature Importances'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'xlabels'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(feat_dict.items()) # sorted by key, return a list of tuples\n",
    "\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title('Global Feature Importances')\n",
    "plt.xlabels(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2062bd0",
   "metadata": {},
   "source": [
    "### Local Explainablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0274d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "TIcvwljsVTHq",
   "metadata": {
    "id": "TIcvwljsVTHq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7109673743008749"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LumhsZwC5P3j",
   "metadata": {
    "id": "LumhsZwC5P3j"
   },
   "source": [
    "## Customize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XryRDsqFpr0s",
   "metadata": {
    "id": "XryRDsqFpr0s"
   },
   "source": [
    "#### Fit parameters\n",
    "\n",
    "<ul>\n",
    "  <li> <b>X_train</b> (np.array): Training Features </li>\n",
    "  <li> <b>y_train</b> (np.array): Training Targets </li>\n",
    "  <li> <b>eval_set</b> (list of eval tuple set):  last one used for early stopping </li>\n",
    "  <li> <b>eval_name</b> (list of str): list of eval set names </li>\n",
    "  <li> <b>eval_metric</b> (list of str: list of evaluation metrics; last used for early stopping </li>\n",
    "  <li> <b>max_epochs</b> (int=200): max epochs for training</li>\n",
    "  <li> <b>patience</b> (int=10):#epochs before early stopping, if 0 then no early stopping performed </li>\n",
    "  <li> <b>weights</b> (int or dict=0): only for TabNetClassifier, sampling param 0 => no sampling, param 0 => automated sampling with inverse class occurences </li>\n",
    "  <li> <b>loss_fn</b>(torch.loss): loss fn for training, w classification can set a list of same length as num tasks  </li>\n",
    "  <li> <b>batch_size</b> (int=1024): #  examples/batch </li>\n",
    "  <li> <b>virtual_batch_size</b> (int=128): size of mini batches for ghost batch normalization  </li>\n",
    "  <li> <b>num_workers</b> (int=0): # workers used in torch.utils.data.Dataloader  </li>\n",
    "  <li> <b>drop_last</b> (bool=False): whether to drop last batch if not complete during training  </li>\n",
    "  <li> <b>callbacks</b> (list of callback fn): list of custom callbacks </li>\n",
    "  <li> <b>pretraining_ratio</b> (float): %input features to mask during pretraining  </li>\n",
    "  <li> <b>warm_start</b> (bool=False): allows to fit twice the same model and start from a warm start  </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [utilities.recall_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QlQWw4np5ufx",
   "metadata": {
    "id": "QlQWw4np5ufx"
   },
   "outputs": [],
   "source": [
    "unused_feat = ['Set']\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PO6tgvAF5XHN",
   "metadata": {
    "id": "PO6tgvAF5XHN"
   },
   "outputs": [],
   "source": [
    "tabnet_params = {\"cat_idxs\":cat_idxs, # list of categorical feature indices\n",
    "                 \"cat_dims\":cat_dims, # list of categorical features number of modalities (#unique values for a categorical feature)\n",
    "                 \"cat_emb_dim\":1, # list of embeddings size for each categorical features\n",
    "                 \"optimizer_fn\":torch.optim.Adam, # pytorch optimizer function\n",
    "                 \"optimizer_params\":dict(lr=2e-2), # parameters compatible with optimizer_fn\n",
    "                 \"scheduler_params\":{\"step_size\":50, # how to use learning rate scheduler\n",
    "                                 \"gamma\":0.9}, # dictionary of parameters to apply to the scheduler\n",
    "                 \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
    "                 \"mask_type\":'entmax' # \"sparsemax\" # either sparsemax or entmac, masking fn for selecting features\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSF99rpmt5L1",
   "metadata": {
    "id": "KSF99rpmt5L1"
   },
   "outputs": [],
   "source": [
    "# BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AtEemh_pcdHR",
   "metadata": {
    "id": "AtEemh_pcdHR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "WxTTTOmlr0__",
   "metadata": {
    "id": "WxTTTOmlr0__"
   },
   "source": [
    "### Implement Semi-supervised Pre-training (tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jMghm70xsycy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "jMghm70xsycy",
    "outputId": "67ebbf4b-fb17-4e92-e08b-a89e17127ddd"
   },
   "outputs": [],
   "source": [
    "# import preprocessed data before imputation\n",
    "df2 = pd.read_csv(gv.tabnet_data)\n",
    "df2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec6b00",
   "metadata": {
    "id": "12ec6b00"
   },
   "outputs": [],
   "source": [
    "# TabNetPretrainer\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax' # \"sparsemax\"\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_val],\n",
    "    pretraining_ratio=0.8,\n",
    ")\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                      \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax' # This will be overwritten if using pretrain model\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    from_unsupervised=unsupervised_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmeP4DRTuHnN",
   "metadata": {
    "id": "fmeP4DRTuHnN"
   },
   "source": [
    "### Save & Load TabNet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CdyC7g9ruJ6P",
   "metadata": {
    "id": "CdyC7g9ruJ6P"
   },
   "outputs": [],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filepath)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "MODEL_tabnet_pytorch.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
