{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385002dd",
   "metadata": {},
   "source": [
    "## TabNet Implentation for Tabular Data\n",
    "\n",
    "TabNet is proposed in [this article] (https://arxiv.org/abs/1908.07442) as a neural network architecture capable of learning a canonical representation of tabular data. This architecture has shown to perform well against the current gold-standard gradient boosting models for learning on tabular data.\n",
    "\n",
    "TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful\n",
    "features to process at each decision step. Instance-wise feature selection enables efficient learning as the model capacity is fully used for the most salient features, and also yields\n",
    "more interpretable decision making via visualization of selection masks. \n",
    "\n",
    "\n",
    "This implementation closely follows [the TabNet implementation in PyTorch linked here](https://github.com/dreamquark-ai/tabnet/tree/b6e1ebaf694f37ad40a6ba525aa016fd3cec15da). \n",
    "\n",
    "<img src=\"images/tabnet_schematic2.jpg\" width=\"1000\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "#### GLU Block\n",
    "\n",
    "Gated Linear Units act as an attention mechanism where the gates formed involve taking two dense layer outputs, applying a sigmoid to one of them, and then multiplying them together\n",
    "\n",
    "Following GLU blcok contains two dense layers, two ghost batch normalization layers, identity and sigmoid activation functions and multiplication operation.\n",
    "\n",
    "\n",
    "### Feature Transformer Block\n",
    "\n",
    "Builds two GLU blocks with a skip connection from the output of the first\n",
    "\n",
    "<img src=\"images/tabnet_feature_transformer.jpg\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "#### Attentive Transformer Block\n",
    "\n",
    "Use TabNet prior as an input to layer and reserve to handle prior updates in TabNet step layer\n",
    "\n",
    "> *prior is used to encourage orthogonal feature selection across decision steps, tell us what we know about features and how we have used them in the previous step\n",
    "\n",
    "<img src=\"images/tabnet_attentive_transformer.jpg\" width=\"200\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y-XevvaSe6_T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-XevvaSe6_T",
    "outputId": "8544313a-d02d-485e-cf73-736002fac06b"
   },
   "outputs": [],
   "source": [
    "# ! pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_hHbyvL7Ub7X",
   "metadata": {
    "id": "_hHbyvL7Ub7X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import global_variables as gv\n",
    "import utilities\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0f3b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "5e0f3b0b",
    "outputId": "ee150c2f-7353-4144-8f21-99e54d86cac1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1319-0.0</th>\n",
       "      <th>1408-0.0</th>\n",
       "      <th>1329-0.0</th>\n",
       "      <th>1448-0.0</th>\n",
       "      <th>1538-0.0</th>\n",
       "      <th>6142-0.0</th>\n",
       "      <th>2050-0.0</th>\n",
       "      <th>1508-0.0</th>\n",
       "      <th>1339-0.0</th>\n",
       "      <th>30710-0.0</th>\n",
       "      <th>1349-0.0</th>\n",
       "      <th>30750-0.0</th>\n",
       "      <th>1468-0.0</th>\n",
       "      <th>20117-0.0</th>\n",
       "      <th>30740-0.0</th>\n",
       "      <th>1160-0.0</th>\n",
       "      <th>2090-0.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>1488-0.0</th>\n",
       "      <th>30850-0.0</th>\n",
       "      <th>4080-0.0</th>\n",
       "      <th>1369-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>1200-0.0</th>\n",
       "      <th>1289-0.0</th>\n",
       "      <th>30790-0.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>30630-0.0</th>\n",
       "      <th>1299-0.0</th>\n",
       "      <th>1220-0.0</th>\n",
       "      <th>1548-0.0</th>\n",
       "      <th>1528-0.0</th>\n",
       "      <th>23099-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>30690-0.0</th>\n",
       "      <th>1389-0.0</th>\n",
       "      <th>2654-0.0</th>\n",
       "      <th>1249-0.0</th>\n",
       "      <th>1309-0.0</th>\n",
       "      <th>1379-0.0</th>\n",
       "      <th>1239-0.0</th>\n",
       "      <th>21003-0.0</th>\n",
       "      <th>30780-0.0</th>\n",
       "      <th>1438-0.0</th>\n",
       "      <th>30870-0.0</th>\n",
       "      <th>1359-0.0</th>\n",
       "      <th>30770-0.0</th>\n",
       "      <th>21001-0.0</th>\n",
       "      <th>1458-0.0</th>\n",
       "      <th>23100-0.0</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>1418-0.0</th>\n",
       "      <th>1478-0.0</th>\n",
       "      <th>4079-0.0</th>\n",
       "      <th>30760-0.0</th>\n",
       "      <th>23101-0.0</th>\n",
       "      <th>2100-0.0</th>\n",
       "      <th>1428-0.0</th>\n",
       "      <th>30640-0.0</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>outcome_cardiomyopathies</th>\n",
       "      <th>outcome_ischemic_heart_disease</th>\n",
       "      <th>outcome_heart_failure</th>\n",
       "      <th>outcome_peripheral_vascular_disease</th>\n",
       "      <th>outcome_cardiac_arrest</th>\n",
       "      <th>outcome_cerebral_infarction</th>\n",
       "      <th>outcome_arrhythmia</th>\n",
       "      <th>outcome_myocardial_infarction</th>\n",
       "      <th>CVD</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>age-binned</th>\n",
       "      <th>race-binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.937</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.622</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.508</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54.4035</td>\n",
       "      <td>20.90</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.593</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>35.6</td>\n",
       "      <td>102.0</td>\n",
       "      <td>6.477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.888</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.977</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.339</td>\n",
       "      <td>24.5790</td>\n",
       "      <td>3.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.706</td>\n",
       "      <td>45.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "      <td>50-59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.900</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.052</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.088</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.47</td>\n",
       "      <td>36.5</td>\n",
       "      <td>113.0</td>\n",
       "      <td>5.512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.520</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.701</td>\n",
       "      <td>35.0861</td>\n",
       "      <td>7.00</td>\n",
       "      <td>42.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.173</td>\n",
       "      <td>74.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "      <td>60-69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.310</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.515</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.1000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>7.079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.227</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.693</td>\n",
       "      <td>19.3835</td>\n",
       "      <td>7.00</td>\n",
       "      <td>15.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.490</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "      <td>60-69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.300</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.449</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.675</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.5620</td>\n",
       "      <td>18.00</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1.474</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>28.5</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.041</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.108</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.317</td>\n",
       "      <td>35.1281</td>\n",
       "      <td>7.00</td>\n",
       "      <td>31.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.169</td>\n",
       "      <td>79.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "      <td>60-69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.616</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.04</td>\n",
       "      <td>20.162</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.1100</td>\n",
       "      <td>22.38</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>24.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.983</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.523</td>\n",
       "      <td>25.8866</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.053</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "      <td>40-49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1319-0.0  1408-0.0  1329-0.0  1448-0.0  1538-0.0  6142-0.0  2050-0.0  \\\n",
       "0       0.0       1.0       2.0       3.0       2.0       1.0       2.0   \n",
       "1       0.0       3.0       2.0       1.0       0.0       1.0       1.0   \n",
       "2       0.0       3.0       3.0       2.0       1.0       2.0       1.0   \n",
       "3       3.0       3.0       3.0       3.0       0.0       2.0       1.0   \n",
       "4       0.0       3.0       2.0       1.0       0.0       5.0       2.0   \n",
       "\n",
       "   1508-0.0  1339-0.0  30710-0.0  1349-0.0  30750-0.0  1468-0.0  20117-0.0  \\\n",
       "0       3.0       2.0       0.34       1.0     34.937       3.0        2.0   \n",
       "1       2.0       2.0       3.94       4.0     40.900       5.0        2.0   \n",
       "2       2.0       2.0       0.55       1.0     40.000       1.0        0.0   \n",
       "3       2.0       2.0       0.45       2.0     37.300       4.0        2.0   \n",
       "4       2.0       2.0       0.75       2.0     32.200       1.0        2.0   \n",
       "\n",
       "   30740-0.0  1160-0.0  2090-0.0  31-0.0  1488-0.0  30850-0.0  4080-0.0  \\\n",
       "0      5.622       7.0       1.0     0.0      6.00      0.508     110.0   \n",
       "1      5.052       9.0       0.0     1.0      2.00     13.088     166.0   \n",
       "2      5.310       5.0       0.0     0.0      0.00      0.515     132.0   \n",
       "3      4.449       7.0       0.0     1.0      5.00      4.675     178.0   \n",
       "4      4.616       6.0       0.0     1.0      3.04     20.162     178.0   \n",
       "\n",
       "   1369-0.0  21000-0.0  1200-0.0  1289-0.0  30790-0.0  845-0.0  48-0.0  \\\n",
       "0       1.0     1001.0       3.0       6.0    54.4035    20.90    74.0   \n",
       "1       2.0     1001.0       2.0       2.0    15.4000    16.00   120.0   \n",
       "2       1.0     1001.0       3.0       2.0    32.1000    16.00    66.0   \n",
       "3       2.0     1001.0       1.0       3.0    43.5620    18.00   110.0   \n",
       "4       1.0     1001.0       3.0       1.0    71.1100    22.38    94.0   \n",
       "\n",
       "   30630-0.0  1299-0.0  1220-0.0  1548-0.0  1528-0.0  23099-0.0  49-0.0  \\\n",
       "0      1.593      10.0       0.0       2.0      2.00       35.6   102.0   \n",
       "1      1.390       2.0       0.0       2.0      2.47       36.5   113.0   \n",
       "2      2.005       4.0       0.0       1.0      1.00       29.5    88.0   \n",
       "3      1.474       2.0       0.0       1.0      2.00       28.5   117.0   \n",
       "4      2.149       1.0       0.0       2.0      2.00       24.8   100.0   \n",
       "\n",
       "   30690-0.0  1389-0.0  2654-0.0  1249-0.0  1309-0.0  1379-0.0  1239-0.0  \\\n",
       "0      6.477       1.0       6.0       1.0       2.0       1.0       0.0   \n",
       "1      5.512       1.0       7.0       1.0       1.0       2.0       0.0   \n",
       "2      7.079       1.0       7.0       3.0       4.0       2.0       0.0   \n",
       "3      5.028       0.0       7.0       1.0       1.0       2.0       1.0   \n",
       "4      7.958       1.0       7.0       2.0       1.0       1.0       0.0   \n",
       "\n",
       "   21003-0.0  30780-0.0  1438-0.0  30870-0.0  1359-0.0  30770-0.0  21001-0.0  \\\n",
       "0       54.0      3.888      10.0      0.977       2.0     26.339    24.5790   \n",
       "1       65.0      3.520      12.0      2.358       3.0     10.701    35.0861   \n",
       "2       69.0      4.227       8.0      0.655       2.0     10.693    19.3835   \n",
       "3       66.0      3.041      10.0      3.108       2.0     25.317    35.1281   \n",
       "4       48.0      4.983       8.0      1.173       1.0     26.523    25.8866   \n",
       "\n",
       "   1458-0.0  23100-0.0  6138-0.0  1418-0.0  1478-0.0  4079-0.0  30760-0.0  \\\n",
       "0      3.86       25.0       1.0       3.0       1.0      77.0      1.706   \n",
       "1      7.00       42.9       3.0       2.0       1.0      91.0      1.173   \n",
       "2      7.00       15.2       3.0       2.0       1.0      67.0      2.490   \n",
       "3      7.00       31.7       3.0       2.0       1.0      84.0      1.169   \n",
       "4      1.00       20.1       1.0       2.0       1.0      88.0      2.053   \n",
       "\n",
       "   23101-0.0  2100-0.0  1428-0.0  30640-0.0  hypertension  \\\n",
       "0       45.2       1.0       0.0      1.211             0   \n",
       "1       74.6       0.0       1.0      1.019             1   \n",
       "2       36.3       0.0       1.0      1.097             0   \n",
       "3       79.6       0.0       3.0      0.923             0   \n",
       "4       61.0       0.0       3.0      1.443             0   \n",
       "\n",
       "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
       "0                         0                               0   \n",
       "1                         0                               1   \n",
       "2                         0                               0   \n",
       "3                         0                               0   \n",
       "4                         0                               0   \n",
       "\n",
       "   outcome_heart_failure  outcome_peripheral_vascular_disease  \\\n",
       "0                      0                                    0   \n",
       "1                      0                                    0   \n",
       "2                      0                                    0   \n",
       "3                      0                                    0   \n",
       "4                      0                                    0   \n",
       "\n",
       "   outcome_cardiac_arrest  outcome_cerebral_infarction  outcome_arrhythmia  \\\n",
       "0                       0                            0                   1   \n",
       "1                       0                            0                   0   \n",
       "2                       0                            0                   0   \n",
       "3                       0                            0                   0   \n",
       "4                       0                            0                   0   \n",
       "\n",
       "   outcome_myocardial_infarction  CVD   age     sex     race age-binned  \\\n",
       "0                              0    1  54.0  Female  British      50-59   \n",
       "1                              1    0  65.0    Male  British      60-69   \n",
       "2                              0    0  69.0  Female  British      60-69   \n",
       "3                              0    0  66.0    Male  British      60-69   \n",
       "4                              0    0  48.0    Male  British      40-49   \n",
       "\n",
       "   race-binary  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/binary_full.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iiRxbosArp5N",
   "metadata": {
    "id": "iiRxbosArp5N"
   },
   "source": [
    "### Test TabNet Binary Classifier out-of-the-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d28ea4",
   "metadata": {
    "id": "c2d28ea4"
   },
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_test1, y_train1, y_val1, y_test1 = utilities.process_features(df, 'CVD', QuantileTransformer(output_distribution='uniform'), one_hot=False)\n",
    "X_train1, y_train1= utilities.resample_data(X_train1, y_train1, 'under')\n",
    "\n",
    "X_train= X_train1.to_numpy()\n",
    "X_val= X_val1.to_numpy()\n",
    "X_test= X_test1.to_numpy()\n",
    "\n",
    "y_train= y_train1.to_numpy().astype(int)\n",
    "y_val= y_val1.to_numpy().astype(int)\n",
    "y_test= y_test1.to_numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61ae020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from keras import backend as K\n",
    "class my_recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return recall_score(y_true, y_score[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9469f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b9469f1",
    "outputId": "68a9b50c-5aa6-4aaa-dd91-8bd3cc426dda",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = TabNetClassifier()  \n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "  eval_set=[(X_val, y_val)],\n",
    "  eval_metric=[\"auc\"]\n",
    ")\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a96fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CVD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IJSsVVNmUEmR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "IJSsVVNmUEmR",
    "outputId": "526bce84-c3c0-4716-dd1e-2406bbcd5903"
   },
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(clf.history['loss'])\n",
    "plt.title('Basic TabNet Loss CVD')\n",
    "plt.savefig('charts/models/TabNet/basic_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b6c65",
   "metadata": {},
   "source": [
    "### Global Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ouDy6aHNUHL5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ouDy6aHNUHL5",
    "outputId": "74754b86-04fe-420b-bdf1-4626a3540228",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_weights = clf.feature_importances_\n",
    "\n",
    "# zip to feature names\n",
    "input_cols = X_train1.columns.to_list()\n",
    "feat_dict = dict(zip(input_cols, feat_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_d = dict( sorted(feat_dict.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028551da",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = dict()\n",
    "# Iterate over all the items in dictionary and filter items which has even keys\n",
    "for (key, value) in sorted_d.items():\n",
    "   # Check if key is even then add pair to new dictionary\n",
    "   if value >= 0.01:\n",
    "        top[key] = value\n",
    "print('Top Contritbuting Features : ')\n",
    "replaced_list = [x if x not in gv.input_mapping else gv.input_mapping[x] for x in list(top.keys()) ]\n",
    "print(replaced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b98971",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_contribution = dict()\n",
    "# Iterate over all the items in dictionary and filter items which has even keys\n",
    "for (key, value) in sorted_d.items():\n",
    "   # Check if key is even then add pair to new dictionary\n",
    "   if value ==0:\n",
    "        no_contribution[key] = value\n",
    "print('Non-contritbuting Features : ')\n",
    "replaced_list2 = [x if x not in gv.input_mapping else gv.input_mapping[x] for x in list(no_contribution.keys()) ]\n",
    "print(replaced_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replaced_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(feat_dict.items()) # sorted by key, return a list of tuples\n",
    "\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "frame1 = plt.gca()\n",
    "plt.plot(x, y)\n",
    "plt.title('Global Feature Importances')\n",
    "frame1.axes.xaxis.set_ticklabels([])\n",
    "plt.show()\n",
    "plt.savefig('charts/models/TabNet/basic_global_features.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2062bd0",
   "metadata": {},
   "source": [
    "### Local Explainablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIcvwljsVTHq",
   "metadata": {
    "id": "TIcvwljsVTHq"
   },
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83892756",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n",
    "\n",
    "plt.savefig('charts/models/TabNet/basic_masks.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LumhsZwC5P3j",
   "metadata": {
    "id": "LumhsZwC5P3j"
   },
   "source": [
    "## Customize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XryRDsqFpr0s",
   "metadata": {
    "id": "XryRDsqFpr0s"
   },
   "source": [
    "#### Fit parameters\n",
    "\n",
    "<ul>\n",
    "  <li> <b>X_train</b> (np.array): Training Features </li>\n",
    "  <li> <b>y_train</b> (np.array): Training Targets </li>\n",
    "  <li> <b>eval_set</b> (list of eval tuple set):  last one used for early stopping </li>\n",
    "  <li> <b>eval_name</b> (list of str): list of eval set names </li>\n",
    "  <li> <b>eval_metric</b> (list of str: list of evaluation metrics; last used for early stopping </li>\n",
    "  <li> <b>max_epochs</b> (int=200): max epochs for training</li>\n",
    "  <li> <b>patience</b> (int=10):#epochs before early stopping, if 0 then no early stopping performed </li>\n",
    "  <li> <b>weights</b> (int or dict=0): only for TabNetClassifier, sampling param 0 => no sampling, param 0 => automated sampling with inverse class occurences </li>\n",
    "  <li> <b>loss_fn</b>(torch.loss): loss fn for training, w classification can set a list of same length as num tasks  </li>\n",
    "  <li> <b>batch_size</b> (int=1024): #  examples/batch </li>\n",
    "  <li> <b>virtual_batch_size</b> (int=128): size of mini batches for ghost batch normalization  </li>\n",
    "  <li> <b>num_workers</b> (int=0): # workers used in torch.utils.data.Dataloader  </li>\n",
    "  <li> <b>drop_last</b> (bool=False): whether to drop last batch if not complete during training  </li>\n",
    "  <li> <b>callbacks</b> (list of callback fn): list of custom callbacks </li>\n",
    "  <li> <b>pretraining_ratio</b> (float): %input features to mask during pretraining  </li>\n",
    "  <li> <b>warm_start</b> (bool=False): allows to fit twice the same model and start from a warm start  </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [utilities.recall_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QlQWw4np5ufx",
   "metadata": {
    "id": "QlQWw4np5ufx"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_18768/3744083480.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\anali\\AppData\\Local\\Temp/ipykernel_18768/3744083480.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    cat_dims = [ categorical_dims[f] for i, f in enumerat\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_cols]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PO6tgvAF5XHN",
   "metadata": {
    "id": "PO6tgvAF5XHN"
   },
   "outputs": [],
   "source": [
    "tabnet_params = {\"cat_idxs\":cat_idxs, # list of categorical feature indices\n",
    "                 \"cat_dims\":cat_dims, # list of categorical features number of modalities (#unique values for a categorical feature)\n",
    "                 \"cat_emb_dim\":1, # list of embeddings size for each categorical features\n",
    "                 \"optimizer_fn\":torch.optim.Adam, # pytorch optimizer function\n",
    "                 \"optimizer_params\":dict(lr=2e-2), # parameters compatible with optimizer_fn\n",
    "                 \"scheduler_params\":{\"step_size\":50, # how to use learning rate scheduler\n",
    "                                 \"gamma\":0.9}, # dictionary of parameters to apply to the scheduler\n",
    "                 \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
    "                 \"mask_type\":'entmax' # \"sparsemax\" # either sparsemax or entmac, masking fn for selecting features\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1729ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# custom metrics\n",
    "class Recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return recall_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / predicted_positives \n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "428ed33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "clf = TabNetClassifier(\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scheduler_params={\"mode\":'max', # max because default eval metric for binary is AUC\n",
    "                 \"factor\":0.1,\n",
    "                 \"patience\":1}\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473852a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.6493  | train_auc: 0.7272  | train_accuracy: 0.67665 | val_auc: 0.73513 | val_accuracy: 0.61267 |  0:00:21s\n",
      "epoch 1  | loss: 0.60668 | train_auc: 0.74737 | train_accuracy: 0.68938 | val_auc: 0.75559 | val_accuracy: 0.66944 |  0:00:56s\n",
      "epoch 2  | loss: 0.59758 | train_auc: 0.74929 | train_accuracy: 0.69043 | val_auc: 0.75676 | val_accuracy: 0.66186 |  0:01:58s\n",
      "epoch 3  | loss: 0.59517 | train_auc: 0.75126 | train_accuracy: 0.6914  | val_auc: 0.7593  | val_accuracy: 0.68246 |  0:02:38s\n",
      "epoch 4  | loss: 0.59961 | train_auc: 0.74791 | train_accuracy: 0.69019 | val_auc: 0.756   | val_accuracy: 0.67699 |  0:03:14s\n",
      "epoch 5  | loss: 0.59767 | train_auc: 0.75005 | train_accuracy: 0.6903  | val_auc: 0.75855 | val_accuracy: 0.72249 |  0:03:57s\n",
      "epoch 6  | loss: 0.5964  | train_auc: 0.75192 | train_accuracy: 0.69174 | val_auc: 0.75975 | val_accuracy: 0.69007 |  0:04:25s\n",
      "epoch 7  | loss: 0.5941  | train_auc: 0.7508  | train_accuracy: 0.69022 | val_auc: 0.75973 | val_accuracy: 0.71047 |  0:04:48s\n",
      "epoch 8  | loss: 0.59424 | train_auc: 0.75165 | train_accuracy: 0.6922  | val_auc: 0.76075 | val_accuracy: 0.69165 |  0:05:14s\n",
      "epoch 9  | loss: 0.59074 | train_auc: 0.75187 | train_accuracy: 0.6919  | val_auc: 0.7611  | val_accuracy: 0.69883 |  0:05:34s\n",
      "epoch 10 | loss: 0.59001 | train_auc: 0.75186 | train_accuracy: 0.6922  | val_auc: 0.76104 | val_accuracy: 0.69669 |  0:06:00s\n",
      "epoch 11 | loss: 0.59379 | train_auc: 0.75193 | train_accuracy: 0.69209 | val_auc: 0.76108 | val_accuracy: 0.69812 |  0:06:27s\n",
      "epoch 12 | loss: 0.59181 | train_auc: 0.75179 | train_accuracy: 0.6919  | val_auc: 0.761   | val_accuracy: 0.69775 |  0:06:53s\n",
      "epoch 13 | loss: 0.59123 | train_auc: 0.75188 | train_accuracy: 0.69214 | val_auc: 0.76103 | val_accuracy: 0.69656 |  0:07:07s\n",
      "epoch 14 | loss: 0.59467 | train_auc: 0.75185 | train_accuracy: 0.69225 | val_auc: 0.76106 | val_accuracy: 0.69682 |  0:07:21s\n",
      "epoch 15 | loss: 0.59524 | train_auc: 0.75189 | train_accuracy: 0.69179 | val_auc: 0.76112 | val_accuracy: 0.69825 |  0:07:38s\n",
      "epoch 16 | loss: 0.59252 | train_auc: 0.75184 | train_accuracy: 0.69182 | val_auc: 0.76106 | val_accuracy: 0.69822 |  0:07:55s\n",
      "epoch 17 | loss: 0.5916  | train_auc: 0.75187 | train_accuracy: 0.69193 | val_auc: 0.76108 | val_accuracy: 0.69933 |  0:08:15s\n",
      "epoch 18 | loss: 0.59289 | train_auc: 0.75191 | train_accuracy: 0.69196 | val_auc: 0.76105 | val_accuracy: 0.69855 |  0:08:33s\n",
      "epoch 19 | loss: 0.59404 | train_auc: 0.75191 | train_accuracy: 0.69212 | val_auc: 0.76105 | val_accuracy: 0.6974  |  0:08:49s\n",
      "epoch 20 | loss: 0.59015 | train_auc: 0.75187 | train_accuracy: 0.69192 | val_auc: 0.76103 | val_accuracy: 0.69792 |  0:09:05s\n",
      "epoch 21 | loss: 0.59541 | train_auc: 0.75186 | train_accuracy: 0.6918  | val_auc: 0.76103 | val_accuracy: 0.69764 |  0:09:20s\n",
      "epoch 22 | loss: 0.59383 | train_auc: 0.75189 | train_accuracy: 0.69203 | val_auc: 0.76104 | val_accuracy: 0.6978  |  0:09:37s\n",
      "epoch 23 | loss: 0.59022 | train_auc: 0.75186 | train_accuracy: 0.69225 | val_auc: 0.76104 | val_accuracy: 0.69689 |  0:09:53s\n",
      "epoch 24 | loss: 0.58882 | train_auc: 0.75187 | train_accuracy: 0.69216 | val_auc: 0.76107 | val_accuracy: 0.69662 |  0:10:09s\n",
      "epoch 25 | loss: 0.59024 | train_auc: 0.75184 | train_accuracy: 0.69164 | val_auc: 0.76102 | val_accuracy: 0.69825 |  0:10:25s\n",
      "epoch 26 | loss: 0.58805 | train_auc: 0.75182 | train_accuracy: 0.69172 | val_auc: 0.76099 | val_accuracy: 0.69856 |  0:10:43s\n",
      "epoch 27 | loss: 0.59158 | train_auc: 0.75186 | train_accuracy: 0.69196 | val_auc: 0.76099 | val_accuracy: 0.6973  |  0:10:58s\n",
      "epoch 28 | loss: 0.59608 | train_auc: 0.75188 | train_accuracy: 0.69211 | val_auc: 0.76103 | val_accuracy: 0.69649 |  0:11:14s\n",
      "epoch 29 | loss: 0.59429 | train_auc: 0.75185 | train_accuracy: 0.69216 | val_auc: 0.76108 | val_accuracy: 0.69724 |  0:11:30s\n",
      "epoch 30 | loss: 0.59378 | train_auc: 0.7519  | train_accuracy: 0.69208 | val_auc: 0.76105 | val_accuracy: 0.69712 |  0:11:48s\n",
      "epoch 31 | loss: 0.59388 | train_auc: 0.75186 | train_accuracy: 0.692   | val_auc: 0.76095 | val_accuracy: 0.69775 |  0:12:04s\n",
      "epoch 32 | loss: 0.59391 | train_auc: 0.7519  | train_accuracy: 0.69185 | val_auc: 0.76106 | val_accuracy: 0.69832 |  0:12:19s\n",
      "epoch 33 | loss: 0.59344 | train_auc: 0.75187 | train_accuracy: 0.69236 | val_auc: 0.76106 | val_accuracy: 0.69699 |  0:12:35s\n",
      "epoch 34 | loss: 0.59479 | train_auc: 0.75185 | train_accuracy: 0.69193 | val_auc: 0.76103 | val_accuracy: 0.69696 |  0:12:52s\n",
      "epoch 35 | loss: 0.59322 | train_auc: 0.75187 | train_accuracy: 0.69206 | val_auc: 0.76107 | val_accuracy: 0.69836 |  0:13:08s\n",
      "epoch 36 | loss: 0.59298 | train_auc: 0.7519  | train_accuracy: 0.69219 | val_auc: 0.76107 | val_accuracy: 0.69802 |  0:13:24s\n",
      "epoch 37 | loss: 0.59289 | train_auc: 0.75184 | train_accuracy: 0.69204 | val_auc: 0.76109 | val_accuracy: 0.69772 |  0:13:40s\n",
      "epoch 38 | loss: 0.59086 | train_auc: 0.75187 | train_accuracy: 0.69217 | val_auc: 0.76097 | val_accuracy: 0.69729 |  0:13:58s\n",
      "epoch 39 | loss: 0.59381 | train_auc: 0.75187 | train_accuracy: 0.6924  | val_auc: 0.76105 | val_accuracy: 0.69684 |  0:14:14s\n",
      "epoch 40 | loss: 0.59297 | train_auc: 0.75183 | train_accuracy: 0.69193 | val_auc: 0.76101 | val_accuracy: 0.6972  |  0:14:36s\n",
      "epoch 41 | loss: 0.59076 | train_auc: 0.75187 | train_accuracy: 0.69201 | val_auc: 0.76105 | val_accuracy: 0.69868 |  0:15:39s\n",
      "epoch 42 | loss: 0.59594 | train_auc: 0.75181 | train_accuracy: 0.69171 | val_auc: 0.76103 | val_accuracy: 0.6985  |  0:16:17s\n",
      "epoch 43 | loss: 0.59499 | train_auc: 0.75192 | train_accuracy: 0.69217 | val_auc: 0.76105 | val_accuracy: 0.69692 |  0:16:58s\n",
      "epoch 44 | loss: 0.59134 | train_auc: 0.75181 | train_accuracy: 0.692   | val_auc: 0.76103 | val_accuracy: 0.69752 |  0:17:25s\n",
      "epoch 45 | loss: 0.59139 | train_auc: 0.7519  | train_accuracy: 0.69216 | val_auc: 0.76102 | val_accuracy: 0.69709 |  0:17:48s\n",
      "epoch 46 | loss: 0.59417 | train_auc: 0.75189 | train_accuracy: 0.69214 | val_auc: 0.761   | val_accuracy: 0.69823 |  0:18:25s\n",
      "epoch 47 | loss: 0.59193 | train_auc: 0.75185 | train_accuracy: 0.69228 | val_auc: 0.76107 | val_accuracy: 0.69715 |  0:18:54s\n",
      "epoch 48 | loss: 0.59148 | train_auc: 0.75186 | train_accuracy: 0.69224 | val_auc: 0.76103 | val_accuracy: 0.69626 |  0:19:25s\n",
      "epoch 49 | loss: 0.59249 | train_auc: 0.75192 | train_accuracy: 0.69201 | val_auc: 0.76107 | val_accuracy: 0.69677 |  0:19:53s\n",
      "epoch 50 | loss: 0.59154 | train_auc: 0.75187 | train_accuracy: 0.69206 | val_auc: 0.76102 | val_accuracy: 0.69752 |  0:20:20s\n",
      "epoch 51 | loss: 0.59244 | train_auc: 0.75188 | train_accuracy: 0.69198 | val_auc: 0.76104 | val_accuracy: 0.69787 |  0:20:53s\n",
      "epoch 52 | loss: 0.59366 | train_auc: 0.75184 | train_accuracy: 0.69177 | val_auc: 0.7611  | val_accuracy: 0.69855 |  0:21:25s\n",
      "epoch 53 | loss: 0.59403 | train_auc: 0.75185 | train_accuracy: 0.69225 | val_auc: 0.76098 | val_accuracy: 0.6971  |  0:22:01s\n",
      "epoch 54 | loss: 0.59496 | train_auc: 0.75185 | train_accuracy: 0.69227 | val_auc: 0.76096 | val_accuracy: 0.69644 |  0:22:35s\n",
      "epoch 55 | loss: 0.59497 | train_auc: 0.75187 | train_accuracy: 0.692   | val_auc: 0.76101 | val_accuracy: 0.69709 |  0:22:58s\n",
      "epoch 56 | loss: 0.59224 | train_auc: 0.75195 | train_accuracy: 0.69222 | val_auc: 0.76107 | val_accuracy: 0.69757 |  0:23:21s\n",
      "epoch 57 | loss: 0.59415 | train_auc: 0.75187 | train_accuracy: 0.6923  | val_auc: 0.76105 | val_accuracy: 0.69715 |  0:23:45s\n",
      "epoch 58 | loss: 0.59591 | train_auc: 0.75187 | train_accuracy: 0.69232 | val_auc: 0.76108 | val_accuracy: 0.698   |  0:24:07s\n",
      "epoch 59 | loss: 0.59562 | train_auc: 0.75184 | train_accuracy: 0.69227 | val_auc: 0.76106 | val_accuracy: 0.69667 |  0:24:28s\n",
      "epoch 60 | loss: 0.59318 | train_auc: 0.75194 | train_accuracy: 0.69225 | val_auc: 0.76108 | val_accuracy: 0.69589 |  0:24:49s\n",
      "epoch 61 | loss: 0.59294 | train_auc: 0.7518  | train_accuracy: 0.69203 | val_auc: 0.76103 | val_accuracy: 0.69681 |  0:25:17s\n",
      "epoch 62 | loss: 0.59426 | train_auc: 0.75197 | train_accuracy: 0.6918  | val_auc: 0.7611  | val_accuracy: 0.69949 |  0:25:41s\n",
      "epoch 63 | loss: 0.59235 | train_auc: 0.7519  | train_accuracy: 0.69204 | val_auc: 0.76108 | val_accuracy: 0.69802 |  0:26:06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 | loss: 0.5901  | train_auc: 0.75185 | train_accuracy: 0.69204 | val_auc: 0.76106 | val_accuracy: 0.69822 |  0:26:30s\n",
      "epoch 65 | loss: 0.59175 | train_auc: 0.75188 | train_accuracy: 0.6924  | val_auc: 0.76102 | val_accuracy: 0.69714 |  0:27:06s\n",
      "epoch 66 | loss: 0.59532 | train_auc: 0.75187 | train_accuracy: 0.69198 | val_auc: 0.76103 | val_accuracy: 0.69853 |  0:27:46s\n",
      "epoch 67 | loss: 0.59318 | train_auc: 0.75189 | train_accuracy: 0.69214 | val_auc: 0.76105 | val_accuracy: 0.69579 |  0:28:19s\n",
      "epoch 68 | loss: 0.5922  | train_auc: 0.75187 | train_accuracy: 0.69201 | val_auc: 0.76095 | val_accuracy: 0.69642 |  0:28:48s\n",
      "epoch 69 | loss: 0.5931  | train_auc: 0.75189 | train_accuracy: 0.69243 | val_auc: 0.76102 | val_accuracy: 0.69636 |  0:29:22s\n",
      "epoch 70 | loss: 0.59166 | train_auc: 0.75187 | train_accuracy: 0.692   | val_auc: 0.76107 | val_accuracy: 0.6978  |  0:29:56s\n",
      "epoch 71 | loss: 0.5901  | train_auc: 0.75185 | train_accuracy: 0.6919  | val_auc: 0.76097 | val_accuracy: 0.69692 |  0:30:27s\n",
      "epoch 72 | loss: 0.59153 | train_auc: 0.75192 | train_accuracy: 0.69201 | val_auc: 0.7611  | val_accuracy: 0.6982  |  0:30:56s\n",
      "epoch 73 | loss: 0.59653 | train_auc: 0.75183 | train_accuracy: 0.69185 | val_auc: 0.76104 | val_accuracy: 0.69868 |  0:31:29s\n",
      "epoch 74 | loss: 0.59239 | train_auc: 0.75183 | train_accuracy: 0.69171 | val_auc: 0.76101 | val_accuracy: 0.69833 |  0:31:59s\n",
      "epoch 75 | loss: 0.59444 | train_auc: 0.75187 | train_accuracy: 0.69188 | val_auc: 0.761   | val_accuracy: 0.69818 |  0:32:29s\n",
      "epoch 76 | loss: 0.59347 | train_auc: 0.75187 | train_accuracy: 0.69225 | val_auc: 0.76101 | val_accuracy: 0.69543 |  0:32:55s\n",
      "epoch 77 | loss: 0.59112 | train_auc: 0.75189 | train_accuracy: 0.69233 | val_auc: 0.76103 | val_accuracy: 0.69641 |  0:33:18s\n",
      "epoch 78 | loss: 0.59319 | train_auc: 0.75188 | train_accuracy: 0.69211 | val_auc: 0.76104 | val_accuracy: 0.69533 |  0:33:42s\n",
      "epoch 79 | loss: 0.59144 | train_auc: 0.75189 | train_accuracy: 0.69236 | val_auc: 0.76108 | val_accuracy: 0.69696 |  0:34:03s\n",
      "epoch 80 | loss: 0.59334 | train_auc: 0.75184 | train_accuracy: 0.69222 | val_auc: 0.76112 | val_accuracy: 0.69513 |  0:34:32s\n",
      "epoch 81 | loss: 0.59564 | train_auc: 0.75186 | train_accuracy: 0.69206 | val_auc: 0.76099 | val_accuracy: 0.69681 |  0:35:07s\n",
      "epoch 82 | loss: 0.59206 | train_auc: 0.75187 | train_accuracy: 0.69211 | val_auc: 0.76102 | val_accuracy: 0.69749 |  0:35:33s\n",
      "epoch 83 | loss: 0.59128 | train_auc: 0.75188 | train_accuracy: 0.69179 | val_auc: 0.761   | val_accuracy: 0.69795 |  0:36:01s\n",
      "epoch 84 | loss: 0.59391 | train_auc: 0.75192 | train_accuracy: 0.69235 | val_auc: 0.76102 | val_accuracy: 0.69634 |  0:36:36s\n",
      "epoch 85 | loss: 0.58982 | train_auc: 0.75183 | train_accuracy: 0.69169 | val_auc: 0.76101 | val_accuracy: 0.69827 |  0:37:03s\n",
      "epoch 86 | loss: 0.59183 | train_auc: 0.75187 | train_accuracy: 0.69196 | val_auc: 0.76094 | val_accuracy: 0.69684 |  0:37:29s\n",
      "epoch 87 | loss: 0.59526 | train_auc: 0.75185 | train_accuracy: 0.69196 | val_auc: 0.7611  | val_accuracy: 0.69812 |  0:37:58s\n",
      "epoch 88 | loss: 0.59185 | train_auc: 0.75189 | train_accuracy: 0.69216 | val_auc: 0.76101 | val_accuracy: 0.6972  |  0:38:21s\n",
      "epoch 89 | loss: 0.59164 | train_auc: 0.75189 | train_accuracy: 0.69211 | val_auc: 0.76109 | val_accuracy: 0.69773 |  0:38:45s\n",
      "epoch 90 | loss: 0.58984 | train_auc: 0.75185 | train_accuracy: 0.69209 | val_auc: 0.76104 | val_accuracy: 0.69744 |  0:39:08s\n",
      "epoch 91 | loss: 0.5945  | train_auc: 0.75189 | train_accuracy: 0.6919  | val_auc: 0.76105 | val_accuracy: 0.698   |  0:39:28s\n",
      "epoch 92 | loss: 0.59151 | train_auc: 0.75196 | train_accuracy: 0.69217 | val_auc: 0.76105 | val_accuracy: 0.69754 |  0:39:47s\n",
      "epoch 93 | loss: 0.59214 | train_auc: 0.75186 | train_accuracy: 0.69208 | val_auc: 0.76103 | val_accuracy: 0.69792 |  0:40:15s\n",
      "epoch 94 | loss: 0.59174 | train_auc: 0.75193 | train_accuracy: 0.69233 | val_auc: 0.76121 | val_accuracy: 0.69696 |  0:40:44s\n",
      "epoch 95 | loss: 0.59266 | train_auc: 0.75182 | train_accuracy: 0.69201 | val_auc: 0.76101 | val_accuracy: 0.69744 |  0:41:18s\n",
      "epoch 96 | loss: 0.59298 | train_auc: 0.75185 | train_accuracy: 0.69153 | val_auc: 0.76102 | val_accuracy: 0.69856 |  0:41:40s\n",
      "epoch 97 | loss: 0.59361 | train_auc: 0.75192 | train_accuracy: 0.69193 | val_auc: 0.76104 | val_accuracy: 0.69838 |  0:42:06s\n",
      "epoch 98 | loss: 0.59218 | train_auc: 0.75186 | train_accuracy: 0.69208 | val_auc: 0.76103 | val_accuracy: 0.69561 |  0:42:36s\n",
      "epoch 99 | loss: 0.59666 | train_auc: 0.75189 | train_accuracy: 0.69214 | val_auc: 0.76106 | val_accuracy: 0.6975  |  0:43:03s\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    eval_name=['train', 'val'],\n",
    "    eval_metric=[\"auc\", 'accuracy'],\n",
    "#     eval_metric=[Recall],\n",
    "\n",
    "    max_epochs=100 , patience=0,\n",
    "    batch_size=400,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a0a48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6829503972514495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "test_recall = recall_score( y_true=y_test, y_pred=preds)\n",
    "test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da7421a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "KSF99rpmt5L1",
   "metadata": {
    "id": "KSF99rpmt5L1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AtEemh_pcdHR",
   "metadata": {
    "id": "AtEemh_pcdHR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fmeP4DRTuHnN",
   "metadata": {
    "id": "fmeP4DRTuHnN"
   },
   "source": [
    "### Save & Load TabNet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "CdyC7g9ruJ6P",
   "metadata": {
    "id": "CdyC7g9ruJ6P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at saved_models/binary_tabnet_model_1.zip\n"
     ]
    }
   ],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"saved_models/binary_tabnet_model_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "# define new model with basic parameters and load state dict weights\n",
    "# loaded_clf = TabNetClassifier()\n",
    "# loaded_clf.load_model(saved_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a262968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bdd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "MODEL_tabnet_pytorch.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
