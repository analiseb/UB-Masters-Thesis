{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385002dd",
   "metadata": {},
   "source": [
    "## TabNet Implentation for Tabular Data\n",
    "\n",
    "TabNet is proposed in [this article] (https://arxiv.org/abs/1908.07442) as a neural network architecture capable of learning a canonical representation of tabular data. This architecture has shown to perform well against the current gold-standard gradient boosting models for learning on tabular data.\n",
    "\n",
    "TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful\n",
    "features to process at each decision step. Instance-wise feature selection enables efficient learning as the model capacity is fully used for the most salient features, and also yields\n",
    "more interpretable decision making via visualization of selection masks. \n",
    "\n",
    "\n",
    "This implementation closely follows [the TabNet implementation in PyTorch linked here](https://github.com/dreamquark-ai/tabnet/tree/b6e1ebaf694f37ad40a6ba525aa016fd3cec15da). \n",
    "\n",
    "<img src=\"images/tabnet_schematic2.jpg\" width=\"1000\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "#### GLU Block\n",
    "\n",
    "Gated Linear Units act as an attention mechanism where the gates formed involve taking two dense layer outputs, applying a sigmoid to one of them, and then multiplying them together\n",
    "\n",
    "Following GLU blcok contains two dense layers, two ghost batch normalization layers, identity and sigmoid activation functions and multiplication operation.\n",
    "\n",
    "\n",
    "### Feature Transformer Block\n",
    "\n",
    "Builds two GLU blocks with a skip connection from the output of the first\n",
    "\n",
    "<img src=\"images/tabnet_feature_transformer.jpg\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "#### Attentive Transformer Block\n",
    "\n",
    "Use TabNet prior as an input to layer and reserve to handle prior updates in TabNet step layer\n",
    "\n",
    "> *prior is used to encourage orthogonal feature selection across decision steps, tell us what we know about features and how we have used them in the previous step\n",
    "\n",
    "<img src=\"images/tabnet_attentive_transformer.jpg\" width=\"200\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y-XevvaSe6_T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-XevvaSe6_T",
    "outputId": "8544313a-d02d-485e-cf73-736002fac06b"
   },
   "outputs": [],
   "source": [
    "# ! pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_hHbyvL7Ub7X",
   "metadata": {
    "id": "_hHbyvL7Ub7X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import global_variables as gv\n",
    "import utilities\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0f3b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "5e0f3b0b",
    "outputId": "ee150c2f-7353-4144-8f21-99e54d86cac1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30850-0.0</th>\n",
       "      <th>30780-0.0</th>\n",
       "      <th>30690-0.0</th>\n",
       "      <th>30790-0.0</th>\n",
       "      <th>23101-0.0</th>\n",
       "      <th>23099-0.0</th>\n",
       "      <th>48-0.0</th>\n",
       "      <th>23100-0.0</th>\n",
       "      <th>30710-0.0</th>\n",
       "      <th>30760-0.0</th>\n",
       "      <th>30640-0.0</th>\n",
       "      <th>30750-0.0</th>\n",
       "      <th>49-0.0</th>\n",
       "      <th>30770-0.0</th>\n",
       "      <th>30740-0.0</th>\n",
       "      <th>30630-0.0</th>\n",
       "      <th>30870-0.0</th>\n",
       "      <th>21001-0.0</th>\n",
       "      <th>1488-0.0</th>\n",
       "      <th>4079-0.0</th>\n",
       "      <th>1299-0.0</th>\n",
       "      <th>21003-0.0</th>\n",
       "      <th>1160-0.0</th>\n",
       "      <th>1438-0.0</th>\n",
       "      <th>4080-0.0</th>\n",
       "      <th>1458-0.0</th>\n",
       "      <th>1528-0.0</th>\n",
       "      <th>1319-0.0</th>\n",
       "      <th>845-0.0</th>\n",
       "      <th>1289-0.0</th>\n",
       "      <th>1309-0.0</th>\n",
       "      <th>1418-0.0</th>\n",
       "      <th>1329-0.0</th>\n",
       "      <th>1220-0.0</th>\n",
       "      <th>1428-0.0</th>\n",
       "      <th>1249-0.0</th>\n",
       "      <th>1349-0.0</th>\n",
       "      <th>1369-0.0</th>\n",
       "      <th>20117-0.0</th>\n",
       "      <th>2100-0.0</th>\n",
       "      <th>2654-0.0</th>\n",
       "      <th>1339-0.0</th>\n",
       "      <th>21000-0.0</th>\n",
       "      <th>2050-0.0</th>\n",
       "      <th>1408-0.0</th>\n",
       "      <th>1200-0.0</th>\n",
       "      <th>1538-0.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>6138-0.0</th>\n",
       "      <th>1359-0.0</th>\n",
       "      <th>1389-0.0</th>\n",
       "      <th>1478-0.0</th>\n",
       "      <th>2090-0.0</th>\n",
       "      <th>1508-0.0</th>\n",
       "      <th>1379-0.0</th>\n",
       "      <th>6142-0.0</th>\n",
       "      <th>1468-0.0</th>\n",
       "      <th>1548-0.0</th>\n",
       "      <th>1239-0.0</th>\n",
       "      <th>1448-0.0</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>outcome_cardiomyopathies</th>\n",
       "      <th>outcome_ischemic_heart_disease</th>\n",
       "      <th>outcome_heart_failure</th>\n",
       "      <th>outcome_myocardial_infarction</th>\n",
       "      <th>outcome_peripheral_vascular_disease</th>\n",
       "      <th>outcome_cardiac_arrest</th>\n",
       "      <th>outcome_cerebral_infarction</th>\n",
       "      <th>outcome_arrhythmia</th>\n",
       "      <th>multi-labels</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50800</td>\n",
       "      <td>3.88800</td>\n",
       "      <td>6.47700</td>\n",
       "      <td>65.1984</td>\n",
       "      <td>45.2</td>\n",
       "      <td>35.6</td>\n",
       "      <td>74.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.70600</td>\n",
       "      <td>1.21100</td>\n",
       "      <td>35.065</td>\n",
       "      <td>102.0</td>\n",
       "      <td>26.339</td>\n",
       "      <td>5.62200</td>\n",
       "      <td>1.59300</td>\n",
       "      <td>0.97700</td>\n",
       "      <td>24.5790</td>\n",
       "      <td>6.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.52</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.08800</td>\n",
       "      <td>3.52000</td>\n",
       "      <td>5.51200</td>\n",
       "      <td>15.4000</td>\n",
       "      <td>74.6</td>\n",
       "      <td>36.5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1.17300</td>\n",
       "      <td>1.01900</td>\n",
       "      <td>40.900</td>\n",
       "      <td>113.0</td>\n",
       "      <td>10.701</td>\n",
       "      <td>5.05200</td>\n",
       "      <td>1.39000</td>\n",
       "      <td>2.35800</td>\n",
       "      <td>35.0861</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>65</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.73364</td>\n",
       "      <td>4.10892</td>\n",
       "      <td>6.47949</td>\n",
       "      <td>50.8588</td>\n",
       "      <td>71.7</td>\n",
       "      <td>29.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.58546</td>\n",
       "      <td>1.22432</td>\n",
       "      <td>84.100</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.763</td>\n",
       "      <td>13.71763</td>\n",
       "      <td>1.74423</td>\n",
       "      <td>2.78764</td>\n",
       "      <td>30.7934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 1, 1]</td>\n",
       "      <td>55</td>\n",
       "      <td>Male</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.78800</td>\n",
       "      <td>2.88700</td>\n",
       "      <td>5.56500</td>\n",
       "      <td>56.5183</td>\n",
       "      <td>40.2</td>\n",
       "      <td>29.8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.11500</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>36.400</td>\n",
       "      <td>91.0</td>\n",
       "      <td>31.672</td>\n",
       "      <td>4.82700</td>\n",
       "      <td>1.89100</td>\n",
       "      <td>1.15700</td>\n",
       "      <td>20.7577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>Irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75600</td>\n",
       "      <td>2.67000</td>\n",
       "      <td>4.68000</td>\n",
       "      <td>4.7700</td>\n",
       "      <td>46.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.49300</td>\n",
       "      <td>0.73300</td>\n",
       "      <td>34.200</td>\n",
       "      <td>105.0</td>\n",
       "      <td>42.209</td>\n",
       "      <td>5.06300</td>\n",
       "      <td>1.86900</td>\n",
       "      <td>1.67700</td>\n",
       "      <td>25.9766</td>\n",
       "      <td>7.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   30850-0.0  30780-0.0  30690-0.0  30790-0.0  23101-0.0  23099-0.0  48-0.0  \\\n",
       "0    0.50800    3.88800    6.47700    65.1984       45.2       35.6    74.0   \n",
       "1   13.08800    3.52000    5.51200    15.4000       74.6       36.5   120.0   \n",
       "2    9.73364    4.10892    6.47949    50.8588       71.7       29.7   112.0   \n",
       "3    1.78800    2.88700    5.56500    56.5183       40.2       29.8    67.0   \n",
       "4    0.75600    2.67000    4.68000     4.7700       46.5       30.1    85.0   \n",
       "\n",
       "   23100-0.0  30710-0.0  30760-0.0  30640-0.0  30750-0.0  49-0.0  30770-0.0  \\\n",
       "0       25.0       0.34    1.70600    1.21100     35.065   102.0     26.339   \n",
       "1       42.9       3.94    1.17300    1.01900     40.900   113.0     10.701   \n",
       "2       30.3       3.88    1.58546    1.22432     84.100   107.0     18.763   \n",
       "3       17.0       0.87    2.11500    0.81000     36.400    91.0     31.672   \n",
       "4       20.0       0.18    1.49300    0.73300     34.200   105.0     42.209   \n",
       "\n",
       "   30740-0.0  30630-0.0  30870-0.0  21001-0.0  1488-0.0  4079-0.0  1299-0.0  \\\n",
       "0    5.62200    1.59300    0.97700    24.5790       6.0      77.0      10.0   \n",
       "1    5.05200    1.39000    2.35800    35.0861       2.0      91.0       2.0   \n",
       "2   13.71763    1.74423    2.78764    30.7934       0.0      99.0       2.0   \n",
       "3    4.82700    1.89100    1.15700    20.7577       0.0      71.0       5.0   \n",
       "4    5.06300    1.86900    1.67700    25.9766       7.0      73.0       4.0   \n",
       "\n",
       "   21003-0.0  1160-0.0  1438-0.0  4080-0.0  1458-0.0  1528-0.0  1319-0.0  \\\n",
       "0       54.0       7.0      10.0     110.0      3.73       2.0       0.0   \n",
       "1       65.0       9.0      12.0     166.0      7.00       2.4       0.0   \n",
       "2       55.0       7.0      10.0     135.0      7.00       2.0       0.0   \n",
       "3       49.0       8.0      14.0     116.0      5.00       3.0       1.0   \n",
       "4       61.0       7.0       2.0     113.0      7.00       4.0       2.0   \n",
       "\n",
       "   845-0.0  1289-0.0  1309-0.0  1418-0.0  1329-0.0  1220-0.0  1428-0.0  \\\n",
       "0    23.52       6.0       2.0         3         2         0         0   \n",
       "1    16.00       2.0       1.0         2         2         0         1   \n",
       "2    21.00       3.0       1.0         2         1         0         0   \n",
       "3    18.00       5.0       1.0         2         2         0         0   \n",
       "4    16.00       3.0       3.0         3         2         1         1   \n",
       "\n",
       "   1249-0.0  1349-0.0  1369-0.0  20117-0.0  2100-0.0  2654-0.0  1339-0.0  \\\n",
       "0         1         1         1          2         1         6         2   \n",
       "1         1         4         2          2         0         7         2   \n",
       "2         1         2         1          2         0         7         2   \n",
       "3         4         1         2          2         0         7         2   \n",
       "4         4         1         1          2         0         7         3   \n",
       "\n",
       "   21000-0.0  2050-0.0  1408-0.0  1200-0.0  1538-0.0  31-0.0  6138-0.0  \\\n",
       "0          0         2         1         3         2       0         1   \n",
       "1          0         1         3         2         0       1         3   \n",
       "2          0         1         2         2         1       1         3   \n",
       "3          2         1         2         1         2       0         6   \n",
       "4          0         1         3         1         0       0         3   \n",
       "\n",
       "   1359-0.0  1389-0.0  1478-0.0  2090-0.0  1508-0.0  1379-0.0  6142-0.0  \\\n",
       "0         2         1         1         1         3         1         1   \n",
       "1         3         1         1         0         2         2         1   \n",
       "2         3         2         1         0         2         2         1   \n",
       "3         2         2         1         0         2         2         1   \n",
       "4         3         1         2         0         1         1         1   \n",
       "\n",
       "   1468-0.0  1548-0.0  1239-0.0  1448-0.0  hypertension  \\\n",
       "0         3         2         0         3             0   \n",
       "1         5         2         0         1             1   \n",
       "2         4         2         0         3             1   \n",
       "3         3         2         0         3             0   \n",
       "4         4         2         0         3             1   \n",
       "\n",
       "   outcome_cardiomyopathies  outcome_ischemic_heart_disease  \\\n",
       "0                         0                               0   \n",
       "1                         0                               1   \n",
       "2                         0                               1   \n",
       "3                         0                               0   \n",
       "4                         0                               0   \n",
       "\n",
       "   outcome_heart_failure  outcome_myocardial_infarction  \\\n",
       "0                      0                              0   \n",
       "1                      0                              1   \n",
       "2                      0                              0   \n",
       "3                      0                              0   \n",
       "4                      0                              1   \n",
       "\n",
       "   outcome_peripheral_vascular_disease  outcome_cardiac_arrest  \\\n",
       "0                                    0                       0   \n",
       "1                                    0                       0   \n",
       "2                                    0                       1   \n",
       "3                                    0                       0   \n",
       "4                                    0                       0   \n",
       "\n",
       "   outcome_cerebral_infarction  outcome_arrhythmia              multi-labels  \\\n",
       "0                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "1                            0                   0  [1, 0, 1, 0, 0, 0, 0, 0]   \n",
       "2                            1                   1  [0, 0, 1, 0, 0, 1, 1, 1]   \n",
       "3                            0                   1  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "4                            0                   0  [1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "   age  gender     race  \n",
       "0   54  Female  British  \n",
       "1   65    Male  British  \n",
       "2   55    Male  British  \n",
       "3   49  Female    Irish  \n",
       "4   61  Female  British  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(gv.data_link)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iiRxbosArp5N",
   "metadata": {
    "id": "iiRxbosArp5N"
   },
   "source": [
    "### Test TabNet Binary Classifier out-of-the-box (predicting Ischemic Heart Disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2d28ea4",
   "metadata": {
    "id": "c2d28ea4"
   },
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_test1, y_train1, y_val1, y_test1 = utilities.process_features(df, gv.outcomes[2], StandardScaler(), one_hot=True)\n",
    "X_train1, y_train1= utilities.resample_data(X_train1, y_train1, 'under')\n",
    "\n",
    "X_train= X_train1.to_numpy()\n",
    "X_val= X_val1.to_numpy()\n",
    "X_test= X_test1.to_numpy()\n",
    "\n",
    "y_train= y_train1.to_numpy()\n",
    "y_val= y_val1.to_numpy()\n",
    "y_test= y_test1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from keras import backend as K\n",
    "class my_recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return recall_score(y_true, y_score[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9469f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b9469f1",
    "outputId": "68a9b50c-5aa6-4aaa-dd91-8bd3cc426dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 0.70599 | val_0_auc: 0.59174 |  0:00:03s\n",
      "epoch 1  | loss: 0.67856 | val_0_auc: 0.60514 |  0:00:07s\n",
      "epoch 2  | loss: 0.67074 | val_0_auc: 0.63598 |  0:00:11s\n",
      "epoch 3  | loss: 0.6608  | val_0_auc: 0.65745 |  0:00:15s\n",
      "epoch 4  | loss: 0.65713 | val_0_auc: 0.66567 |  0:00:20s\n",
      "epoch 5  | loss: 0.65427 | val_0_auc: 0.66975 |  0:00:24s\n",
      "epoch 6  | loss: 0.65039 | val_0_auc: 0.67362 |  0:00:29s\n",
      "epoch 7  | loss: 0.64773 | val_0_auc: 0.67414 |  0:00:33s\n",
      "epoch 8  | loss: 0.64835 | val_0_auc: 0.67648 |  0:00:37s\n",
      "epoch 9  | loss: 0.64655 | val_0_auc: 0.6827  |  0:00:41s\n",
      "epoch 10 | loss: 0.64215 | val_0_auc: 0.68757 |  0:00:45s\n",
      "epoch 11 | loss: 0.63986 | val_0_auc: 0.69226 |  0:00:49s\n",
      "epoch 12 | loss: 0.63758 | val_0_auc: 0.69549 |  0:00:53s\n",
      "epoch 13 | loss: 0.63491 | val_0_auc: 0.69701 |  0:00:57s\n",
      "epoch 14 | loss: 0.63313 | val_0_auc: 0.69772 |  0:01:01s\n",
      "epoch 15 | loss: 0.63194 | val_0_auc: 0.69824 |  0:01:05s\n",
      "epoch 16 | loss: 0.631   | val_0_auc: 0.70142 |  0:01:09s\n",
      "epoch 17 | loss: 0.62911 | val_0_auc: 0.70444 |  0:01:13s\n",
      "epoch 18 | loss: 0.62706 | val_0_auc: 0.70461 |  0:01:17s\n",
      "epoch 19 | loss: 0.62683 | val_0_auc: 0.70521 |  0:01:21s\n",
      "epoch 20 | loss: 0.62326 | val_0_auc: 0.70465 |  0:01:25s\n",
      "epoch 21 | loss: 0.62291 | val_0_auc: 0.70675 |  0:01:29s\n",
      "epoch 22 | loss: 0.62124 | val_0_auc: 0.7085  |  0:01:33s\n",
      "epoch 23 | loss: 0.62149 | val_0_auc: 0.7073  |  0:01:36s\n",
      "epoch 24 | loss: 0.62002 | val_0_auc: 0.71023 |  0:01:40s\n",
      "epoch 25 | loss: 0.61872 | val_0_auc: 0.70806 |  0:01:44s\n",
      "epoch 26 | loss: 0.61568 | val_0_auc: 0.70953 |  0:01:48s\n",
      "epoch 27 | loss: 0.61542 | val_0_auc: 0.71011 |  0:01:52s\n",
      "epoch 28 | loss: 0.61404 | val_0_auc: 0.71072 |  0:01:56s\n",
      "epoch 29 | loss: 0.61296 | val_0_auc: 0.71092 |  0:01:59s\n",
      "epoch 30 | loss: 0.61101 | val_0_auc: 0.71174 |  0:02:03s\n",
      "epoch 31 | loss: 0.61074 | val_0_auc: 0.71206 |  0:02:07s\n",
      "epoch 32 | loss: 0.60847 | val_0_auc: 0.71376 |  0:02:11s\n",
      "epoch 33 | loss: 0.60848 | val_0_auc: 0.71268 |  0:02:15s\n",
      "epoch 34 | loss: 0.60627 | val_0_auc: 0.71025 |  0:02:19s\n",
      "epoch 35 | loss: 0.6057  | val_0_auc: 0.71352 |  0:02:23s\n",
      "epoch 36 | loss: 0.605   | val_0_auc: 0.71013 |  0:02:27s\n",
      "epoch 37 | loss: 0.60455 | val_0_auc: 0.71146 |  0:02:31s\n",
      "epoch 38 | loss: 0.60411 | val_0_auc: 0.71142 |  0:02:37s\n",
      "epoch 39 | loss: 0.60441 | val_0_auc: 0.7085  |  0:02:42s\n",
      "epoch 40 | loss: 0.60344 | val_0_auc: 0.70607 |  0:02:46s\n",
      "epoch 41 | loss: 0.60161 | val_0_auc: 0.7121  |  0:02:51s\n",
      "epoch 42 | loss: 0.60026 | val_0_auc: 0.70993 |  0:02:55s\n",
      "\n",
      "Early stopping occured at epoch 42 with best_epoch = 32 and best_val_0_auc = 0.71376\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier()  \n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "  eval_set=[(X_val, y_val)],\n",
    "  eval_metric=[\"auc\"]\n",
    ")\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "IJSsVVNmUEmR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "IJSsVVNmUEmR",
    "outputId": "526bce84-c3c0-4716-dd1e-2406bbcd5903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21815363700>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(clf.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b6c65",
   "metadata": {},
   "source": [
    "### Global Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ouDy6aHNUHL5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ouDy6aHNUHL5",
    "outputId": "74754b86-04fe-420b-bdf1-4626a3540228",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_weights = clf.feature_importances_\n",
    "\n",
    "# zip to feature names\n",
    "input_cols = X_train1.columns.to_list()\n",
    "feat_dict = dict(zip(input_cols, feat_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2062bd0",
   "metadata": {},
   "source": [
    "### Local Explainablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0274d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "TIcvwljsVTHq",
   "metadata": {
    "id": "TIcvwljsVTHq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7109673743008749"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LumhsZwC5P3j",
   "metadata": {
    "id": "LumhsZwC5P3j"
   },
   "source": [
    "## Customize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XryRDsqFpr0s",
   "metadata": {
    "id": "XryRDsqFpr0s"
   },
   "source": [
    "#### Fit parameters\n",
    "\n",
    "<ul>\n",
    "  <li> <b>X_train</b> (np.array): Training Features </li>\n",
    "  <li> <b>y_train</b> (np.array): Training Targets </li>\n",
    "  <li> <b>eval_set</b> (list of eval tuple set):  last one used for early stopping </li>\n",
    "  <li> <b>eval_name</b> (list of str): list of eval set names </li>\n",
    "  <li> <b>eval_metric</b> (list of str: list of evaluation metrics; last used for early stopping </li>\n",
    "  <li> <b>max_epochs</b> (int=200): max epochs for training</li>\n",
    "  <li> <b>patience</b> (int=10):#epochs before early stopping, if 0 then no early stopping performed </li>\n",
    "  <li> <b>weights</b> (int or dict=0): only for TabNetClassifier, sampling param 0 => no sampling, param 0 => automated sampling with inverse class occurences </li>\n",
    "  <li> <b>loss_fn</b>(torch.loss): loss fn for training, w classification can set a list of same length as num tasks  </li>\n",
    "  <li> <b>batch_size</b> (int=1024): #  examples/batch </li>\n",
    "  <li> <b>virtual_batch_size</b> (int=128): size of mini batches for ghost batch normalization  </li>\n",
    "  <li> <b>num_workers</b> (int=0): # workers used in torch.utils.data.Dataloader  </li>\n",
    "  <li> <b>drop_last</b> (bool=False): whether to drop last batch if not complete during training  </li>\n",
    "  <li> <b>callbacks</b> (list of callback fn): list of custom callbacks </li>\n",
    "  <li> <b>pretraining_ratio</b> (float): %input features to mask during pretraining  </li>\n",
    "  <li> <b>warm_start</b> (bool=False): allows to fit twice the same model and start from a warm start  </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [utilities.recall_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QlQWw4np5ufx",
   "metadata": {
    "id": "QlQWw4np5ufx"
   },
   "outputs": [],
   "source": [
    "unused_feat = ['Set']\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PO6tgvAF5XHN",
   "metadata": {
    "id": "PO6tgvAF5XHN"
   },
   "outputs": [],
   "source": [
    "tabnet_params = {\"cat_idxs\":cat_idxs, # list of categorical feature indices\n",
    "                 \"cat_dims\":cat_dims, # list of categorical features number of modalities (#unique values for a categorical feature)\n",
    "                 \"cat_emb_dim\":1, # list of embeddings size for each categorical features\n",
    "                 \"optimizer_fn\":torch.optim.Adam, # pytorch optimizer function\n",
    "                 \"optimizer_params\":dict(lr=2e-2), # parameters compatible with optimizer_fn\n",
    "                 \"scheduler_params\":{\"step_size\":50, # how to use learning rate scheduler\n",
    "                                 \"gamma\":0.9}, # dictionary of parameters to apply to the scheduler\n",
    "                 \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
    "                 \"mask_type\":'entmax' # \"sparsemax\" # either sparsemax or entmac, masking fn for selecting features\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSF99rpmt5L1",
   "metadata": {
    "id": "KSF99rpmt5L1"
   },
   "outputs": [],
   "source": [
    "# BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AtEemh_pcdHR",
   "metadata": {
    "id": "AtEemh_pcdHR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "WxTTTOmlr0__",
   "metadata": {
    "id": "WxTTTOmlr0__"
   },
   "source": [
    "### Implement Semi-supervised Pre-training (tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jMghm70xsycy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "jMghm70xsycy",
    "outputId": "67ebbf4b-fb17-4e92-e08b-a89e17127ddd"
   },
   "outputs": [],
   "source": [
    "# import preprocessed data before imputation\n",
    "df2 = pd.read_csv(gv.tabnet_data)\n",
    "df2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec6b00",
   "metadata": {
    "id": "12ec6b00"
   },
   "outputs": [],
   "source": [
    "# TabNetPretrainer\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax' # \"sparsemax\"\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_val],\n",
    "    pretraining_ratio=0.8,\n",
    ")\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                      \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax' # This will be overwritten if using pretrain model\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    from_unsupervised=unsupervised_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmeP4DRTuHnN",
   "metadata": {
    "id": "fmeP4DRTuHnN"
   },
   "source": [
    "### Save & Load TabNet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CdyC7g9ruJ6P",
   "metadata": {
    "id": "CdyC7g9ruJ6P"
   },
   "outputs": [],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filepath)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "MODEL_tabnet_pytorch.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
